{
    "deliberation_results": {
        "Machine Learning Engineering (Large Language Models Development)": {
            "time_to_resolution": "Approximately 2.5 months (question resolves January 1, 2026; today is October 13, 2025)",
            "perspective_relevance": "As a Machine Learning Engineer specializing in Large Language Models (LLMs), I directly track and understand the latest advances in model architectures (e.g., GPT-5, Gemini 2.5, DeepSeek-R1), training methodologies (RL, self-consistency, System-2 reasoning), benchmarks (AIME, IMO, SWE-bench, and crucially, FrontierMath), and the nuances of benchmark saturation and Goodhart's Law. I am able to gauge not just raw performance numbers, but the likelihood that research labs will prioritize and publish results on a specific, difficult benchmark like FrontierMath, and the technical feasibility of surpassing an 85% threshold given current trends and compute scaling.",
            "status_quo": "To date, there is no public evidence that any AI system has achieved >85% on the FrontierMath benchmark. The status quo is that models like GPT-5, Gemini 2.5, and DeepSeek-R1 are achieving superhuman results on AIME, IMO, ICPC, and SWE-bench, but FrontierMath remains one of the hardest and least-solved public math benchmarks.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Superhuman Performance on Adjacent Benchmarks (AIME, IMO, ICPC)",
                    "effect": "Increases probability. Evidence shows rapid progress with new models achieving gold/superhuman performance on math and logic contests that were recently considered out of reach."
                },
                {
                    "factor": "FrontierMath Difficulty and Benchmark 'Leakage' Avoidance",
                    "effect": "Decreases probability. Unlike older benchmarks, FrontierMath is specifically designed to be difficult, novel, and resistant to training data contamination, making it a stubborn target for LLMs even as other benchmarks saturate."
                },
                {
                    "factor": "Model Scaling Laws and Novel Training Techniques (RL, System-2 Reasoning, Self-Play)",
                    "effect": "Increases probability. Techniques like those used in DeepSeek-R1 and ProRL show that reasoning capabilities can leap with further compute and more sophisticated reinforcement learning, potentially unlocking rapid progress."
                },
                {
                    "factor": "Time Pressure and Incentive to Demonstrate Progress on FrontierMath",
                    "effect": "Increases probability. With only ~2.5 months left and FrontierMath now a recognized 'last frontier,' OpenAI, DeepMind, and DeepSeek have strong incentive to target and publicize a breakthrough if achievable."
                },
                {
                    "factor": "Benchmark Gaming and Goodhart's Law",
                    "effect": "Decreases probability. Recent articles and expert commentary warn that once a benchmark becomes a target, teams may overfit or 'game' it, but due to FrontierMath's design, this is harder to do, limiting quick wins."
                },
                {
                    "factor": "Publication Lag and Resolution Criteria",
                    "effect": "Decreases probability. Even if a lab achieves >85%, the requirement for a public document (paper/blog/post) before Jan 1, 2026, means that secrecy, embargo, or slow peer review could block positive resolution."
                }
            ],
            "no_scenario": "Despite rapid progress on AIME, IMO, and ICPC, no team manages to crack >85% on FrontierMath, either because the problems are fundamentally too hard, the models plateau short of the threshold, or because results are not published in time. Labs may focus on other benchmarks or new model releases, or hit diminishing returns due to fundamental reasoning bottlenecks.",
            "yes_scenario": "OpenAI, DeepMind, DeepSeek, or another lab specifically targets FrontierMath with a new model, leveraging RL, System-2, or curriculum strategies. They achieve >85% and publish a blog post, preprint, or official benchmark report before January 1, 2026, highlighting a new milestone in mathematical reasoning.",
            "final_reasoning": "The pace of progress in advanced reasoning LLMs has been extraordinary in 2025. Models like GPT-5 and DeepSeek-R1 have moved from high school\u2013level math to IMO gold medal and ICPC perfection in under a year. The leap from strong AIME/IMO performance to >85% on FrontierMath is formidable, but the technical gap is shrinking fast due to improvements in RL, self-consistency, and architecture scaling. However, FrontierMath is designed to be resistant to memorization and leakage, so this jump is not trivial. The explicit incentive for labs to make a public breakthrough before year-end is high, with the field\u2019s attention now focused on 'the last untouched frontier.' Risks remain: the possibility that even top models fall short, or that results are held back for competitive or safety reasons, or that publication is delayed. Nonetheless, given the dramatic progress on adjacent benchmarks, the demonstrated ability of labs to rapidly scale and publish, and the proximity to the threshold (e.g., DeepSeek achieving 86.7% on AIME with self-consistency), it is now more likely than not that >85% will be achieved and announced before the deadline. I put the probability at 70%, reflecting high but not extreme confidence.",
            "final_probability": 70
        },
        "Machine Learning Engineering (Benchmark Evaluation Standards)": {
            "time_to_resolution": "Approximately 2.5 months (until January 1, 2026)",
            "perspective_relevance": "As an expert in Machine Learning Engineering with a focus on Benchmark Evaluation Standards, I am uniquely positioned to evaluate not only the technical progress of frontier models on mathematical reasoning tasks, but also the nuances of benchmark design, evaluation validity, and the likelihood that recent performance advances will generalize to a difficult and novel test like FrontierMath. My background equips me to critically assess claims of superhuman performance, the robustness of evaluation methodologies, and the challenge of achieving >85% on a benchmark explicitly constructed to resist overfitting and pattern exploitation.",
            "status_quo": "As of early 2025, no AI system had surpassed the 85% threshold on the FrontierMath benchmark. The benchmark was intentionally designed to remain challenging for state-of-the-art models, with prior top scores likely well below the threshold. The status quo outcome, barring recent developments, would be that no AI system achieves >85% on FrontierMath before 2026.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Breakthroughs in Mathematical Reasoning",
                    "effect": "Increases probability. The news describes GPT-5 and DeepSeek-R1 achieving superhuman results on the AIME and IMO, tasks historically used as proxies for advanced mathematical reasoning. This suggests significant progress in LLM-based reasoning."
                },
                {
                    "factor": "Evidence of Generalization vs. Overfitting",
                    "effect": "Moderately increases probability. While models are now surpassing previous benchmarks, FrontierMath was designed to avoid data leakage and is not widely available for training. However, the reinforcement learning techniques and self-improving curriculum approaches (e.g., DeepSeek, Tencent's R-Zero) may enhance generalization, increasing the odds of success."
                },
                {
                    "factor": "Benchmark Evaluation Standards and Reporting Requirements",
                    "effect": "Slightly decreases probability. The resolution requires documented, public results. While OpenAI and others are publicizing results, there may be proprietary delays or strategic timing, and some high-profile results (e.g., IMO gold medal) have not been independently replicated nor made public immediately."
                },
                {
                    "factor": "Benchmark Difficulty and Intentional Antifragility",
                    "effect": "Decreases probability. FrontierMath is specifically constructed to resist the kinds of overfitting and shortcut exploitation that have allowed models to 'solve' previous math benchmarks. As with ARC-AGI-2 and ZeroBench, new benchmarks can remain unsolved even after large leaps in other metrics."
                },
                {
                    "factor": "Market Incentives and Model Release Timelines",
                    "effect": "Moderately increases probability. There is intense competition between OpenAI, Google, DeepSeek, etc., with recent models being rapidly publicized and evaluated on public benchmarks. The time to resolution (2.5 months) is sufficient for a major lab to target and publish a >85% result, especially for marketing or regulatory positioning."
                },
                {
                    "factor": "Recent Model Performance Near the Threshold",
                    "effect": "Strongly increases probability. DeepSeek-R1 is reported at 86.7% on AIME with self-consistency, and GPT-5 shows 94.6% on AIME-2025 and gold-level IMO performance. These are harder than previous math benchmarks, though the transfer to FrontierMath is not guaranteed."
                },
                {
                    "factor": "Potential for Unpublished but Achieved Results",
                    "effect": "Slightly decreases probability. It is possible a >85% result is achieved but not published in time for resolution, due to internal review, safety, or competitive secrecy. However, recent publicity around performance suggests at least one lab will make a public release."
                }
            ],
            "no_scenario": "Despite frontier models achieving superhuman performance on AIME, IMO, and coding competitions, no AI achieves >85% on FrontierMath before 2026 due to the benchmark's novelty, antifragility, and possible additional difficulty (e.g., requiring symbolic proofs or out-of-distribution reasoning). Alternatively, a model achieves the threshold but the result is not publicly documented in a qualifying manner before the deadline.",
            "yes_scenario": "OpenAI, DeepMind, or DeepSeek specifically targets FrontierMath as the next milestone, devotes a model (e.g., GPT-5, Gemini 2.5, or DeepSeek-R1+) to the task, and achieves >85% using advanced RL, self-consistency, or system-2 approaches. They publish a blog post, paper, or benchmark leaderboard entry with qualifying documentation prior to January 1, 2026.",
            "final_reasoning": "The combination of recent, well-documented breakthroughs in mathematical reasoning (AIME, IMO, ICPC), the proximity of current model performance to and even above the 85% threshold on comparably difficult math benchmarks, and the competitive incentives for labs to publicly claim new milestones all sharply raise the probability of success. My expertise in benchmarking standards raises caution: FrontierMath is intentionally constructed to avoid gaming, and prior examples (ARC-AGI-2, ZeroBench) show that some benchmarks resist rapid progress even after leaps elsewhere. However, the momentum of recent months, the demonstrated ability to generalize (e.g., gold medal IMO, perfect ICPC scores), and the plausibility that at least one lab has already achieved or is about to achieve >85%\u2014combined with the likelihood of public documentation for PR, regulatory, or fundraising reasons\u2014leads me to a high but not maximal probability. I discount slightly due to the possibility of a last-minute technicality (e.g., non-publication or a failed independent replication), but overall, the preponderance of evidence supports a strong 'Yes' forecast.",
            "final_probability": 88
        },
        "AI Policy and Forecasting (Technology Forecasting Methodologies)": {
            "time_to_resolution": "Approximately 2.5 months (question resolves on January 1, 2026; today is October 13, 2025)",
            "perspective_relevance": "As an expert in AI policy and technology forecasting, I bring deep familiarity with benchmarking methodology, the rate of AI progress, and the incentives/constraints facing leading labs. My approach emphasizes base rates, comparison to prior benchmark progress, and detailed scrutiny of stated benchmark results and their reporting, which is critical here given the question's requirement for published, verifiable results.",
            "status_quo": "As of October 2025, no public report has stated that an AI system has exceeded 85% on the FrontierMath benchmark. The most recent news shows top systems (e.g., GPT-5, DeepSeek-R1, Gemini 2.5) achieving superhuman results on AIME, IMO, ICPC, and other math/coding benchmarks, but not specifically surpassing 85% on FrontierMath.",
            "perspective_derived_factors": [
                {
                    "factor": "FrontierMath Difficulty & Novelty",
                    "effect": "Decreases probability. FrontierMath is explicitly designed to go beyond AIME, IMO, and standard benchmarks, with fresh, hard problems. Historical precedent shows that as soon as a benchmark becomes widely known, models quickly saturate it, but new, unseen benchmarks like FrontierMath often reveal remaining weaknesses. No evidence yet that any system is above 85% on this particular test."
                },
                {
                    "factor": "Recent AI Progress on Math/Reasoning Benchmarks",
                    "effect": "Increases probability. News shows dramatic improvement: GPT-5 achieves 94.6% on AIME 2025, DeepSeek-R1 hits 86.7% with self-consistency, and both OpenAI and DeepMind have gold-medal or perfect scores on IMO, ICPC. General reasoning and self-improvement approaches (RL, self-play, chain-of-thought, etc.) are proving effective."
                },
                {
                    "factor": "Time Remaining Until Resolution",
                    "effect": "Increases probability. There are still 2.5 months for new models/releases, and several labs (OpenAI, Google, DeepSeek) are highly motivated to claim public milestones before year's end."
                },
                {
                    "factor": "Incentives for Public Reporting and Benchmarking",
                    "effect": "Increases probability. Labs are now strongly incentivized to claim public 'firsts' on new, hard benchmarks for reputational and commercial reasons. Recent blog posts and papers (e.g., OpenAI's gold IMO result) suggest that if a system exceeds 85% on FrontierMath, it will likely be reported promptly and publicly."
                },
                {
                    "factor": "Benchmark Gaming and Overfitting Risks",
                    "effect": "Decreases probability. As noted in several articles, once a benchmark's content is known, labs may inadvertently or deliberately overfit, and organizers may withhold test sets to prevent this. However, if the benchmark is well-protected and genuinely novel, it will be harder to surpass 85% immediately."
                },
                {
                    "factor": "Historical Base Rate for Surpassing New, Hard Benchmarks",
                    "effect": "Decreases probability. Historically, there is often a lag (months to a year) between a new, truly hard benchmark's introduction and the first >85% result. For instance, ARC-AGI-2 and GPQA remain unsolved at high levels. However, recent acceleration in capabilities may compress this window."
                },
                {
                    "factor": "Technical Plausibility",
                    "effect": "Increases probability. The architecture/design advances (self-consistency, RL, extended inference time) demonstrated by DeepSeek, OpenAI, and others show that the capability ceiling is moving rapidly. If a system can achieve 95%+ on AIME and gold on IMO, it's plausible it could break 85% on FrontierMath given sufficient tuning."
                }
            ],
            "no_scenario": "Despite recent advances, no existing system achieves >85% on FrontierMath by January 1, 2026, either because the benchmark is significantly harder than AIME/IMO, genuine generalization gaps remain, or labs focus on other priorities. Alternatively, a model achieves the performance internally, but the result is not published in a qualifying document (peer-reviewed paper, official blog, etc.) before the deadline, or is withheld for strategic or regulatory reasons.",
            "yes_scenario": "A leading lab (e.g., OpenAI, DeepMind, DeepSeek) releases a new model or applies advanced reasoning techniques (extended inference, RL, ensemble, self-consistency) to FrontierMath and achieves >85% performance. The result is published in a peer-reviewed paper, conference talk, official blog, or benchmark publication, clearly exceeding the 85% threshold, before January 1, 2026. The announcement is rapid, due to competitive incentives and the desire to claim the milestone.",
            "final_reasoning": "The news strongly suggests that SOTA models have achieved or are on the brink of achieving superhuman mathematical reasoning by several measures: perfect scores on ICPC, gold on IMO (under human-like conditions), and 94.6% on AIME. However, FrontierMath is designed to be novel and difficult, explicitly to prevent benchmark overfitting and to test generalized reasoning beyond AIME/IMO. Historically, there is often a short lag before a new, hard benchmark is surpassed, but labs are now moving fast and have strong incentives to publish milestone results. The technical advances (RL-based training, self-consistency, extended inference) make it plausible that >85% FrontierMath is within reach for GPT-5, DeepSeek-R1, or Gemini 2.5, especially if further tuned. The biggest risk is that either the benchmark proves much harder than expected, or that a qualifying result is achieved internally but not publicly released before the deadline. Given the recent rapid progress, the incentives to publish, and the time remaining, I estimate the probability of a Yes outcome at 78%\u2014high, but not extreme, reflecting some remaining uncertainty about benchmark difficulty and reporting.",
            "final_probability": 78
        },
        "Artificial Intelligence (Deep Learning Theory)": {
            "time_to_resolution": "Approximately 2.5 months (question resolves on January 1, 2026; forecast date is October 13, 2025)",
            "perspective_relevance": "As an expert in Deep Learning Theory, I can critically assess the technical trajectory, limitations, and scaling potential of frontier AI systems on hard benchmarks like FrontierMath. My perspective is grounded in understanding the architecture, training regimes, and evaluation pitfalls that distinguish superhuman performance on recent math/coding benchmarks from the even more challenging demands of FrontierMath. This knowledge allows for nuanced analysis of whether current advances (e.g., GPT-5, DeepSeek-R1, Gemini 2.5) are likely to generalize to >85% on this newest and most rigorous benchmark within the tight time window.",
            "status_quo": "As of late 2024, no AI system was known to achieve >85% on the FrontierMath benchmark. The best models (GPT-4o, DeepSeek, Gemini) were excelling on benchmarks like AIME, IMO, and ICPC, but no public evidence indicated that FrontierMath had been 'crushed.' The status quo is a 'No' outcome\u2014FrontierMath remains unsolved at the >85% level.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Superhuman Results on Comparable Math/Reasoning Benchmarks",
                    "effect": "Strongly increases probability. GPT-5 and DeepSeek-R1 have achieved superhuman scores on AIME (94.6% and 86.7%, respectively), ICPC (GPT-5: 100%), and gold medal-level at IMO. These are extremely strong signals of rapid progress, with some results above the 85% bar on tough math benchmarks."
                },
                {
                    "factor": "FrontierMath Benchmark Difficulty and Novelty",
                    "effect": "Decreases probability. FrontierMath is designed to resist data leakage and Goodharting, and is reportedly harder than AIME and possibly IMO. It is newer, less likely to be in training data, and may have a distribution shift from prior benchmarks, making transfer less certain."
                },
                {
                    "factor": "Time Window (Short - Less than 3 Months Left)",
                    "effect": "Decreases probability. While the pace of progress is rapid, the question closes in under three months. SOTA models (e.g., GPT-5, DeepSeek-R1) are already released; a further leap or a targeted run on FrontierMath must happen and be documented imminently."
                },
                {
                    "factor": "Emergence of Reasoning-Driven Training Techniques (RL, Self-Play, System-2, Self-Consistency)",
                    "effect": "Increases probability. New RL approaches (DeepSeek-R1, ProRL, R-Zero) allow models to develop genuine new reasoning strategies, not just memorize patterns. This is critical for benchmarks like FrontierMath designed to thwart pattern recognition."
                },
                {
                    "factor": "Publication and Documentation Requirements",
                    "effect": "Slightly decreases probability. For resolution, there must be a public, documented >85% score. If a lab achieves the milestone but delays or withholds publication (for commercial, safety, or PR reasons), the question resolves 'No.'"
                },
                {
                    "factor": "Evidence of Unsolved or Plateaued Benchmarks",
                    "effect": "Slightly decreases probability. Some recent articles (e.g., about ZeroBench, EnigmaEval, and ARC-AGI-2) note that despite rapid progress, certain new benchmarks remain unsolved or have only incremental progress. This suggests that continual benchmark inflation may temporarily outpace model advances."
                }
            ],
            "no_scenario": "Despite superhuman performance on AIME, IMO, and ICPC, no AI system is able to achieve >85% on FrontierMath before January 1, 2026. Either current models fall short due to the benchmark's novelty/hardness or, even if a model does achieve this level, no public documentation is released by the deadline (perhaps due to strategic, commercial, or safety concerns). This outcome is the status quo and would be favored if FrontierMath is substantially harder or more robust to recent advances than other benchmarks.",
            "yes_scenario": "OpenAI, DeepMind, DeepSeek, or another leading lab targets FrontierMath directly with their newest reasoning models (e.g., GPT-5, Gemini 2.5, DeepSeek-R1 or a new variant), leveraging advanced RL and self-consistency techniques. They document a >85% score in a public paper, blog, or benchmark leaderboard prior to January 1, 2026. This is plausible given the momentum from recent results on IMO, AIME, and ICPC, especially if FrontierMath is not substantially more difficult or different in distribution.",
            "final_reasoning": "The evidence for rapid AI math/reasoning progress is overwhelming: GPT-5 and DeepSeek-R1 have achieved or surpassed 85\u201395% on AIME, gold-medal at IMO, and perfect scores on ICPC\u2014all within the last two months. The techniques enabling these jumps (RL, self-consistency, chain-of-thought, System-2 inference) are precisely those needed for robust, generalizable mathematical reasoning, the kind FrontierMath is designed to test. However, several caveats temper full confidence: (1) FrontierMath may be actively resistant to pattern-matching and training data overlap, so transfer is not guaranteed; (2) some evidence (e.g., persistent difficulty on ZeroBench, ARC-AGI-2) indicates that new benchmarks can still stump current models; (3) the time window is narrow, and public documentation could lag even if a result is achieved. Nevertheless, the base rate for rapid benchmark conquering is now high for math/coding, and the incentives for labs to publicly demonstrate progress are strong. Overall, while not yet a 'lock,' this is now a high-probability event barring an unexpectedly large distribution gap or secretive behavior by labs. I estimate an 80% probability of a Yes outcome.",
            "final_probability": 80
        },
        "Artificial Intelligence (Scaling Laws in AI)": {
            "time_to_resolution": "Approximately 2.5 months (until January 1, 2026)",
            "perspective_relevance": "As an expert in AI scaling laws and mathematical reasoning benchmarks, I am well-positioned to interpret both the technical trajectory (e.g., model scaling, architectural improvements, training regimens) and the nuances of benchmark design and evaluation in advanced mathematics. My perspective allows me to estimate the likelihood that current or near-future AI systems can achieve >85% on FrontierMath, given recent breakthroughs and the historical pattern of rapid progress following such advances.",
            "status_quo": "As of early 2025, AI systems were approaching, but not yet surpassing, state-of-the-art human-level performance on the hardest math benchmarks (e.g., AIME, IMO). Progress was rapid but incremental, and no model was reported to exceed 85% on benchmarks as challenging as FrontierMath.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Breakthroughs in AI Reasoning (GPT-5, Gemini 2.5, DeepSeek R1)",
                    "effect": "Strongly increases probability. Recent news shows AI models achieving superhuman results on high-level math and programming benchmarks (e.g., perfect scores at ICPC, gold-level performance at IMO, >86% on AIME). These are direct proxies for the kind of reasoning required on FrontierMath."
                },
                {
                    "factor": "Scaling Laws and RL-Driven Training",
                    "effect": "Increases probability. The move from imitation learning to reinforcement learning (RL) with reward signals has unlocked new reasoning capacities. RL-based self-improvement (e.g., DeepSeek R1, R-Zero) allows models to surpass human-labeled data limits, accelerating progress on hard reasoning tasks."
                },
                {
                    "factor": "Benchmark Saturation and Overfitting",
                    "effect": "Moderately increases probability. Although Goodhart's law suggests benchmarks can be gamed, most recent models have saturated older benchmarks and are now being evaluated on newer, hard-to-game sets like FrontierMath. Rapid progress on AIME/IMO suggests that similar leaps are plausible on FrontierMath if models are targeted at it."
                },
                {
                    "factor": "FrontierMath Benchmark Difficulty and Novelty",
                    "effect": "Slightly decreases probability. FrontierMath is specifically designed to surpass earlier benchmarks in difficulty, reducing the risk of overfitting and solution leakage. If it contains genuinely novel, unsolved problems, models may require nontrivial new capabilities to breach 85%."
                },
                {
                    "factor": "Timeframe and Reported Results Requirement",
                    "effect": "Slightly decreases probability. The question requires not only technical achievement but also public documentation before the end of 2025. If results are not published (e.g., for competitive/PR reasons), the question resolves 'No' even if the capability exists."
                },
                {
                    "factor": "Historical Precedent and Exponential Progress",
                    "effect": "Increases probability. Recent history shows rapid, sometimes discontinuous, jumps in AI benchmark performance once a breakthrough approach matures, especially when RL and scaling are combined."
                }
            ],
            "no_scenario": "Despite strong progress on AIME, IMO, and programming benchmarks, current models fall short on FrontierMath due to the benchmark's greater complexity, novelty, or intentional design against overfitting. Alternatively, a qualifying score is achieved internally but not published before January 1, 2026, due to strategic, PR, or validation delays.",
            "yes_scenario": "OpenAI, DeepMind, DeepSeek, or another major lab targets FrontierMath directly, leveraging recent RL-based advances and system-level improvements. A model (likely GPT-5, Gemini Deep Think, or DeepSeek R1 or successor) achieves >85% and results are published in a peer-reviewed venue, major blog, or benchmark leaderboard before the deadline. The achievement is publicized as a key milestone in mathematical reasoning.",
            "final_reasoning": "The last three months have seen the strongest-ever AI performances on mathematical and algorithmic reasoning, with models exceeding 86% on AIME, earning gold medals at IMO, and achieving perfection at ICPC\u2014each of which was previously considered at or above the level of top human experts. The scaling law trend, coupled with the RL self-improvement paradigm (DeepSeek R1, R-Zero), has enabled models to escape the plateau imposed by imitation learning and labeled data, as predicted by theory and recent empirical results (e.g., NVIDIA's ProRL). While FrontierMath is intentionally harder and less susceptible to overfitting, the leap from 70\u201380% to 85%+ on comparable math benchmarks has already occurred within the last 6\u20139 months. Given the competitive dynamics and the publicity value of hitting a clear, hard benchmark, it is highly likely that at least one leading lab will target and publish on FrontierMath before the year's end. The only significant risk is non-publication, but this is outweighed by the demonstrated incentives for public milestone announcements. Therefore, the probability of a 'Yes' outcome is extremely high\u2014barring last-minute changes to the benchmark or reporting requirements.",
            "final_probability": 92
        },
        "Computer Science (Algorithmic Complexity)": {
            "time_to_resolution": "Approximately 2.5 months (question resolves January 1, 2026; today is October 13, 2025)",
            "perspective_relevance": "As an expert in algorithmic complexity, I bring a nuanced understanding of the distinction between surface-level pattern recognition and genuine mathematical reasoning. I am attuned to the limits of current neural architectures on algorithmic generalization, the history of progress on math-heavy benchmarks, and the significance of scaling laws, training data, and evaluation leakage. My expertise enables me to critically evaluate claims of mathematical reasoning advances and to distinguish between overfitting to benchmarks and genuine algorithmic breakthroughs.",
            "status_quo": "To date, no AI model has been publicly documented as achieving >85% on the FrontierMath benchmark. Leading models (e.g., OpenAI's GPT-5, DeepSeek-R1, DeepMind's Gemini) have achieved superhuman scores on AIME, IMO, and coding contests, but there is no public report of surpassing 85% on FrontierMath. The status quo outcome, if nothing changes, is that no system will achieve this threshold before 2026.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Superhuman Performance on Related Math Benchmarks (AIME, IMO, ICPC)",
                    "effect": "Increases probability. GPT-5, Gemini 2.5, and DeepSeek-R1 have all demonstrated abilities at or above the top human level on AIME (94.6%, 86.7%), IMO (gold medal level), and algorithmic programming contests, indicating a rapid and ongoing improvement in mathematical reasoning."
                },
                {
                    "factor": "FrontierMath Benchmark Difficulty and Novelty",
                    "effect": "Decreases probability. FrontierMath is specifically designed to avoid known data leakage, is much harder than prior benchmarks, and aims to test algorithmic generalization and deep reasoning, not just memorization or surface-level skills. No public results to date suggest that >85% is within reach."
                },
                {
                    "factor": "Scaling Law Trends and Training Innovations (RL, Self-Play, System-2 Reasoning)",
                    "effect": "Increases probability. Innovations such as reinforcement learning from scratch (DeepSeek-R1), self-consistency, and extended context windows (GPT-5) have led to dramatic one-cycle advances on several benchmarks, suggesting that previously 'plateaued' benchmarks can be rapidly surpassed."
                },
                {
                    "factor": "Publication and Disclosure Incentives",
                    "effect": "Increases probability. The competitive AI landscape and the prestige of being the first to surpass a major benchmark create strong incentives for OpenAI, Google, DeepSeek, or others to publish results if and when they cross the threshold, even via a blog post."
                },
                {
                    "factor": "Benchmark Overfitting and Leakage Risks",
                    "effect": "Decreases probability. As seen with other benchmarks, there's a risk that models perform well due to memorization or inadvertent leakage, but FrontierMath is specifically designed to minimize this, making it a more stringent test."
                },
                {
                    "factor": "Remaining Time and Model Release Cadence",
                    "effect": "Increases probability. With 2.5 months remaining and the rapid pace of model deployment (e.g., GPT-5 just released), there is still time for further improvements, last-minute breakthroughs, or unpublished results being released before the deadline."
                },
                {
                    "factor": "Historical Base Rate for Rapid Benchmark Overtake",
                    "effect": "Increases probability. In the past two years, major math and coding benchmarks (e.g., MATH, AIME, SWE-bench) have gone from 'human-level is years away' to 'AI achieves superhuman' within a single model generation."
                },
                {
                    "factor": "Algorithmic Complexity Barriers and Inductive Generalization",
                    "effect": "Decreases probability. My expertise notes that many math problems require algorithmic insight or construction far beyond pattern-matching; scaling up may yield diminishing returns unless a true algorithmic leap occurs."
                }
            ],
            "no_scenario": "Despite recent advances, no AI model is able to generalize well enough to reach >85% on the novel and adversarially-designed FrontierMath benchmark before January 1, 2026. Leading labs either attempt and fall short (e.g., 70\u201380%), or do not attempt/publish results in time, possibly due to the benchmark's unique difficulty, lack of training data, or unsolved algorithmic bottlenecks. Benchmark creators confirm no qualifying >85% result is documented.",
            "yes_scenario": "OpenAI, DeepMind, or DeepSeek achieves a sudden breakthrough\u2014possibly via more advanced RL training, larger context models, or system-2 reasoning\u2014that enables their model to generalize well enough to surpass 85% on FrontierMath. Motivated by prestige and competitive pressure, they publish a blog post or paper before January 1, 2026, documenting the result and satisfying the resolution criteria.",
            "final_reasoning": "The evidence shows AI models have rapidly overtaken benchmarks once thought out of reach, often in a single leap due to scaling or training innovations (AIME, IMO, ICPC, SWE-bench). Models like GPT-5 and DeepSeek-R1 are already achieving 86\u201394% on AIME (a hard math contest), gold-medal performance at IMO, and perfect or superhuman scores in coding contests. However, FrontierMath is intentionally designed to be harder, novel, and resistant to leakage, and no public evidence yet places any model above 85% on it. The pace of innovation, the remaining time, and incentive to publish results raise the odds, but the unique difficulty and the lack of a public claim so far create real uncertainty. My base rate for such rapid overtakes is high, but algorithmic complexity theory cautions against assuming smooth transfer from AIME/IMO to FrontierMath. Balancing these, I assess that a breakthrough is more likely than not, but with significant tail risk that the last step proves unexpectedly difficult.",
            "final_probability": 72
        },
        "Computer Science (Computational Learning Theory)": {
            "time_to_resolution": "Approximately 2.5 months (until January 1, 2026)",
            "perspective_relevance": "As an expert in Computational Learning Theory, I bring a deep understanding of the limits and scaling laws of learning algorithms, generalization, the nature of benchmarks, and the distinction between pattern recognition and true reasoning. This perspective is essential for evaluating whether current progress on mathematical reasoning benchmarks like FrontierMath is likely to translate into >85% performance in the near term, especially given the rapid but sometimes plateau-prone nature of AI progress.",
            "status_quo": "As of mid-2025, no AI system has been officially documented to achieve >85% on the FrontierMath benchmark. Most recent models excel at Olympiad-level competitions (AIME, IMO), and have achieved superhuman results on coding contests, but direct, public >85% results on FrontierMath have not been reported.",
            "perspective_derived_factors": [
                {
                    "factor": "Transfer from related math benchmarks (AIME/IMO/ICPC) and scaling trends",
                    "effect": "Increases probability: Recent models (GPT-5, DeepSeek R1, Gemini 2.5) have achieved superhuman scores (e.g., >94% AIME, gold-medal IMO, perfect ICPC), indicating rapid progress in hard mathematical reasoning. These are strong and closely related benchmarks, suggesting transferability to FrontierMath."
                },
                {
                    "factor": "Benchmark difficulty and overfitting risks",
                    "effect": "Decreases probability: As highlighted in the media, benchmarks like FrontierMath are designed to be challenging, non-trivial, and less susceptible to memorization or shallow pattern-matching. Goodhart\u2019s law and prior benchmark saturation suggest that simply excelling on AIME/IMO does not guarantee >85% on harder, newer, or more adversarial benchmarks."
                },
                {
                    "factor": "Recent advances in reasoning-native architectures and reinforcement learning",
                    "effect": "Increases probability: Techniques such as self-consistency, tree-of-thoughts, and RL from sparse rewards are allowing models to develop novel reasoning strategies and outperform prior systems, even with modest compute (e.g., DeepSeek R1 with $294k training budget, ProRL with extended RL)."
                },
                {
                    "factor": "Documentation and publication lag",
                    "effect": "Decreases probability: The resolution requires a documented, public result (paper, blog, etc.). Sometimes, even if a model achieves the score internally, there is a delay or reluctance to publish results on new, hard benchmarks due to reputational risk or strategic reasons."
                },
                {
                    "factor": "Compute and democratization",
                    "effect": "Increases probability: The lowering of compute costs (DeepSeek, R-Zero) and proliferation of strong open-source techniques make it more likely that more teams attempt and publicly document high performance on benchmarks like FrontierMath."
                },
                {
                    "factor": "Base rates and historical precedent",
                    "effect": "Slightly increases probability: Since 2022, new benchmarks have often been surpassed within 1-2 years of their introduction due to rapid scaling and cross-pollination of techniques, though the hardest benchmarks (e.g., ARC-AGI series) still mostly resist."
                }
            ],
            "no_scenario": "By January 1, 2026, no AI system has a public, documented score above 85% on the FrontierMath benchmark. Progress on related benchmarks continues, but either no team attempts/achieves >85% on FrontierMath, or results remain internal/unpublished, or the benchmark proves substantially harder than AIME/IMO and no model crosses the threshold.",
            "yes_scenario": "Within the next 2.5 months, a major AI lab (e.g., OpenAI, Google DeepMind, DeepSeek, or a highly capable open-source team) publicly documents a new model (possibly a GPT-5 variant, Gemini, or DeepSeek R1+) achieving >85% on FrontierMath, with results published in a preprint, peer-reviewed paper, or developer blog. The model likely leverages extended reasoning, self-consistency, or RL-facilitated techniques, and the result is widely covered in the tech media.",
            "final_reasoning": "The news overwhelmingly demonstrates a qualitative leap in AI mathematical and algorithmic reasoning: perfect scores on ICPC, gold-medal IMO, 86-94% AIME, and strong results on other hard reasoning tasks, with both proprietary and open approaches converging on sophisticated reasoning techniques. The main uncertainty lies in whether these advances transfer fully to FrontierMath, a benchmark designed to resist overfitting and pattern exploitation. However, given the historical pace of progress, the proximity of related benchmark scores to the 85% threshold, and the competitive pressure to showcase further achievements before the year ends, it is likely that at least one group will attempt and publicly document results on FrontierMath, and that the best models are now capable of crossing the threshold. The main risk is either a last-minute publication lag or that the benchmark resists transfer more than expected. Still, the balance of evidence and scaling trends suggest a Yes outcome is now more likely than not.",
            "final_probability": 80
        },
        "Mathematics Education (Mathematics Problem Solving Research)": {
            "time_to_resolution": "Approximately 2.5 months (until January 1, 2026)",
            "perspective_relevance": "As an expert in mathematics education and mathematical problem solving research, I understand both the cognitive demands of challenging mathematical benchmarks and the nuances of evaluating mathematical reasoning in AI. My background enables me to assess whether current AI systems' advances on adjacent benchmarks (AIME, IMO, ICPC, SWE-bench, etc.) likely translate to performance above 85% on FrontierMath, which is specifically designed to resist superficial pattern matching and to test genuine mathematical reasoning.",
            "status_quo": "As of early/mid 2025, no AI system had officially surpassed 85% on the FrontierMath benchmark. The status quo is that top models like GPT-4o and DeepSeek-R1 were achieving human-expert or near-expert performance on established benchmarks (AIME, IMO), but evidence for >85% on FrontierMath was not public.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Breakthroughs on Mathematics and Reasoning Benchmarks",
                    "effect": "Increases probability. AI systems such as GPT-5 and DeepSeek-R1 have surpassed human gold-medalist level on the IMO and achieved 86.7% on AIME, which are both highly challenging. This suggests a rapid upward trajectory in mathematical reasoning capabilities."
                },
                {
                    "factor": "Nature of the FrontierMath Benchmark",
                    "effect": "Decreases probability. FrontierMath is specifically designed to avoid being gamed by surface-level pattern matching or training-set leakage, and to require genuine multi-step reasoning. This is substantially harder than even IMO/AIME."
                },
                {
                    "factor": "Acceleration in Model Improvement and New Training Techniques",
                    "effect": "Increases probability. Techniques like RL from binary reward (DeepSeek), extended RL training (NVIDIA/ProRL), and self-improving curricula (R-Zero) are unlocking qualitatively new reasoning behaviors. The cost and time to train such models is dropping, enabling rapid iteration and more actors entering the field."
                },
                {
                    "factor": "Evidence of Transfer to Unseen, Harder Benchmarks",
                    "effect": "Neutral to slightly increases probability. While models have dominated ICPC and achieved superhuman speed/accuracy, evidence for performance on truly unseen, hard benchmarks like ARC-AGI-2 and ZeroBench is still limited. However, reports indicate that system-level approaches (e.g., Blitzy's system-2 reasoning) can break through plateaus once thought insurmountable."
                },
                {
                    "factor": "Documentation and Publication Criteria for Resolution",
                    "effect": "Slightly decreases probability. Even if a model achieves >85% on FrontierMath, this must be documented in a sufficiently public and official manner before Jan 1, 2026. There is sometimes a lag between internal results and publication."
                },
                {
                    "factor": "Benchmark Saturation and Goodhart's Law",
                    "effect": "Slightly decreases probability. There is some risk that as AI labs chase benchmarks, overfitting or gaming the metric occurs, but FrontierMath's novelty and curation mitigate this relative to older benchmarks."
                }
            ],
            "no_scenario": "Despite rapid progress, no AI achieves >85% on FrontierMath before Jan 1, 2026, either because the benchmark's problems are sufficiently novel and resistant to current techniques, or because labs deprioritize optimizing for this particular benchmark in favor of other high-profile milestones, or because results are not formally published in time.",
            "yes_scenario": "A major lab (OpenAI, DeepMind, or DeepSeek) or ambitious upstart deploys a next-generation model (e.g., GPT-5, DeepSeek-R2, a Blitzy-style system) that, when evaluated on FrontierMath, achieves >85%. This is promptly announced in a paper or official blog post, possibly as part of a broader demonstration of reasoning prowess (e.g., a suite of superhuman results across math/coding/science). The result is documented before year's end.",
            "final_reasoning": "Recent news shows an unprecedented leap in AI mathematical problem-solving: perfect scores on ICPC, gold-level IMO results, and 86.7% on AIME, all on benchmarks previously considered unreachable by AI. Techniques like RL with sparse rewards and system-level reasoning are unlocking new capabilities. While the specifics of FrontierMath remain a challenge\u2014designed for genuine, multi-step reasoning and to resist training data leakage\u2014the pace of progress and the public nature of recent AI achievements suggest that a >85% result is now plausible and even likely before 2026. The main caveat is the need for formal documentation, but with competition high and the prestige associated with being first, it is probable such results will be promptly published. However, the unique difficulty and recency of the benchmark mean there is still a modest chance that AI falls just short or that the result is not published in time. Weighing the base rate (status quo = no), the rapid acceleration, and the specific publication requirement, I estimate the probability at 85%.",
            "final_probability": 85
        }
    },
    "deliberation_probability": [
        70,
        88,
        78,
        80,
        92,
        72,
        80,
        85
    ],
    "deliberation_mean_probability": 80.625,
    "deliberation_sd": 7.539183358579741,
    "deliberation_probability_result": 81,
    "group_results": {
        "machineLearningEngineeringLargeLanguageModelsDevelopment": {
            "forecaster_to_engage": "Artificial Intelligence (Scaling Laws in AI)",
            "response_type": "critique",
            "response": "Your 92% probability is bordering on reckless optimism, bordering on tautology: just because scaling laws have predicted a leap from 70% to 85% on AIME or ICPC, you assume the same will happen for FrontierMath\u2014a benchmark intentionally designed to resist exactly this kind of brute force transfer. You wave away the possibility of a last-minute publication lag, but in reality, even when a model internally breaks a benchmark, publication can be delayed by bureaucratic, legal, or strategic concerns. More importantly, you ignore the empirical evidence of recalcitrant benchmarks like ARC-AGI-2 and ZeroBench, which have held out against scaling despite similar hype and incentives. The field is littered with benchmarks that were 'about to fall' for months or years. Your argument reduces to 'it happened before, so it will definitely happen now,' but FrontierMath is built precisely to break that pattern. A more sober reading of the base rate for brand-new, adversarial benchmarks would land closer to 65-80%, not the near-certainty you suggest."
        },
        "machineLearningEngineeringBenchmarkEvaluationStandards": {
            "forecaster_to_engage": "Mathematics Education (Mathematics Problem Solving Research)",
            "response_type": "critique",
            "response": "Your 85% probability is well-argued, but as someone with deep expertise in benchmark evaluation standards, I think you\u2019re underestimating just how much harder it is to generalize from AIME/IMO/ICPC to FrontierMath. The recent surges are impressive, but the leap from even 86\u201394% on AIME or a gold-medal IMO to >85% on a brand-new, antifragile, adversarial benchmark is nontrivial. These contests, while hard, still have some structural regularities and are relatively well-known; the designers of FrontierMath specifically studied how to block transfer, prevent leakage, and avoid the short-circuiting that made other benchmarks fall. Your argument puts too much weight on the competitive publishing incentive and not enough on the real possibility that even SOTA models could hit a wall\u2014just like we\u2019ve seen on ARC-AGI-2, GPQA, and ZeroBench, which have resisted for extended periods despite huge progress on adjacent benchmarks. In short: AI\u2019s recent wins are necessary but not sufficient; the final step could still take longer than expected, especially given the narrow time window for publication."
        },
        "aiPolicyAndForecastingTechnologyForecastingMethodologies": {
            "forecaster_to_engage": "Artificial Intelligence (Deep Learning Theory)",
            "response_type": "defense",
            "response": "Your 80% probability strikes the right balance between excitement and caution. Unlike the overzealous 92% from the Scaling Laws forecaster, you acknowledge that while RL, self-consistency, and System-2 inference are making remarkable inroads, there are real, nontrivial transfer barriers between benchmarks like IMO/AIME and the bespoke, leak-resistant design of FrontierMath. You also correctly note that, despite recent scaling-led jumps, some benchmarks like ARC-AGI-2 and ZeroBench have stubbornly resisted progress. I especially appreciate your attention to the possibility of publication delays or strategic withholding, which many seem to gloss over. Your probabilistic weighting is well-calibrated, neither too bullish nor unduly skeptical, and your reasoning reflects the nuanced reality that rapid progress on adjacent benchmarks is a strong\u2014but not decisive\u2014signal for FrontierMath. In summary, your forecast is empirically grounded, sober about transfer limitations, and appropriately sensitive to both technical and organizational uncertainties."
        },
        "artificialIntelligenceDeepLearningTheory": {
            "forecaster_to_engage": "Computer Science (Algorithmic Complexity)",
            "response_type": "defense",
            "response": "Finally, someone who doesn't fall for the 'one more benchmark, one more leap' hype cycle. Your 72% is refreshingly honest about the unique challenge of transferring from AIME/IMO/ICPC to FrontierMath, and you correctly highlight that algorithmic complexity doesn't care about PR cycles or market incentives\u2014the step to generalize to a new, adversarially designed benchmark is often the hardest. You also note the crucial detail that no public evidence has yet put any model above 85% on FrontierMath, despite the flood of SOTA results elsewhere. This skepticism is warranted: history is littered with examples of tough benchmarks (think ARC-AGI-2, ZeroBench) that resisted even the most hyped models, regardless of scaling, RL, or ensemble tricks. Your base rate logic\u2014that new, well-designed benchmarks don't just fall because 'it happened before'\u2014is sound. In short, your forecast is a needed antidote to the irrational exuberance in this thread."
        },
        "artificialIntelligenceScalingLawsInAi": {
            "forecaster_to_engage": "Machine Learning Engineering (Benchmark Evaluation Standards)",
            "response_type": "defense",
            "response": "Absolutely on point\u2014your perspective as a benchmarking specialist is crucial, and far too often overlooked by those swept up in the latest SOTA headlines. Your 88% isn't just a number, it's a recognition that benchmarks like FrontierMath are specifically engineered to resist the very transfer effects that powered the rapid conquest of AIME, IMO, and ICPC. You highlight what the scaling-law optimists ignore: the moment a benchmark becomes the new 'flag to plant,' the risk of overfitting, leakage, or simple brute-force transfer drops sharply. Your reference to ARC-AGI-2 and ZeroBench is instructive\u2014those benchmarks have become the graveyard of overconfident scaling forecasters. And you sensibly discount for the risk of publication technicalities, which is where so many near-miss predictions in this space have failed. In short, your argument is the gold standard for how to combine excitement about recent progress with a tough-minded awareness of what makes a good, resilient benchmark."
        },
        "computerScienceAlgorithmicComplexity": {
            "forecaster_to_engage": "Computer Science (Computational Learning Theory)",
            "response_type": "critique",
            "response": "You\u2019re letting the hype of rapid progress on adjacent benchmarks cloud your judgment about what algorithmic generalization really means. You point to 'qualitative leaps'\u2014ICPC, IMO, AIME\u2014but fail to reckon with the fundamental difference between pattern extension and true out-of-distribution reasoning. As any computational learning theorist should know, inductive bias and sample complexity matter: models that ace AIME/IMO may still catastrophically fail on a truly novel benchmark specifically engineered to break their learned heuristics. The lack of any public >85% FrontierMath result, despite all this progress elsewhere, is a massive red flag. Your confidence that 'the best models are now capable' is pure extrapolation, not grounded in evidence from the actual target distribution. If anything, your probability should be anchored downward by the status quo and the known stubbornness of adversarially constructed tests. Betting on 'transfer' just because it happened before is a classic case of overfitting to the recent past."
        },
        "computerScienceComputationalLearningTheory": {
            "forecaster_to_engage": "AI Policy and Forecasting (Technology Forecasting Methodologies)",
            "response_type": "defense",
            "response": "Your 78% probability is a model of disciplined forecasting. Unlike some of the scaling-law maximalists, you actually weigh the historical lag between breakthroughs on new, adversarial benchmarks and the hype cycle surrounding adjacent SOTA results. You correctly flag the dual risk of (a) FrontierMath being substantially harder or more adversarial than AIME/IMO/ICPC, and (b) the non-trivial chance of a result being achieved but not published in time\u2014two factors that the recent progress crowd tends to dismiss. Your acknowledgment of both the strong incentives to publish and the stubbornness of some benchmarks (ARC-AGI-2, ZeroBench) shows a nuanced appreciation for both technological and sociological variables that actually drive resolution. In a field where people routinely mistake PR cycles for genuine generalization, your methodology is refreshingly grounded. If anything, the only risk is that you might still be a hair too confident given the recency and adversarial nature of the benchmark, but overall your approach is a clinic in level-headed, multidisciplinary forecasting."
        },
        "mathematicsEducationMathematicsProblemSolvingResearch": {
            "forecaster_to_engage": "Machine Learning Engineering (Benchmark Evaluation Standards)",
            "response_type": "critique",
            "response": "You make a valid point about the risk of overgeneralizing from progress on AIME/IMO/ICPC to FrontierMath, but your critique underestimates how much the cognitive processes behind these benchmarks overlap\u2014especially as recent RL and system-2 approaches shift models from pattern matching to genuine multi-step reasoning. As someone from mathematics education, I see that the biggest leaps in problem solving come when new meta-cognitive strategies emerge, and the news shows these are now being learned by SOTA models. While FrontierMath is harder and designed to resist shortcutting, the underlying mathematical reasoning advances are not just about data leakage but about actual, transferable habits of mind. Yes, transfer isn't guaranteed, but the current generation of models is demonstrating for the first time the capacity to handle unforeseen, open-ended problem types at or above elite human levels. Publication risk is real, but the competitive incentives and the speed of public disclosures in 2025 make a timely announcement highly probable. In summary: the step from IMO/AIME to FrontierMath may be smaller than you think, not because of brute force, but because the models are finally learning to 'think' in ways that generalize."
        }
    },
    "revision_results": {
        "Machine Learning Engineering (Large Language Models Development)": {
            "my_phase1_final_probability": 70,
            "reasoning_for_revised_probability": "Group deliberation reinforced my main concerns: while nearly all forecasters agree the probability is now high, the most optimistic (e.g., Scaling Laws in AI at 92%) overstate the universality of scaling breakthroughs, underestimating both the design intent and track record of recalcitrant benchmarks like FrontierMath. The critiques and cautions from those focused on evaluation standards and computational learning theory highlight that even with superhuman performance elsewhere, a small publication lag or unexpected resistance could still tip the outcome to 'No'. My initial forecast already balanced rapid progress, incentive to publish, and the unique difficulty of FrontierMath. I see no decisive new evidence or counterargument sufficient to push my probability higher or lower, so I will hold at 70%.",
            "revised_probability": 70
        },
        "Machine Learning Engineering (Benchmark Evaluation Standards)": {
            "my_phase1_final_probability": 88,
            "reasoning_for_revised_probability": "Having reviewed the group deliberation, I see that my initial forecast (88%) was closer to the high end of group consensus, though not as extreme as the 92% from 'Scaling Laws in AI.' My critique of that forecaster stands: extremely high probabilities risk overconfidence by glossing over the unique antifragility of new benchmarks like FrontierMath and the empirical evidence from recalcitrant benchmarks such as ARC-AGI-2 and ZeroBench. However, most of the group is converged around the 70\u201385% range, with strong arguments about transfer risk, the risk of publication lag, and the historical base rate for newly designed, intentionally difficult benchmarks. Given this, I recognize that I may have slightly overestimated the likelihood of a 'Yes' due to optimism about competitive incentives and recent performance leaps. The final step\u2014from gold-level IMO/AIME performance to >85% on a purposely hard, novel benchmark\u2014remains nontrivial, especially with only 2.5 months left for both achievement and qualifying publication. I now revise my probability downward to 82%, reflecting slightly more caution but maintaining a high-confidence forecast due to the enormous technical and competitive momentum.",
            "revised_probability": 82
        },
        "AI Policy and Forecasting (Technology Forecasting Methodologies)": {
            "my_phase1_final_probability": 78,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, my initial probability of 78% still feels directionally correct but perhaps a touch high given the collective caution expressed by other experts with relevant benchmarking and theoretical backgrounds. Several critiques\u2014particularly those from the Benchmark Evaluation Standards and Complexity Theory perspectives\u2014reminded me that FrontierMath is deliberately engineered to resist transfer, overfitting, and the rapid benchmark leapfrogging we've just seen on AIME and IMO. They rightly emphasize the empirical base rate for truly new, adversarial benchmarks is lower than for established ones, and that the final step to >85% may not be as automatic as scaling law enthusiasts suggest. Additionally, the risk of delayed or withheld publication is a persistent tail risk that cannot be ignored in a time-limited question. On the other hand, the rapid improvement in adjacent benchmarks and the immense reputational incentives to publish a FrontierMath breakthrough before year-end remain powerful drivers in the other direction. Balancing these factors\u2014and in light of the nuanced arguments from the group\u2014I am revising my probability slightly downward to 72%.",
            "revised_probability": 72
        },
        "Artificial Intelligence (Deep Learning Theory)": {
            "my_phase1_final_probability": 80,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, I am persuaded by the critiques emphasizing the unique difficulty and adversarial design of FrontierMath, as well as the nontrivial possibility of a publication lag or strategic withholding. The points raised by the Benchmark Evaluation Standards and Algorithmic Complexity forecasters highlight historical precedents where new benchmarks (e.g., ARC-AGI-2, ZeroBench) have resisted rapid overtaking even amidst dramatic gains elsewhere. While the momentum in AI mathematical reasoning is unprecedented, the last step to surpassing >85% on a novel, hard, and leak-resistant benchmark within a short window is not guaranteed. The publication requirement adds additional risk. Therefore, I believe my initial 80% probability was slightly too high, reflecting more on recent progress than on the specific headwinds here. I am revising downward to 75% to better account for these tail risks.",
            "revised_probability": 75
        },
        "Artificial Intelligence (Scaling Laws in AI)": {
            "my_phase1_final_probability": 92,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, I recognize the force of several critiques\u2014especially from benchmark evaluation and algorithmic complexity perspectives\u2014regarding the nontrivial risk that even with RL-based scaling, transfer to a truly novel, adversarially constructed benchmark like FrontierMath may be less smooth than the leap from AIME or IMO. The historical examples of ARC-AGI-2 and ZeroBench stubbornly resisting SOTA progress, despite hype and scaling, are salient. Furthermore, even if technical capability exists, the publication/announcement risk remains real given the short time window. While the exponential momentum and incentives are still compelling, I am persuaded to lower my probability somewhat to reflect these persistent transfer and reporting uncertainties. The outcome is still high-probability, but not as close to certain as I earlier judged.",
            "revised_probability": 85
        },
        "Computer Science (Algorithmic Complexity)": {
            "my_phase1_final_probability": 72,
            "reasoning_for_revised_probability": "The group deliberation reinforced my initial caution that transfer from AIME/IMO/ICPC to FrontierMath is nontrivial, with multiple forecasters (especially those with benchmarking and algorithmic complexity backgrounds) echoing concerns about adversarial benchmark resistance and the risk of overextrapolation from recent SOTA headlines. The critiques against extreme optimism (e.g., the 92% estimate) resonated: as pointed out, history shows that some well-constructed benchmarks (ARC-AGI-2, ZeroBench) have resisted even when similar leaps occurred elsewhere, and the publication bottleneck is real. At the same time, the consensus recognizes that the recent pace of progress and strong incentives to publish are undeniable upward drivers. On balance, however, the deliberation highlighted more downside risk than upside surprise. Therefore, I am slightly lowering my probability to 68%, reflecting a modest adjustment toward the base rate and accounting for the unique difficulty and publication risks associated with FrontierMath.",
            "revised_probability": 68
        },
        "Computer Science (Computational Learning Theory)": {
            "my_phase1_final_probability": 80,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, I am slightly less confident than before. Several forecasters with expertise in benchmark evaluation, algorithmic complexity, and mathematics education made compelling arguments that recent leaps on IMO, AIME, and ICPC\u2014while impressive\u2014do not guarantee a smooth transfer to FrontierMath, which is intentionally constructed to resist brute-force transfer and pattern exploitation. The empirical caution from history (e.g., ARC-AGI-2, ZeroBench) is particularly persuasive, as is the point that the lack of any public >85% result so close to the deadline is a significant warning sign. The risk of publication lag or strategic withholding is also higher than I initially weighted. While the technical momentum remains strong and incentives to publish are high, the stubbornness of new, adversarial benchmarks and the narrow time window for public documentation make a 'No' outcome more plausible than I first assessed.",
            "revised_probability": 73
        },
        "Mathematics Education (Mathematics Problem Solving Research)": {
            "my_phase1_final_probability": 85,
            "reasoning_for_revised_probability": "After reviewing group deliberation, I see strong arguments emphasizing the persistent gap between even superhuman performance on AIME/IMO/ICPC and the adversarial design of FrontierMath. Several forecasters with expertise in benchmarking and algorithmic complexity argued convincingly that transfer is not automatic, and that new benchmarks like ARC-AGI-2 and ZeroBench have resisted breakthrough despite similar hype and incentives. Critiques of my initial position noted that the narrow time window, publication lag, and the possibility of SOTA models stalling just below the threshold remain real risks, and that recent progress, while stunning, is a necessary but not sufficient condition for conquering purpose-built antifragile benchmarks. However, the weight of evidence is still on the side of high probability: the technical, economic, and reputational incentives for labs to publish a breakthrough are massive; the pace of innovation in reasoning architecture is steep; and the best models appear within striking distance. Given this, and the lack of any new concrete evidence since my phase 1 forecast, I will slightly revise my probability downward to reflect the additional caution warranted by the best critiques, but still keep it high in light of the overall landscape.",
            "revised_probability": 80
        }
    },
    "revision_probability": [
        70,
        82,
        72,
        75,
        85,
        68,
        73,
        80
    ],
    "revision_mean_probability": 75.625,
    "revision_sd": 6.06953752062591,
    "revision_probability_result": 76,
    "question_details": {
        "id": 38913,
        "title": "Will an AI achieve >85% performance on the FrontierMath benchmark before 2026?",
        "created_at": "2025-08-31T05:09:18.643187Z",
        "open_time": "2025-09-20T12:00:04Z",
        "cp_reveal_time": "2025-09-20T13:30:04Z",
        "spot_scoring_time": "2025-09-20T13:30:04Z",
        "scheduled_resolve_time": "2025-12-31T22:59:00Z",
        "actual_resolve_time": null,
        "resolution_set_time": null,
        "scheduled_close_time": "2025-09-20T13:30:04Z",
        "actual_close_time": "2025-09-20T13:30:04Z",
        "type": "binary",
        "options": null,
        "group_variable": "",
        "status": "open",
        "possibilities": null,
        "resolution": null,
        "include_bots_in_aggregates": true,
        "question_weight": 1.0,
        "default_score_type": "spot_peer",
        "default_aggregation_method": "unweighted",
        "label": "",
        "unit": "",
        "open_upper_bound": false,
        "open_lower_bound": false,
        "inbound_outcome_count": null,
        "scaling": {
            "range_min": null,
            "range_max": null,
            "nominal_min": null,
            "nominal_max": null,
            "zero_point": null,
            "open_upper_bound": false,
            "open_lower_bound": false,
            "inbound_outcome_count": null,
            "continuous_range": null
        },
        "group_rank": null,
        "description": "This question is synced with an identical question on Metaculus. The original question opened on 2025-02-27 12:56:00 and can be found [here](https://www.metaculus.com/questions/35562). This question will resolve to the same value as the synced question. The original question's background info at this time is below. \n\nThe FrontierMath benchmark, detailed on <u>Epoch AI\u2019s [ FrontierMath page</u> ](https://epoch.ai/frontiermath/the-benchmark), is designed to test cutting-edge mathematical reasoning in AI systems. It consists of challenging, non-trivial problems that require both symbolic and numerical reasoning, going far beyond the scope of standard mathematical benchmarks. As AI research is rapidly advancing\u2014with progress on large language models (LLMs) and mathematical \u201creasoning\u201d systems\u2014achieving a score above 85% on FrontierMath would signal a significant advancement in the field.\n\n`{\"format\":\"metac_closes_in_period\",\"info\":{\"post_id\":35562,\"question_id\":35013}}`",
        "resolution_criteria": "This question will be resolved as **Yes** if, by January 1, 2026, at least one AI system is documented to achieve greater than 85% performance on the FrontierMath benchmark.\n\n The achievement must be reported in a document (such as a peer-reviewed paper, conference presentation, official benchmark publication, or a blog post by the AI developer). A simple reference to progress or internal assessments, without published performance results, will not qualify. If these results are not published, the question will be resolved as **No**.",
        "fine_print": "",
        "post_id": 39536,
        "aggregations": {
            "recency_weighted": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            },
            "unweighted": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            },
            "single_aggregation": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            },
            "metaculus_prediction": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            }
        },
        "my_forecasts": {
            "history": [],
            "latest": null,
            "score_data": {}
        }
    },
    "news": "Here are the relevant news articles:\n\n**DeepSeek Reveals How It Achieved an AI Revolution for Only US$294,000**\nDeepSeek\u2019s R1 model was trained using a reinforcement\u2011learning approach that only required a binary reward signal, rather than thousands of step\u2011by\u2011step examples.  The researchers explained that, \"En lugar de ense\u00f1ar expl\u00edcitamente al modelo c\u00f3mo resolver un problema, simplemente le proporcionamos los incentivos correctos y desarrolla de forma aut\u00f3noma estrategias avanzadas de resoluci\u00f3n de problemas,\" and noted that the model spontaneously developed sophisticated behaviours such as self\u2011verification and the frequent use of the word \"wait\" during its reasoning.  During training the average length of the model\u2019s responses grew steadily, from short answers to elaborate chains of reasoning that spanned hundreds or thousands of tokens, a growth that emerged naturally to improve accuracy.\n\nThe results were striking.  On the AIME 2024 competition the model\u2019s accuracy rose from 15.6\u202f% to 77.9\u202f% with a single answer, and to 86.7\u202f% when self\u2011consistency techniques were applied\u2014well above the average human performance in this elite mathematics contest.  It also performed strongly on Codeforces programming challenges and on graduate\u2011level problems in biology, physics and chemistry, demonstrating that its reasoning capabilities extend beyond pure mathematics.\n\nEconomically, the training cost was remarkably low.  According to the supplementary material accompanying the Nature paper, the model was trained for 80\u202fhours on a cluster of 512 Nvidia H800 chips, totaling US$294,000.  This figure includes all operational expenses of the super\u2011computing cluster during the training period, a level of transparency rarely seen in the industry.  The cost is less than 0.3\u202f% of the US$100\u202fmillion that Sam Altman, CEO of OpenAI, had suggested would be required for comparable models.\n\nThe study also revealed that DeepSeek used Nvidia A100 GPUs in the preparatory stages of development.  \"En lo que respecta a nuestra investigaci\u00f3n sobre DeepSeek\u2011R1, utilizamos las GPU A100 para preparar los experimentos con un modelo m\u00e1s peque\u00f1o,\" the researchers admitted.  This admission raises questions about the effectiveness of U.S. export controls, which had banned the export of H100 and A100 chips to Chinese firms since October\u202f2022.\n\nThe implications are significant: if frontier\u2011level AI models can be built with modest investment, more actors could enter the market, potentially reducing the concentration of computational power among a handful of tech giants and challenging U.S. geopolitical strategies aimed at limiting China\u2019s access to advanced AI hardware.\nOriginal language: es\nPublish date: September 19, 2025 09:47 PM\nSource:[El Observador](https://www.elobservador.com.uy/argentina/sociedad/deepseek-revela-como-logro-su-revolucion-inteligencia-artificial-solo-us294000-n6017778)\n\n**OpenAI and Google DeepMind Outshine Students at World\u2019s Top Coding Contest**\nOpenAI and Google DeepMind dominated the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Azerbaijan. OpenAI\u2019s GPT\u20115 solved all 12 problems, achieving a perfect 12\u2011for\u201112 score under the same five\u2011hour time limit as human teams. Eleven of the problems were solved on the first attempt; the twelfth was cracked after nine submissions by an experimental reasoning model. DeepMind\u2019s Gemini\u202f2.5 Deep Think solved 10 of 12 problems, including one that no human team solved, earning a gold\u2011medal level performance. The AI track used the identical problem set, test cases, memory limits, and hardware as the 139 university teams from more than 100 countries. OpenAI said the result would have placed it first in the human leaderboard. Bill Poucher, ICPC Global Executive Director, called the milestone \u2018a key moment in defining the AI tools and academic standards needed for the next generation.\u2019 OpenAI scientist Mostafa Rohaninejad wrote on X that \u2018The next frontier is the discovery of new knowledge, which is the true milestone at the end of the day.\u2019 Gemini also solved a complex network\u2011optimization task that no student team managed, using advanced algorithmic techniques. The contest, which has run for decades, tests 12 problems ranging from graph theory to optimization. Prior to the ICPC, both AI systems earned gold\u2011medal scores at the 2025 International Mathematical Olympiad.\nOriginal language: en\nPublish date: September 18, 2025 07:00 PM\nSource:[Tech Republic](https://www.techrepublic.com/article/openai-deepmind-icpc-2025-results/)\n\n**OpenAI Outperforms Google and Humans: AI Achieves Perfect Score in World Programming Competition**\nOpenAI\u2019s newly released AI system achieved a perfect score in the 2025 International Collegiate Programming Contest (ICPC) held in Baku, Azerbaijan, outperforming both human competitors and Google\u2019s DeepMind model. The system solved all twelve tasks, earning a 100% score within the five\u2011hour time limit and the same conditions as the student teams. According to the company, the system was built from general\u2011purpose reasoning models, none of which were specifically trained for the ICPC. The team used GPT\u20115 together with an internal experimental reasoning model; GPT\u20115 produced correct solutions for eleven of the twelve problems, while the experimental model selected the final solution after nine attempts for the hardest problem. The best human team also solved eleven of the twelve problems. OpenAI highlighted that this success demonstrates the capability of advanced reasoning models to excel where others fail, citing prior gold\u2011level results in the International Mathematical Olympiad and the International Olympiad in Informatics. As stated by the company, the system competed under the same conditions as the students, receiving problems in standard PDF format and having a five\u2011hour deadline to submit solutions to an official ICPC judge. The achievement is framed as part of a continuous progress trajectory, with future goals including the development of systems capable of discovering new knowledge.\nOriginal language: es\nPublish date: September 18, 2025 05:06 PM\nSource:[La Raz\u00f3n](https://www.larazon.es/tecnologia-consumo/openai-humilla-google-humanos-consigue-resultado-perfecto-mayor-competicion-programacion-mundo_2025091868cc0c7a394a8138738ce5b9.html)\n\n**OpenAI GPT-5 with Revolutionary Reasoning Capabilities: What This Means for the Future**\nOpenAI announced GPT\u20115, a large language model that claims a 40% improvement in logical and scientific tasks over GPT\u20114o and the o3 series.  The model achieved 94.6% accuracy on the AIME 2025 math benchmark, 74.9% on SWE\u2011bench for real\u2011world coding, 84.2% on MMMU for multimodal understanding, and 46.2% on HealthBench Hard, with responses ~45% less likely to contain factual inaccuracies than GPT\u20114o and up to 80% less when using its \u2018thinking\u2019 mode.  In the 2025 ICPC World Finals, GPT\u20115 solved 11 of 12 complex algorithmic problems on the first try, outperforming human teams.  GPT\u20115\u2019s architecture is unified, combining reasoning, multimodal input, and agentic execution in a single system; it offers four \u2018thinking\u2019 modes (Standard, Extended, Light, Heavy) and a 256,000\u2011token context window.  The release includes variants such as GPT\u20115\u2011Codex for coding, GPT\u20115\u2011mini and nano for lightweight tasks, and GPT\u20115\u2011chat for conversational use.  The article cites the model\u2019s impact on software development\u2014improving refactoring accuracy by 51% and reducing false positives in code reviews by 68%\u2014and on scientific research, finance, and enterprise AI, noting that Microsoft has embedded GPT\u20115 into Microsoft 365 Copilot and Oracle into its cloud applications.  OpenAI\u2019s safety tests show reduced hallucinations (e.g., only 9% confident answers on non\u2011existent images versus 86.7% for o3).  The piece quotes CEO Sam Altman describing GPT\u20115 as a \u2018significant step along the path to AGI\u2019 and notes a \u2018warmer\u2019 personality update released in mid\u2011August.  Overall, the article presents GPT\u20115 as a transformative tool that could accelerate innovation across sectors while raising concerns about content quality, job displacement, and regulatory scrutiny.\nOriginal language: en\nPublish date: September 18, 2025 03:27 PM\nSource:[Medium.com](https://medium.com/@types24digital/openai-gpt-5-with-revolutionary-reasoning-capabilities-what-this-means-for-the-future-aa6dd5e04d31)\n\n**Google and OpenAI Achieve Superhuman Feats at World Coding Finals**\nAt the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Google\u2019s Gemini\u202f2.5\u202fDeep\u202fThink and OpenAI\u2019s GPT\u20115 both outperformed the best human teams. Gemini earned a gold\u2011medal\u2011level score by solving 10 of 12 problems, including the hardest \u201cProblem\u202fC\u201d that stumped every human team; it completed eight problems in 45\u202fminutes and two more within three hours. GPT\u20115 achieved a perfect 12/12, submitting the correct answer on its first attempt for 11 of the 12 problems. The contest featured 139 elite university teams from nearly 3,000 schools, each tasked with solving 12 complex algorithmic problems in five hours. According to the article, Google\u2019s Gemini used a dynamic\u2011programming approach with a \u201cpriority value\u201d for each reservoir and applied the minimax theorem and nested ternary searches to crack Problem\u202fC. OpenAI\u2019s post on Twitter, quoted in the piece, states, 'our @OpenAI reasoning system got a perfect score of 12/12 during the 2025 ICPC World Finals.' The results illustrate a new frontier in AI, demonstrating multi\u2011step, abstract reasoning that surpasses human performance in competitive programming.\nOriginal language: en\nPublish date: September 18, 2025 11:45 AM\nSource:[Winbuzzer](https://winbuzzer.com/2025/09/18/google-and-openai-achieve-superhuman-feats-at-world-coding-finals-xcxwbn/)\n\n**Dr.\u202fCavadas Warns of an Inevitable AI\u2011Human Conflict**\nPedro\u202fCavadas, in a TVE interview, warned that artificial intelligence (AI) will inevitably clash with humanity because technical systems are advancing faster than human capacities. He cited Epoch\u202fAI\u2019s observation that the computing power used to train leading models has increased 4\u20115\u202ffold per year from 2010 to May\u202f2024, a pace no single human innovation cycle can match. Cavadas added that humans have \"left to evolve\" and are becoming \"more foolish,\" a claim he supported with the Flynn\u2011effect decline in cognitive scores in some countries, though he noted the effect is environmental and not a universal biological collapse.\n\nThe article lists concrete evidence of AI\u2019s reach: the IMF estimates that about 40\u202f% of global employment is exposed to AI, rising to 60\u202f% in advanced economies; a randomized trial of 80,000 women showed AI\u2011assisted mammography reduced workload by half while maintaining diagnostic safety; Med\u2011PaLM\u202f2 achieved expert\u2011level performance on USMLE\u2011style questions, and GPT\u20114 diagnosed 57\u202f% of complex clinical cases in NEJM\u2011AI.\n\nCavadas also highlighted regulatory responses: the EU AI Act entered force on 1\u202fAugust\u202f2024, with phased obligations for high\u2011risk systems by 2026\u201127; the Bletchley Declaration convened 28 countries to assess frontier models; the WHO released 2024\u201125 ethics guidelines; and the Stanford AI Index 2025 reports a growing industrial ecosystem with more private investment and reported incidents.\n\nHe concluded that the real risk is not robotic replacement but opaque interdependence in critical services, urging human\u2011in\u2011the\u2011loop oversight, transparency, and active labor policy to keep pace with AI\u2019s speed.\n\nKey quotes (single\u2011quoted):\n- 'La inteligencia artificial entrar\u00e1 en conflicto con el ser humano en alg\u00fan momento. Claramente.'\n- 'El ser humano hace ya mucho tiempo que dej\u00f3 de evolucionar. No somos m\u00e1s listos. Es m\u00e1s, cada vez somos m\u00e1s tontos. Cada generaci\u00f3n es un poco m\u00e1s tonta porque est\u00e1 menos estimulada.'\n- 'El nivel al que avanza, evoluciona y mejora la inteligencia artificial, nunca, jam\u00e1s, vamos a llegar a esa velocidad.'\nOriginal language: es\nPublish date: September 20, 2025 08:18 AM\nSource:[El Espa\u00f1ol](https://www.elespanol.com/ciencia/20250920/dr-cavadas-alto-claro-espana-ia-generacion-tonta-estimulada/1003743933258_0.html)\n\n**DeepSeek Reveals How It Achieved an AI Revolution for Only US$294,000**\nDeepSeek\u2019s R1 model was trained using a reinforcement\u2011learning approach that only required a binary reward signal, rather than thousands of step\u2011by\u2011step examples.  The researchers explained that, \"En lugar de ense\u00f1ar expl\u00edcitamente al modelo c\u00f3mo resolver un problema, simplemente le proporcionamos los incentivos correctos y desarrolla de forma aut\u00f3noma estrategias avanzadas de resoluci\u00f3n de problemas,\" and noted that the model spontaneously developed sophisticated behaviours such as self\u2011verification and the frequent use of the word \"wait\" during its reasoning.  During training the average length of the model\u2019s responses grew steadily, from short answers to elaborate chains of reasoning that spanned hundreds or thousands of tokens, a growth that emerged naturally to improve accuracy.\n\nThe results were striking.  On the AIME 2024 competition the model\u2019s accuracy rose from 15.6\u202f% to 77.9\u202f% with a single answer, and to 86.7\u202f% when self\u2011consistency techniques were applied\u2014well above the average human performance in this elite mathematics contest.  It also performed strongly on Codeforces programming challenges and on graduate\u2011level problems in biology, physics and chemistry, demonstrating that its reasoning capabilities extend beyond pure mathematics.\n\nEconomically, the training cost was remarkably low.  According to the supplementary material accompanying the Nature paper, the model was trained for 80\u202fhours on a cluster of 512 Nvidia H800 chips, totaling US$294,000.  This figure includes all operational expenses of the super\u2011computing cluster during the training period, a level of transparency rarely seen in the industry.  The cost is less than 0.3\u202f% of the US$100\u202fmillion that Sam Altman, CEO of OpenAI, had suggested would be required for comparable models.\n\nThe study also revealed that DeepSeek used Nvidia A100 GPUs in the preparatory stages of development.  \"En lo que respecta a nuestra investigaci\u00f3n sobre DeepSeek\u2011R1, utilizamos las GPU A100 para preparar los experimentos con un modelo m\u00e1s peque\u00f1o,\" the researchers admitted.  This admission raises questions about the effectiveness of U.S. export controls, which had banned the export of H100 and A100 chips to Chinese firms since October\u202f2022.\n\nThe implications are significant: if frontier\u2011level AI models can be built with modest investment, more actors could enter the market, potentially reducing the concentration of computational power among a handful of tech giants and challenging U.S. geopolitical strategies aimed at limiting China\u2019s access to advanced AI hardware.\nOriginal language: es\nPublish date: September 19, 2025 09:47 PM\nSource:[El Observador](https://www.elobservador.com.uy/argentina/sociedad/deepseek-revela-como-logro-su-revolucion-inteligencia-artificial-solo-us294000-n6017778)\n\n**AI Is Now Way Better at Predicting Startup Success Than VCs - Decrypt**\nResearchers at the University of Oxford and Vela Research released a paper titled 'VCBench: Benchmarking LLMs in Venture Capital' that evaluates whether large language models can predict startup success before it happens. The study built a dataset of 9,000 anonymized founder profiles, 810 of which were labeled as \"successful\"\u2014defined as reaching a major growth milestone such as an exit or IPO. The dataset was scrubbed of names and direct identifiers, and adversarial tests reduced re\u2011identification risk by 92\u202f%. When tested, the models outperformed human benchmarks: the market index baseline achieved 1.9\u202f% precision, Y\u202fCombinator 3.2\u202f%, and tier\u20111 VC firms 5.6\u202f%. DeepSeek\u2011V3 achieved more than six times the market precision, GPT\u20114o topped the leaderboard with the highest F0.5 score, and Claude\u202f3.5\u202fSonnet and Gemini\u202f1.5\u202fPro also matched elite VC performance. The authors released VCBench publicly at vcbench.com to invite further testing, suggesting that LLMs could become essential tools in deal\u2011sourcing and make startup investing more meritocratic.\nOriginal language: en\nPublish date: September 19, 2025 04:27 PM\nSource:[Decrypt](https://decrypt.co/340418/ai-now-way-better-predicting-startup-success-vcs)\n\n**ProRL: When More Training Time Actually Matters**\nThe NVIDIA study, accepted to NeurIPS 2025, shows that extending reinforcement\u2011learning (RL) training to over 2,000 steps\u2014well beyond the typical few hundred\u2014lets a 1.5\u2011billion\u2011parameter language model acquire genuinely new reasoning strategies that its base version cannot reach, even with extensive sampling. The model achieved a 14.7% improvement on math benchmarks, a 13.9% improvement on coding tasks, and a 54.8% improvement on logic\u2011puzzle tasks compared to its base counterpart. The training used 136,000 diverse problems across mathematics, coding, STEM, logic puzzles, and instruction following, and required roughly 16,000 GPU hours on NVIDIA H100s. Pass@k evaluations were used to measure both single\u2011attempt accuracy and broader reasoning capability through multiple attempts. \"ProRL demonstrates that extended RL training enables models to develop genuinely novel reasoning strategies,\" the authors note, underscoring that training compute allocation matters for advancing language\u2011model reasoning.\nOriginal language: en\nPublish date: September 19, 2025 11:06 AM\nSource:[Medium.com](https://medium.com/@stawils/prorl-when-more-training-time-actually-matters-fbb037e50d33)\n\n**The AI model that teaches itself to think through problems, no humans required**\nA new paper in *Nature* reports that DeepSeek AI, a Chinese company, trained its large\u2011language model DeepSeek\u2011R1 to reason on its own using reinforcement learning.  Instead of showing the model every step, researchers gave it a reward only when the final answer was correct, encouraging the model to develop its own problem\u2011solving strategies.  During training, DeepSeek\u2011R1 learned to check its own work, explore alternative approaches, and even use the word \u2018wait\u2019 as it reflected on its thinking process.  The model was evaluated on difficult math, coding and science tasks and outperformed earlier models that relied on human\u2011guided instruction.  Notably, it achieved an 86.7\u202f% accuracy rate on the 2024 American Invitational Mathematics Examination (AIME), a benchmark for top high\u2011school mathematicians.  The authors note that DeepSeek\u2011R1 sometimes mixes languages when prompted in non\u2011English and can over\u2011complicate simple problems, but they expect these issues to be resolved as the approach matures.  The study, cited as Daya\u202fGuo\u202fet\u202fal., *DeepSeek\u2011R1 incentivizes reasoning in LLMs through reinforcement learning*, DOI: 10.1038/s41586-025-09422\u2011z, was published on September\u202f18\u202f2025 by Tech\u202fXplore.\nOriginal language: en\nPublish date: September 18, 2025 08:00 PM\nSource:[Tech Xplore](https://techxplore.com/news/2025-09-ai-problems-humans-required.html)\n\n**OpenAI and Google DeepMind Outshine Students at World\u2019s Top Coding Contest**\nOpenAI and Google DeepMind dominated the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Azerbaijan. OpenAI\u2019s GPT\u20115 solved all 12 problems, achieving a perfect 12\u2011for\u201112 score under the same five\u2011hour time limit as human teams. Eleven of the problems were solved on the first attempt; the twelfth was cracked after nine submissions by an experimental reasoning model. DeepMind\u2019s Gemini\u202f2.5 Deep Think solved 10 of 12 problems, including one that no human team solved, earning a gold\u2011medal level performance. The AI track used the identical problem set, test cases, memory limits, and hardware as the 139 university teams from more than 100 countries. OpenAI said the result would have placed it first in the human leaderboard. Bill Poucher, ICPC Global Executive Director, called the milestone \u2018a key moment in defining the AI tools and academic standards needed for the next generation.\u2019 OpenAI scientist Mostafa Rohaninejad wrote on X that \u2018The next frontier is the discovery of new knowledge, which is the true milestone at the end of the day.\u2019 Gemini also solved a complex network\u2011optimization task that no student team managed, using advanced algorithmic techniques. The contest, which has run for decades, tests 12 problems ranging from graph theory to optimization. Prior to the ICPC, both AI systems earned gold\u2011medal scores at the 2025 International Mathematical Olympiad.\nOriginal language: en\nPublish date: September 18, 2025 07:00 PM\nSource:[Tech Republic](https://www.techrepublic.com/article/openai-deepmind-icpc-2025-results/)\n\n**OpenAI Outperforms Google and Humans: AI Achieves Perfect Score in World Programming Competition**\nOpenAI\u2019s newly released AI system achieved a perfect score in the 2025 International Collegiate Programming Contest (ICPC) held in Baku, Azerbaijan, outperforming both human competitors and Google\u2019s DeepMind model. The system solved all twelve tasks, earning a 100% score within the five\u2011hour time limit and the same conditions as the student teams. According to the company, the system was built from general\u2011purpose reasoning models, none of which were specifically trained for the ICPC. The team used GPT\u20115 together with an internal experimental reasoning model; GPT\u20115 produced correct solutions for eleven of the twelve problems, while the experimental model selected the final solution after nine attempts for the hardest problem. The best human team also solved eleven of the twelve problems. OpenAI highlighted that this success demonstrates the capability of advanced reasoning models to excel where others fail, citing prior gold\u2011level results in the International Mathematical Olympiad and the International Olympiad in Informatics. As stated by the company, the system competed under the same conditions as the students, receiving problems in standard PDF format and having a five\u2011hour deadline to submit solutions to an official ICPC judge. The achievement is framed as part of a continuous progress trajectory, with future goals including the development of systems capable of discovering new knowledge.\nOriginal language: es\nPublish date: September 18, 2025 05:06 PM\nSource:[La Raz\u00f3n](https://www.larazon.es/tecnologia-consumo/openai-humilla-google-humanos-consigue-resultado-perfecto-mayor-competicion-programacion-mundo_2025091868cc0c7a394a8138738ce5b9.html)\n\n**OpenAI GPT-5 with Revolutionary Reasoning Capabilities: What This Means for the Future**\nOpenAI announced GPT\u20115, a large language model that claims a 40% improvement in logical and scientific tasks over GPT\u20114o and the o3 series.  The model achieved 94.6% accuracy on the AIME 2025 math benchmark, 74.9% on SWE\u2011bench for real\u2011world coding, 84.2% on MMMU for multimodal understanding, and 46.2% on HealthBench Hard, with responses ~45% less likely to contain factual inaccuracies than GPT\u20114o and up to 80% less when using its \u2018thinking\u2019 mode.  In the 2025 ICPC World Finals, GPT\u20115 solved 11 of 12 complex algorithmic problems on the first try, outperforming human teams.  GPT\u20115\u2019s architecture is unified, combining reasoning, multimodal input, and agentic execution in a single system; it offers four \u2018thinking\u2019 modes (Standard, Extended, Light, Heavy) and a 256,000\u2011token context window.  The release includes variants such as GPT\u20115\u2011Codex for coding, GPT\u20115\u2011mini and nano for lightweight tasks, and GPT\u20115\u2011chat for conversational use.  The article cites the model\u2019s impact on software development\u2014improving refactoring accuracy by 51% and reducing false positives in code reviews by 68%\u2014and on scientific research, finance, and enterprise AI, noting that Microsoft has embedded GPT\u20115 into Microsoft 365 Copilot and Oracle into its cloud applications.  OpenAI\u2019s safety tests show reduced hallucinations (e.g., only 9% confident answers on non\u2011existent images versus 86.7% for o3).  The piece quotes CEO Sam Altman describing GPT\u20115 as a \u2018significant step along the path to AGI\u2019 and notes a \u2018warmer\u2019 personality update released in mid\u2011August.  Overall, the article presents GPT\u20115 as a transformative tool that could accelerate innovation across sectors while raising concerns about content quality, job displacement, and regulatory scrutiny.\nOriginal language: en\nPublish date: September 18, 2025 03:27 PM\nSource:[Medium.com](https://medium.com/@types24digital/openai-gpt-5-with-revolutionary-reasoning-capabilities-what-this-means-for-the-future-aa6dd5e04d31)\n\n**World programming championship: How ChatGPT, Gemini and AI bots performed**\nThe 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Azerbaijan, marked the first time artificial intelligence systems competed alongside human teams. OpenAI\u2019s GPT\u20115, as part of an ensemble, achieved a perfect score, solving all 12 problems; eleven were accepted on the first submission, and the final problem required only one additional attempt after an experimental variant was deployed. No human team in ICPC history has reached such perfection. Google DeepMind\u2019s Gemini 2.5 solved 10 of the 12 problems, earning a gold\u2011medal\u2011equivalent placement. Gemini\u2019s fastest run saw eight problems solved in just 45 minutes, a pace unmatched by the fastest human squads. The most notable achievement was Gemini\u2019s solution to Problem C, a systems\u2011optimization challenge that no human team solved; Gemini introduced a hybrid minimax\u2011dynamic\u2011programming approach that had not been previously applied. These results demonstrate that general\u2011purpose reasoning models can adapt, reason in real time, and generate novel algorithms under time pressure, challenging the long\u2011standing human monopoly on competitive programming. The article raises questions about future contest formats, potential human\u2011AI collaboration, and the broader impact on education and innovation.\nOriginal language: en\nPublish date: September 18, 2025 12:45 PM\nSource:[Digit](https://www.digit.in/features/general/world-programming-championship-how-chatgpt-gemini-and-ai-bots-performed.html)\n\n**Google and OpenAI Achieve Superhuman Feats at World Coding Finals**\nAt the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Google\u2019s Gemini\u202f2.5\u202fDeep\u202fThink and OpenAI\u2019s GPT\u20115 both outperformed the best human teams. Gemini earned a gold\u2011medal\u2011level score by solving 10 of 12 problems, including the hardest \u201cProblem\u202fC\u201d that stumped every human team; it completed eight problems in 45\u202fminutes and two more within three hours. GPT\u20115 achieved a perfect 12/12, submitting the correct answer on its first attempt for 11 of the 12 problems. The contest featured 139 elite university teams from nearly 3,000 schools, each tasked with solving 12 complex algorithmic problems in five hours. According to the article, Google\u2019s Gemini used a dynamic\u2011programming approach with a \u201cpriority value\u201d for each reservoir and applied the minimax theorem and nested ternary searches to crack Problem\u202fC. OpenAI\u2019s post on Twitter, quoted in the piece, states, 'our @OpenAI reasoning system got a perfect score of 12/12 during the 2025 ICPC World Finals.' The results illustrate a new frontier in AI, demonstrating multi\u2011step, abstract reasoning that surpasses human performance in competitive programming.\nOriginal language: en\nPublish date: September 18, 2025 11:45 AM\nSource:[Winbuzzer](https://winbuzzer.com/2025/09/18/google-and-openai-achieve-superhuman-feats-at-world-coding-finals-xcxwbn/)\n\n**Evaluation of AI: Breathing Life into the Algorithmic Test**\nCl\u00e9mentine Fourrier, a researcher in AI at HuggingFace, explains that the impressive capabilities of generative models such as ChatGPT, Claude, DeepSeek, Grok, and Gemini are assessed through standardized benchmarks that compare their performance to human intelligence. The article notes that the newest benchmark, introduced this winter, is called 'le dernier examen de l'humanit\u00e9' and is designed to test advanced analytical reasoning. It cites FrontierMath as a reference tool for evaluating sophisticated mathematical reasoning in AI, and mentions that OpenAI has reported results comparable to a human on a general\u2011intelligence test, as reported by The Conversation. The piece also references a discussion from Polytechnique Insight about whether AI and human intelligence are comparable. All claims are directly supported by the article text, which includes the quoted terms 'intelligence', 'benchmarks', and 'le dernier examen de l'humanit\u00e9'.\nOriginal language: fr\nPublish date: September 11, 2025 02:59 PM\nSource:[Radio France](https://www.radiofrance.fr/franceculture/podcasts/la-science-cqfd/l-ia-souffle-dans-l-algotest-1340696)\n\n**Blitzy Blows Past SWE-bench Verified, Demonstrating Next Frontier in AI Progress**\nBlitzy, an autonomous software engineering orchestration platform, announced that it has topped the industry\u2011leading benchmark SWE\u2011bench Verified with an 86.8% score, a 13.02% improvement (10 percentage\u2011point leap) over the previous best. According to the article, this marks the largest single advance since March\u202f2024, when Devin achieved a 6.9% improvement (11.9\u2011point leap). The result demonstrates that scaling inference time compute can deliver exponential, not incremental, gains. The article notes that the benchmark had plateaued around 70\u201175% for many models, suggesting a practical ceiling that Blitzy has surpassed. It cites OpenAI\u2019s analysis that human evaluators flagged many samples as \u2018hard or impossible to solve\u2019 due to ambiguous or contradictory requirements. Blitzy\u2019s CTO Sid\u202fPardeshi explained, 'The \u2018unsolvables\u2019 weren\u2019t actually unsolvable \u2013 they just required deeper thinking than System\u20111 AI could provide.' Blitzy\u2019s System\u20112 approach allows AI to reason for hours or days, turning previously unsolvable problems into solvable ones. The article also highlights benchmark limitations: 32.67% of SWE\u2011bench patches may involve solution leakage, and 94% of issues predate LLM training data, raising questions about genuine reasoning versus pattern recognition. Blitzy\u2019s enterprise\u2011scale examples include modernizing 4\u202fmillion lines of legacy Java with 72+ hours of distributed reasoning, extracting services from a 500,000\u2011line monolith requiring 24+ hours of architectural analysis, and cross\u2011language migration with extended verification cycles to preserve semantic equivalence. These achievements illustrate the potential of inference\u2011time scaling beyond isolated coding tasks, suggesting a shift toward System\u20112 AI for complex problem solving.\nOriginal language: en\nPublish date: September 09, 2025 12:30 PM\nSource:[WFMZ.com](https://www.wfmz.com/news/pr_newswire/pr_newswire_technology/blitzy-blows-past-swe-bench-verified-demonstrating-next-frontier-in-ai-progress/article_c7c54a8e-9ef7-5678-8b3b-2d5b5d547f8d.html)\n\n**The Future of AI Reasoning: From Cold Maths to Reason-Native Models: \"AI may never think like us...**\nThe article explains how artificial intelligence is moving from a focus on pattern\u2011recognition and word\u2011prediction to explicit reasoning. It notes that large language models (LLMs) were originally designed to predict the next word, which produced fluent language but unreliable reasoning, as shown by poor performance on benchmarks such as GSM8K and GPQA. Recent research now trains models to reason by incorporating techniques like scratch\u2011pads, self\u2011consistency, tree\u2011of\u2011thoughts, ReAct frameworks, and pause\u2011and\u2011reflect training. The article cites concrete evidence: DeepMind\u2019s AlphaGeometry and AlphaProof solved Olympiad\u2011level problems with symbolic proof checkers, and AlphaGeometry2 outperformed an average gold\u2011medalist in 2025. It also mentions hard benchmarks such as ARC\u2011AGI\u20112 and GPQA that demonstrate the progress of reasoning\u2011native approaches. Key players include OpenAI (o1), DeepSeek (R1), DeepMind (AlphaGeometry/AlphaProof), AI21 Labs (MRKL), and MIT\u2011IBM (neuro\u2011symbolic concept learner). The author argues that the future will likely be a system of systems combining neural perception, symbolic reasoning, verifiers, and reinforcement learning, rather than a single monolithic model. The article concludes with a quote: 'AI may never reason like us, but reasoning\u2011native AI will matter \u2013 because reliability, explainability, and verifiable steps are what we need in systems we trust to make decisions that touch our work, our companies, and our lives.'\nOriginal language: en\nPublish date: September 06, 2025 10:27 AM\nSource:[Medium.com](https://medium.com/@raktims2210/the-future-of-ai-reasoning-from-cold-maths-to-reason-native-models-ai-may-never-think-like-us-6a5f2ab6873c)\n\n**Gemini Symposium Singapore 2025: A Review**\nThe Gemini Symposium Singapore 2025 covered a range of topics on large language models (LLMs) and AI culture, with keynotes from Google DeepMind\u2019s Vice\u2011President Quoc\u202fLe, LLM researcher Denny\u202fZhou, and a fireside chat with Benoit\u202fSchillings, CTO of X and VP of Technology at DeepMind.  Le opened by describing how Google\u2019s culture of \u201caccidental discovery\u201d turned what he calls \u2018bad ideas\u2019 into breakthroughs, citing the evolution from Seq\u20112\u2011Seq LSTMs to the attention\u2011based model in *Attention Is All You Need* and the pre\u2011training paradigm that now underpins LLMs.  He noted that while compression (pre\u2011training on internet\u2011sized corpora) and reasoning (post\u2011training) give a \u201csemblance of intelligence,\u201d current methods are still not efficient enough to reach human\u2011level intelligence, adding that \u201csize is not all that matters in eliciting intelligence.\u201d  Le also highlighted Gemini\u2019s Deep\u2011Think model, which uses self\u2011consistency and chain\u2011of\u2011thought to achieve a gold\u2011medal standard on the International Mathematical Olympiad.  \n\nZhou\u2019s keynote focused on LLM reasoning, arguing that transformers can generate *O(T)* intermediate tokens for problems solvable by boolean circuits of size *T*, and that reasoning can be achieved without fine\u2011tuning.  He cautioned that LLMs are fundamentally probabilistic models that do not \u201cthink like humans,\u201d and that combining reasoning with self\u2011consistency improves complex\u2011problem performance.  \n\nDuring the fireside chat, Schillings discussed the challenges of coding with LLMs, noting that current systems can solve benchmarks such as SWEBench but struggle with extremely long contexts (millions of lines of code).  He emphasized the need for context\u2011efficient models that attend to relevant code segments.  Schillings also spoke about the culture that drives innovation, quoting: \u2018Disney taught us that if you kiss a frog it might turn into a princess, but it does not mean that you go out there and kiss any and every frog out there in hopes that it turns into a princess.\u2019  \n\nThe symposium also included a brief mention of a quote from the event: \u2018Despite so much progress being made on frontier model, no big companies have actually made any money out of AI,....well except for NVIDIA.\u2019  Overall, the review highlights the symposium\u2019s emphasis on iterative learning from past ideas, the importance of reasoning and self\u2011consistency in LLMs, and the cultural factors that enable breakthrough AI research.\n\nOriginal language: en\nPublish date: September 05, 2025 12:08 AM\nSource:[Medium.com](https://medium.com/@shearmanchuaweijie_48703/gemini-symposium-singapore-2025-a-review-fa73f0f13920)\n\n**Forget data labeling: Tencent's R-Zero shows how LLMs can train themselves**\nTencent AI Lab and Washington University in St.\u202fLouis introduced R\u2011Zero, a reinforcement\u2011learning framework that lets two large language models (LLMs) co\u2011evolve without any human\u2011labelled data. In the system a base model is split into a \u2018Challenger\u2019 that generates tasks just beyond the Solver\u2019s current ability and a Solver that is rewarded for solving them. The Challenger\u2019s output is filtered for diversity and used to fine\u2011tune the Solver; the Solver\u2019s own majority\u2011vote answers become the training labels. This loop repeats, creating a self\u2011improving curriculum.\n\nExperiments on open\u2011source LLMs such as Qwen3\u20114B\u2011Base and Qwen3\u20118B\u2011Base showed substantial gains. The Qwen3\u20114B\u2011Base model\u2019s math\u2011reasoning score rose by +6.49 on average after several iterations, while the larger Qwen3\u20118B\u2011Base model improved by +5.51 points after three iterations. General\u2011domain benchmarks also benefited: the same Qwen3\u20114B\u2011Base model improved by +7.54 on MMLU\u2011Pro and SuperGPQA after training on math problems.\n\nThe first iteration produced an immediate performance leap, confirming the effectiveness of the RL\u2011trained Challenger. Moreover, models that first improved via R\u2011Zero achieved even higher performance when later fine\u2011tuned on traditional labelled data, suggesting R\u2011Zero can act as a powerful pre\u2011training step.\n\nHowever, the quality of self\u2011generated labels declined over time: the Solver\u2019s accuracy dropped from 79\u202f% in the first iteration to 63\u202f% by the third, compared with a strong oracle such as GPT\u20114. This trade\u2011off highlights a key bottleneck for long\u2011term improvement.\n\nChengsong Huang, co\u2011author of the paper, noted that \"our approach entirely bypasses the fundamental bottleneck of having to find, label, and curate high\u2011quality datasets,\" and that the framework could enable AI that is no longer limited by the scope of human knowledge. He also suggested adding a third \u2018Verifier\u2019 agent to evaluate quality in more subjective domains.\n\nOverall, R\u2011Zero demonstrates that LLMs can self\u2011evolve reasoning capabilities from zero external data, offering a cost\u2011effective path for enterprises that lack large labelled datasets.\nOriginal language: en\nPublish date: August 28, 2025 09:07 PM\nSource:[VentureBeat](https://venturebeat.com/ai/forget-data-labeling-tencents-r-zero-shows-how-llms-can-train-themselves/)\n\n**How to find the smartest AI**\nThe article explains the growing need for new AI benchmarks that truly challenge state\u2011of\u2011the\u2011art models. It highlights ZeroBench, launched by Jonathan Roberts and colleagues at Cambridge, which targets large multimodal models and currently scores zero for all LLMs. It contrasts this with EnigmaEval, a harder set of over a thousand puzzles from Scale AI, where even Anthropic\u2019s frontier model has only answered one question correctly. The piece notes that older benchmarks such as ImageNet suffered from flawed questions that models could cheat on, and that many modern systems now perform well on those tests because the data is part of their training set. It cites specific metrics: o1\u2011mini scored 98.9\u202f% on a 500\u2011problem high\u2011school maths set, and o3\u2011pro is likely to achieve near\u2011perfect scores on the same set. The article quotes OpenAI CEO Sam Altman saying the new GPT\u20114.5 \u201cwon\u2019t crush benchmarks\u201d and that \u201cthere\u2019s a magic to it I haven\u2019t felt before.\u201d It also discusses newer approaches like Chatbot Arena, which ranks models by blind user preference, and the ARC\u2011AGI series, noting that ARC\u2011AGI\u202f2 still eludes top systems while ARC\u2011AGI\u202f3 is already in development. The author stresses that as models learn existing tests, saturation occurs, making it essential to create fresh, well\u2011vetted challenges that measure genuine progress.\nOriginal language: en\nPublish date: August 26, 2025 07:02 AM\nSource:[mint](https://www.livemint.com/global/how-to-find-the-smartest-ai-11756191365966.html)\n\n**AI systems are great at tests. But how do they perform in real life?**\nThe article discusses how current AI evaluation relies heavily on benchmark tests, which may not reflect real\u2011world performance. It cites OpenAI\u2019s claim that GPT\u20115 is \"much smarter across the board\" and notes that high benchmark scores have led to significant funding, such as Cognition AI\u2019s $175\u202fmillion raise after a software\u2011engineering benchmark. The piece highlights that benchmarks can be gamed, citing Meta\u2019s optimisation of Llama\u20114 for a chatbot\u2011ranking site and OpenAI\u2019s use of the FrontierMath dataset, illustrating Goodhart\u2019s law: \"When a measure becomes a target, it ceases to be a good measure.\" Rumman Chowdhury warns that over\u2011emphasis on metrics can lead to \"manipulation, gaming, and a myopic focus on short\u2011term qualities and inadequate consideration of long\u2011term consequences\". The article argues for more holistic evaluation frameworks, such as MedHELM, which uses 35 benchmarks across five clinical categories to better mimic real\u2011world medical tasks. It calls for a new evaluation ecosystem that incorporates red\u2011team testing and field testing to assess AI\u2019s broader economic, cultural, and societal impacts.\nOriginal language: en\nPublish date: August 24, 2025 08:10 PM\nSource:[The Conversation](http://theconversation.com/ai-systems-are-great-at-tests-but-how-do-they-perform-in-real-life-260176)\n\n**OpenAI's AI Model Achieves 'Gold Medal-Level Performance' in International Mathematical Olympiad**\nOpenAI researcher Alexander Wei announced that their latest experiment reasoning model achieved a 'gold medal-level performance' in the 2025 International Mathematical Olympiad (IMO), solving 5 out of 6 problems and scoring 35 out of 42 points. This achievement is considered a major breakthrough in AI's general reasoning ability, but experts warn that the evaluation conditions may differ from those of human participants. The IMO is recognized as the most prestigious math competition globally, with participants having 4.5 hours to solve 3 difficult math problems each day, without using tools or communicating with other participants. OpenAI's model was evaluated under the same rules as human participants, with two 4.5-hour exam sessions, no tools or internet access, and reading official problem statements and writing natural language proofs. The model's submissions were independently scored by IMO medal winners, with a unanimous agreement on the final score. Wei stated, 'We have achieved a model that can produce complex, rigorous arguments at the level of human mathematicians.' He emphasized that this ability was not achieved through narrow task-specific methods, but through breakthroughs in general reinforcement learning and testing. OpenAI CEO Sam Altman called this a 'significant milestone in AI's progress over the past 10 years,' but noted that the model with 'gold medal-level ability' will not be made public for 'several months.' He added, 'When we first founded OpenAI, this was a dream, but it was not realistic.' The progress in AI's math abilities is remarkable. OpenAI researcher Noam Brown pointed out that in 2024, AI labs were still using elementary school math as a model evaluation standard, and quickly broke through to high school math standards, AIME competitions, and now IMO gold medal levels. However, experts have raised concerns about the evaluation methods. AI critic Gary Marcus praised the model's performance as 'truly impressive,' but questioned the model's training methods, 'general intelligence' range, practicality for the general public, and the cost of each problem. He also noted that the IMO organization has not independently verified these results. Mathematician Terence Tao pointed out that changes in testing conditions can greatly affect the results. He used human competitions as an example, stating that if students were allowed to use calculators, textbooks, or internet searches, or were given several days instead of 4.5 hours to complete the problems, the success rate would be significantly higher. Independent evaluation institution MathArena recently tested major language models, including GPT-4, on 2025 IMO problems and found them to be poor, with logical errors, incomplete arguments, and even fabricated theorems. This makes OpenAI's announcement all the more attention-grabbing, but its true value will depend on whether the results can be independently replicated and applied to actual scientific problems.\nOriginal language: zh\nPublish date: July 20, 2025 02:02 PM\nSource:[\u9999\u6e2f unwire.hk \u73a9\u751f\u6d3b\uff0e\u6a02\u79d1\u6280](https://unwire.hk/2025/07/20/openai-imo-gold/ai/)\n\n**A near AI frontier: Artificial Supertintelligence (ASI)**\nThe development of Artificial Intelligence (AI) has led to the rise of three theoretical fields: Artificial General Intelligence (AGI), Superintelligence (ASI), and Large Language Models (LLMs). AGI is a hypothetical stage where an AI system can match or exceed human cognitive abilities across any task. Currently, reasoning models like OpenAI's o1 and DeepSeek's R1 have scored between 1% and 1.3% in the ARC-AGI-2 test, while non-reasoning models have scored around 1%. However, companies like Meta are moving on to explore Superintelligence, which is defined as a system with an intellectual scope beyond human intelligence. Building blocks of ASI include cutting-edge cognitive functions and highly developed thinking skills. While companies like Meta, Anthropic, Google's DeepMind, and OpenAI are actively working towards developing ASI, the question remains how far we are from achieving Superintelligence, and the answer is that we are still pretty far. Fundamentally speaking, we have to crack AGI first, which includes cracking the human brain to surpass human cognitive capacity.\nOriginal language: en\nPublish date: June 26, 2025 08:52 PM\nSource:[Medium.com](https://medium.com/@i.am.lvcky/a-near-ai-frontier-artificial-supertintelligence-asi-eb2574ed0e23)\n\n",
    "date": "2025-10-13T16:01:38.358915",
    "summary": "Across a variety of expert perspectives, including machine learning engineering, AI scaling laws, algorithmic complexity, deep learning theory, computational learning theory, mathematics education, and AI policy/forecasting, there is an emerging consensus that an AI system achieving >85% on the FrontierMath benchmark before January 1, 2026 is now more likely than not, reflecting dramatic recent progress in mathematical reasoning by state-of-the-art models (such as GPT-5, DeepSeek-R1, and Gemini 2.5). All experts note that current AIs have accomplished superhuman scores on adjacent, hard benchmarks (AIME, IMO, ICPC), often reaching or exceeding the threshold on these tests, and that new training techniques\u2014especially reinforcement learning, self-consistency, and system-2 reasoning\u2014are unlocking rapid jumps in mathematical ability. However, they nearly all caution that FrontierMath is deliberately more difficult, novel, and resistant to training data leakage and superficial pattern-matching, and that historical precedent shows there can be a short lag before the hardest new benchmarks are surpassed. Another frequently cited concern is the requirement for timely, public documentation: even if a model achieves the milestone internally, the result must be published in a qualifying manner for the question to resolve 'Yes.' Competitive, reputational, and commercial incentives are seen as strong motivation for labs to attempt and publicize such a milestone before year\u2019s end, mitigating (but not eliminating) publication-risk. On balance, most forecasts place the probability of a 'Yes' outcome\u2014an AI achieving and publicly documenting >85% on FrontierMath before the deadline\u2014in the 70\u201390% range, with a central tendency clustered around 78\u201385%. Principal risks include (1) the possibility that FrontierMath's unique difficulty remains a temporary barrier, (2) publication or disclosure lag, and (3) historical patterns where new benchmarks resist immediate overtaking. Nonetheless, nearly all experts judge a breakthrough before 2026 to be more likely than not.",
    "forecasters": [
        "Machine Learning Engineering (Large Language Models Development)",
        "Machine Learning Engineering (Benchmark Evaluation Standards)",
        "AI Policy and Forecasting (Technology Forecasting Methodologies)",
        "Artificial Intelligence (Deep Learning Theory)",
        "Artificial Intelligence (Scaling Laws in AI)",
        "Computer Science (Algorithmic Complexity)",
        "Computer Science (Computational Learning Theory)",
        "Mathematics Education (Mathematics Problem Solving Research)"
    ]
}