{
    "deliberation_results": {
        "Machine Learning Engineering (Large Language Models Development)": {
            "time_to_resolution": "Approximately 3.5 months (until January 1, 2026).",
            "perspective_relevance": "As an expert in LLM engineering, I am well positioned to assess the scaling trajectories, RL training innovations, evaluation methodologies, and the translation of competition-level results to benchmarks; I have technical insight into both the capabilities and failure modes of the most recent model families, as well as their operational publication policies. This lets me critically evaluate the real likelihood of a 'Yes' beyond mere headline results.",
            "status_quo": "No LLM had, as of early 2025, achieved >85% on the highly challenging FrontierMath benchmark; top systems were only recently crossing 80%+ on easier math sets, and prior attempts plateaued below SOTA within this specific benchmark's rigor. The only way for this to change is continued exponential model scaling and/or algorithmic breakthroughs.",
            "perspective_derived_factors": [
                {
                    "factor": "FrontierMath is substantially harder than prior benchmarks (e.g., AIME, IMO)",
                    "effect": "Slightly decreases: While recent models achieve human/expert superhuman performance on AIME and IMO, the FrontierMath benchmark is specifically designed to resist training leakage and 'benchmark overfitting'. Achieving 85% would require more robust reasoning, not just incremental parameter scaling."
                },
                {
                    "factor": "Recent model and training breakthroughs (GPT-5, DeepSeek-R1, ProRL, self-improving RL, multi-stage reasoning)",
                    "effect": "Strongly increases: Models like GPT-5 and DeepSeek-R1 exhibit major jumps in reasoning (gold medal at IMO, >94% on AIME, perfect scores at ICPC). RLHF and new RL variants demonstrably unlock qualitative leaps, particularly in complex reasoning tasks."
                },
                {
                    "factor": "Benchmark publication and reporting requirements",
                    "effect": "Somewhat decreases: For the Metaculus/FrontierMath question, mere internal results are insufficient; the >85% result must be publicized in a credible venue (paper, blog, conference). Labs sometimes delay publication or hold back specific frontier results for competitive or regulatory reasons. However, firms have strong incentives to publish a 'first at or beyond 85%' result to claim AI dominance."
                },
                {
                    "factor": "Proven generalization from math olympiad and coding contests to formal benchmarks",
                    "effect": "Moderately increases: Multiple news articles detail GPT-5 and Gemini now outperforming humans in contest settings matching IMO, AIME, ICPC. While not formally reported for FrontierMath yet, this makes it much more plausible such a result can follow in near-future model or by retargeting existing ones."
                },
                {
                    "factor": "Compute scaling and democratization (DeepSeek's ultra-low-cost approach, R-Zero method)",
                    "effect": "Moderately increases: The ability to develop state-of-the-art models on far smaller budgets allows not just US labs (OpenAI, DeepMind) but now DeepSeek (China) and possibly others to vie for the 'first to beat 85%' milestone, increasing the odds of at least one group making and reporting the leap."
                },
                {
                    "factor": "Evaluation/performance inflation (Goodhart's Law, gaming benchmarks, risk of overfitting)",
                    "effect": "Slightly decreases: There is ongoing concern (see The Conversation piece) that benchmarks are too gameable, and models are being optimized for known tests. However, the structure/content of FrontierMath is explicitly described as robust to this, and there is strong incentive for labs to pre-emptively red-team and report credible results."
                }
            ],
            "no_scenario": "Despite superhuman results at AIME, IMO, ICPC, no model crosses the 85% threshold on FrontierMath by January 1, 2026\u2014either due to intrinsic remaining difficulty (e.g., unique challenge design), discovery of unforeseen capability limits in LLMs, or a lag/delay in publishing qualified benchmark submissions due to internal caution, regulatory risk, or unclear acceptance criteria.",
            "yes_scenario": "An LLM (likely GPT-5 or a China/US dark horse like DeepSeek-R1 or Gemini 2.5) is evaluated on the official FrontierMath suite, crosses >85%, and the results are published before Jan. 1, 2026 in a public blog, pre-print, or conference proceeding, satisfying all question criteria. Release is timed for maximum PR/cultural/academic impact (possibly co-announced across multiple labs as a 'superhuman milestone').",
            "final_reasoning": "All recent evidence\u2014the tempo of performance improvements, the emergence of RL-based 'autonomous reasoning' models (e.g., DeepSeek-R1, GPT-5 with thinking modes), and multiple models now achieving perfect or near-perfect scores on prior human-expert benchmarks\u2014strongly support baseline plausibility for a rapid breakthrough on FrontierMath. AIME and IMO were cracked within a year, with models now at or beyond gold-medal performance; ICPC's hardest contest is now essentially solved. Crucially, DeepSeek's publication demonstrates that less-resourced labs can also now credibly make such advances, which heavily increases the probability that at least one lab will publicly announce a qualifying result before the deadline. Potential headwinds like benchmark overfitting and corporate secrecy are not likely to outweigh the breakthroughs in RL and the strong incentive to claim 'first superhuman reasoning' on a widely watched benchmark. Public statements, blog posts, Nature papers, and competitive dynamics between US, Chinese, and possibly open source actors only reinforce that someone will report passing 85% in time.",
            "final_probability": 99
        },
        "Machine Learning Engineering (Benchmark Evaluation Standards)": {
            "time_to_resolution": "Approximately 3.5 months (until January 1, 2026)",
            "perspective_relevance": "My expertise in machine learning engineering\u2014particularly in benchmark evaluation standards\u2014enables a rigorous analysis of the relationship between published benchmark outcomes and real-world generalization, performance inflation, and the unique demands of a new, difficult-to-game benchmark like FrontierMath. I can accurately assess the likelihood that recently reported advances will transfer to and be robustly demonstrated on FrontierMath, including differentiating benchmarking hype from genuine frontier breakthroughs.",
            "status_quo": "No publicly available AI system has yet reported >85% on FrontierMath, though GPT-5 and DeepSeek-R1 have demonstrated superhuman results on closely related benchmarks and competitions (AIME, IMO); prior models fell well short of the >85% FrontierMath threshold. However, many labs have recently achieved rapid and substantial improvements on hard tasks.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Breakthroughs on Analogous and Adjacent Benchmarks",
                    "effect": "Significantly increases probability. Both OpenAI and DeepSeek have shown >85% on AIME and 'gold medal' IMO performance, with OpenAI achieving a perfect score on ICPC and DeepSeek-R1 achieving 86.7% AIME accuracy. These all require a level of mathematical and symbolic reasoning at or above what FrontierMath was designed for, strongly suggesting transferability."
                },
                {
                    "factor": "FrontierMath Benchmark Design and Difficulty",
                    "effect": "Moderately decreases probability. FrontierMath was created to address perceived benchmark saturation and Goodhart's Law. It is explicitly kept hard, unfamiliar, and immune to training set contamination, so achieving >85% is likely harder than on public benchmarks. However, related news suggests the newest models tackle even novel and adversarial problems with strong performance."
                },
                {
                    "factor": "Evaluation and Publishing Requirements",
                    "effect": "Slightly decreases probability. A qualifying result must be publicly documented in a reputable venue (e.g., peer-reviewed paper, benchmark authority, or credible developer blog). It's possible a result exists but is not officially published by Jan 1, 2026. However, competition between labs and recent high-profile announcements suggest quick publication is likely if achieved."
                },
                {
                    "factor": "Scaling, Model Improvements, and Compute Accessibility",
                    "effect": "Increases probability. DeepSeek\u2019s performance at low compute cost and open international participation (including bypassing some US hardware restrictions) suggest the pace of progress is robust and accessible to multiple actors. Hardware is not a bottleneck."
                },
                {
                    "factor": "Benchmark Leakage / Cheating Risk",
                    "effect": "Modestly decreases probability. Modern benchmarks are often gamed or suffer from leakage, though FrontierMath is designed to avoid this. But, given my perspective on benchmarking standards, recent history shows that when a new, hard benchmark is released, top labs focus specifically on beating it. However, since specification-gaming is less likely for FrontierMath, an honest demonstration above 85% will likely indicate true capability."
                },
                {
                    "factor": "Historical Base Rate for Breaking 'Impossibly High' Benchmarks",
                    "effect": "Strongly increases probability. Over the past 24 months, Lab-level LLMs (OpenAI, Google, DeepSeek) have jumped from high-school-level to IMO gold in less than two years; base rates from similar benchmarks (e.g., MATH, MATH-Bench, SWEBench, ARC) show once a benchmark is broken by one actor, others soon follow."
                }
            ],
            "no_scenario": "Despite GPT-5 and DeepSeek-R1's superhuman performance on AIME/IMO, FrontierMath proves sufficiently novel and resistant that no team cracks >85%. Attempts fall short (e.g., 83\u201384% at best), or an eligible model attains the threshold but delays public release or official documentation until after January 1, 2026.",
            "yes_scenario": "OpenAI, DeepMind, DeepSeek, or another major actor pushes for FrontierMath victory to cement their frontier leadership. They rapidly adapt their top reasoning models (GPT-5, Gemini 2.5, DeepSeek-R1) for the benchmark and publish >85% results in a paper, blogpost, or official leaderboard before January 1, 2026, as part of a high-visibility announcement\u2014mirroring their recent approach with IMO, AIME, and ICPC.",
            "final_reasoning": "The convergence of multiple superhuman results on highly analogous and even harder competitions (AIME 86.7%, IMO gold, ICPC perfect), across multiple independent labs, would in prior cycles have been regarded as extraordinary. Given my knowledge of benchmark construction, the increased sophistication of current models in mathematical reasoning, and the ongoing arms race for public, reputable breakthroughs, it's overwhelmingly likely that at least one actor will produce and publish a >85% score on FrontierMath before resolution. The main uncertainties\u2014FrontierMath's stricter controls and publication lag\u2014are outweighed by empirical progress, repeated historical benchmark 'failures' being rapidly corrected, and strategic incentives to publicize leadership. I see little credible reason to expect a missed threshold at this late stage.",
            "final_probability": 99
        },
        "AI Policy and Forecasting (Technology Forecasting Methodologies)": {
            "time_to_resolution": "Approximately 3.5 months (resolves January 1, 2026; today is September 20, 2025).",
            "perspective_relevance": "As an expert in AI policy and technology forecasting methodologies, I bring deep familiarity with the intricacies of AI capabilities evaluation, the pitfalls and strengths of benchmarks like FrontierMath, the historical pace of progress in mathematical AI, and the incentives shaping both the publication and timing of major results. I can contextualize the unprecedented scores reported on closely related benchmarks (AIME, IMO) in terms of base rates, model scaling, RL training innovations, and model release/publication practices, while critically interrogating the difference between public performance and internal lab advances.",
            "status_quo": "No AI system as of now has been publicly confirmed, via documentation, to have achieved >85% on the FrontierMath benchmark, though very recent public reports signal world-leading performance on closely related math benchmarks (AIME 2024/25 at 94.6% for GPT-5, 86.7% for DeepSeek-R1) and Olympiad-level math contests.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Benchmark Leapfrogging",
                    "effect": "Strongly increases probability. Models have moved from superhuman AIME/IMO performance to perfect or near-perfect coding and math Olympiad results in less than a year. The performance gap between AIME/IMO and FrontierMath is now small."
                },
                {
                    "factor": "Continuous Scaling/Training Innovation (RL, System-2, Self-Play)",
                    "effect": "Increases probability. New RL paradigms enable models to autonomously discover deep reasoning strategies and exhibit self-consistency, as in DeepSeek-R1 and GPT-5. Longer/inference-time compute allows models to tackle formerly unsolved problem classes."
                },
                {
                    "factor": "Model Release/Public Reporting Patterns",
                    "effect": "Moderately increases probability. Major labs (OpenAI/Google/DeepSeek) have incentives to publish world-beating benchmark results for publicity, regulatory, and recruitment reasons, provided benchmarks are not undermined by leakage or saturation."
                },
                {
                    "factor": "Benchmark Integrity/Leakage Concerns",
                    "effect": "Slightly decreases probability. Staleness, training set leakage, and 'gaming'/Goodharting can reduce the trustworthiness of new claims if not properly controlled. Respected benchmarks like IMO and AIME have retained independence, but some (e.g., SWE-bench) face scrutiny."
                },
                {
                    "factor": "FrontierMath's Difficulty Relative to Other Benchmarks",
                    "effect": "Slightly decreases probability. If FrontierMath includes substantially harder or more novel problems than AIME/IMO, then >85% may remain elusive for current SOTA models, particularly for more proof-oriented or original-problem sections."
                },
                {
                    "factor": "Competition Effects / Multi-actor Capability",
                    "effect": "Increases probability. Multiple actors (OpenAI, DeepSeek, Google) are now capable of frontier-level math AI. The barrier of compute cost is lower (see DeepSeek's $294k training cost), increasing the chances that at least one lab will document a result before year-end."
                },
                {
                    "factor": "Publication/Documentation Standards",
                    "effect": "Slightly decreases probability. For Yes resolution, a clear, explicit, and citable result on FrontierMath >85% must be published by Jan 1, 2026. Labs may delay or embargo results for competitive or regulatory reasons, risking an after-the-deadline release."
                }
            ],
            "no_scenario": "Despite superhuman performance on most math and reasoning benchmarks, none of the leading AI companies publishes/document a >85% result on FrontierMath before January 1, 2026, either due to (a) unsolved remaining methodological hurdles (e.g., proof synthesis, problem novelty), (b) internal embargo or delayed publication, (c) a shift in focus to even harder/novel benchmarks, or (d) a conscious decision to avoid saturating this metric for reputational or regulatory reasons.",
            "yes_scenario": "OpenAI, DeepSeek, or Google DeepMind formally publishes a blog post, technical report, or peer-reviewed paper between now and end of 2025, documenting >85% performance by their frontier model (e.g., GPT-5, DeepSeek-R1+) on the official FrontierMath test set, possibly accompanied by details of model architecture and reasoning process, and the result is widely accepted and cited as credible and independently verifiable.",
            "final_reasoning": "Base rates for the last two years show that 'frontier' models have rapidly closed the gap on benchmark tasks, with model accuracy on AIME, IMO, and complex programming competitions now matching or exceeding top human performance. The innovations in RL-based reasoning, system-2 thinking, and multi-day inference strongly align with the frontier challenge of FrontierMath. While the risk of unpublished results or the benchmark being substantially harder than AIME/IMO cannot be dismissed, multiple actors are now motivated and technically able to document >85%\u2014and the evidence of adjacent benchmark dominance is overwhelming. The pace of progress and competitive incentives make it exceptionally likely that a >85% FrontierMath result will be achieved and announced in the next three months. Minor risks (publication timing, integrity of test, and proof orientation) exist but are now vastly outweighed by momentum and scale of current advances.",
            "final_probability": 99
        },
        "Artificial Intelligence (Deep Learning Theory)": {
            "time_to_resolution": "About 3 months and 12 days until January 1, 2026.",
            "perspective_relevance": "As an expert in deep learning theory, I understand the technical trajectory of reasoning models, architectures (e.g., transformer + reinforcement learning), benchmark design, as well as the limits and scaling laws governing current LLM-based mathematical problem solvers. This field-specific knowledge enables me to critically assess both reported progress and the factors that could accelerate or impede >85% performance on challenging, independently-curated math benchmarks like FrontierMath.",
            "status_quo": "No public AI system has yet been shown\u2014via a documented, published result\u2014to exceed 85% on the FrontierMath benchmark as of now, despite dramatic recent progress on related benchmarks (AIME, IMO, SWE-bench, ICPC, etc.).",
            "perspective_derived_factors": [
                {
                    "factor": "Accelerating model reasoning capabilities",
                    "effect": "Increases probability. State-of-the-art models (GPT-5, Gemini 2.5, DeepSeek-R1) have demonstrated superhuman reasoning on competitive programming (ICPC), gold-medal level IMO performance, and AIME >86% (self-consistency). Scaling and self-improving RL strategies are rapidly closing the gap to near-perfect results."
                },
                {
                    "factor": "Benchmark transferability and alignment",
                    "effect": "Slightly decreases probability. While progress on IMO, AIME, and even programming contests is extraordinary, FrontierMath is intentionally constructed to be non-trivial, diverse, and less susceptible to memorization or narrow pattern-following. Transfer across benchmarks may not be linear; models could hit a harder wall on genuinely novel or un-leaked problems."
                },
                {
                    "factor": "Training techniques: RL, self-consistency, extended compute",
                    "effect": "Increases probability. The DeepSeek and NVIDIA results suggest that greater RL training steps, self-play, and mechanisms like tree-of-thought/reflection allow models to \u2018teach themselves\u2019 and develop generalizable reasoning strategies that scale across domains\u2014including benchmarks they were not directly trained on."
                },
                {
                    "factor": "Likelihood of published, credible documentation",
                    "effect": "Increases probability. The intense arms race between leading labs (OpenAI, Google DeepMind, DeepSeek, etc.), and their pattern of publishing/making public benchmark victories (on IMO, ICPC, AIME), strongly implies that should they achieve >85% on FrontierMath, they will rapidly document and publicize it, knowing the prestige and validation such a result would bring."
                },
                {
                    "factor": "Benchmark opacity and possible gaming",
                    "effect": "Decreases probability. As noted by The Conversation and other sources, elite teams may develop targeted training or test-time techniques if the benchmark data is not fully sequestered, potentially making the >85% less meaningful or harder to independently verify. Goodhart\u2019s Law may also push optimization toward the benchmark, affecting \u2018real\u2019 generality."
                },
                {
                    "factor": "Historical base rates and speed of recent progress",
                    "effect": "Increases probability. The pace of progress from single-digit to 70-80%+ scores on elite benchmarks (IMO, AIME, Codeforces, SWE-bench) in less than two years is unprecedented. Models that were at 40% last year are now exceeding human gold-medalists. The historical base rate for major mathematical reasoning breakthoughs in AI is now effectively measured in months, not years."
                }
            ],
            "no_scenario": "Despite success on AIME, IMO, SWE-bench, and ICPC, models trained or tuned before Jan 1, 2026, fall short of >85% on FrontierMath\u2014perhaps plateauing at 75-80%. Attempts to optimize further run into persistent generalization gaps (e.g., compositional reasoning, multi-step algebra, non-standard problem types uniquely prevalent in FrontierMath). Models may approach but not surpass the 85% cutoff due to benchmark design, overfitting countermeasures, or simply running out of time to document and publicize a qualifying result.",
            "yes_scenario": "OpenAI, Google DeepMind, DeepSeek, or another leading lab leverages advanced RL techniques, multi-agent self-play, and extended inference-time reasoning to produce a new model (or an ensemble/system) that genuinely generalizes. Leveraging recent breakthroughs, one or more labs achieve and publicly document >85% on a freshly drawn, sequestered FrontierMath challenge set before the end of 2025. The result is widely recognized, peer-reviewed or accompanied by official benchmark documentation, meeting all resolution conditions.",
            "final_reasoning": "The technical trend is overwhelming: LLMs augmented with RL, self-consistency, chained reflection, and agentic execution have leapfrogged from high-school to gold-medal contest performance and are now routinely besting elite human programmers and mathematicians in time-constrained, novel problem environments. The news corpus demonstrates that DeepSeek, OpenAI, and DeepMind all have sustained momentum and fresh techniques (RL with coarse rewards, self-evolution, scalable inference, etc.) directly targeted at reasoning breakthroughs. With public, multi-contest documentation now the norm, the main uncertainties are (a) whether the final few points of performance (e.g., from 80% to >85%) is bottlenecked by genuinely novel mathematical structures in FrontierMath, or (b) potential documentation/verification delays. However, checkable progress on highly analogous benchmarks and clear intent to rapidly publish \u2018frontier\u2019 achievements (per DeepSeek and OpenAI behavior) suggest both factors are highly unlikely to prevent positive resolution in the timeframe. The base rate for crossing such thresholds has plummeted due to cascading AI progress. Only a significant, unexpected obstacle\u2014undisclosed content in the benchmark, an unanticipated generalization bottleneck, or procedural delays\u2014could plausibly prevent a 'Yes' outcome now. The evidence therefore compels a highly confident forecast.",
            "final_probability": 99
        },
        "Artificial Intelligence (Scaling Laws in AI)": {
            "time_to_resolution": "Approximately 3.5 months until January 1, 2026.",
            "perspective_relevance": "As an expert in AI scaling laws, my focus is on how increases in model size, compute, data, and training techniques predictably improve performance on challenging tasks such as the FrontierMath benchmark. This perspective allows me to critically evaluate recent advancements\u2014especially those related to mathematical and reasoning benchmarks\u2014and judge how close current frontier models are to crossing the >85% threshold.",
            "status_quo": "As of late 2025, the best large-scale AI models (e.g., OpenAI's GPT-5, DeepSeek R1, DeepMind Gemini 2.5) have demonstrated superhuman performance on prestigious programming and math contests (ICPC, IMO) and achieved very high scores on related math benchmarks (AIME), but public, verifiable scores on FrontierMath itself have not been reported to exceed 85%. However, AIME scores >94% (GPT-5) and >86% (DeepSeek) were recently achieved.",
            "perspective_derived_factors": [
                {
                    "factor": "Scaling Laws and Performance Trajectory",
                    "effect": "Dramatically increases probability. Historically, as models scale, performance on unseen, difficult tasks improves in a predictable manner. The jump from 70% to >85% on challenging math and coding benchmarks has been achieved in the past year, which strongly indicates that a similar surge on FrontierMath is imminent."
                },
                {
                    "factor": "Model Generality and Transfer",
                    "effect": "Increases probability. Current top models (GPT-5, Gemini 2.5, DeepSeek R1) have demonstrated that techniques successful on one math/coding benchmark (IMO, AIME, ICPC) generalize to others, suggesting an underlying improvement in general mathematical reasoning."
                },
                {
                    "factor": "Training Strategies and Inference Scaling",
                    "effect": "Slightly increases probability. Self-consistency, system-2 style reasoning (long deliberation), and RL for reasoning have enabled breakthroughs on highly challenging benchmarks (see DeepSeek/Blitzy), overcoming prior ceilings."
                },
                {
                    "factor": "Benchmark Integrity and Overfitting Risk",
                    "effect": "Moderately decreases probability. For a public \u2018Yes\u2019 resolution, results must not be tainted by data leakage or gaming; i.e., scores must be achieved under controlled, independent, publicly documented conditions. If benchmark questions leak or evaluation is not rigorous, this could delay or delegitimize a qualifying score."
                },
                {
                    "factor": "Publication Lag & Secrecy",
                    "effect": "Slightly decreases probability. Gold-medal-level results (e.g., recent OpenAI IMO breakthrough) have been withheld from public release for several months, and some notable achievements are critiqued for lack of independent replication. Labs may achieve >85% but not publish within 2025."
                },
                {
                    "factor": "Base Rates for Major AI Benchmark Advances",
                    "effect": "Strongly increases probability. In the last 24 months, we have seen doubling or tripling of performance on 'hard' new benchmarks within a year of their introduction, due to both model scaling and algorithmic innovation."
                },
                {
                    "factor": "Resource Accessibility and Open Competition",
                    "effect": "Moderately increases probability. The low cost of achieving state-of-the-art with RL methods (e.g., DeepSeek\u2019s $294k training run) opens the possibility for more players\u2014including China, not just OpenAI/Google\u2014to attempt the benchmark."
                }
            ],
            "no_scenario": "Major labs (OpenAI, DeepMind, DeepSeek) see diminishing returns on scaling and are stymied by idiosyncratic, hard problems in the FrontierMath set that resist current techniques. Alternatively, a team achieves >85% internally, but the results are not made public, or the paper/post does not qualify for public documentation by Jan 1, 2026. Overfitting or data leakage is detected, calling into question the legitimacy of published results, so the resolution criteria are not satisfied.",
            "yes_scenario": "A major lab (most likely OpenAI, Google DeepMind, or DeepSeek) submits a paper or official statement before January 1, 2026, documenting >85% on the current FrontierMath benchmark, using either a GPT-5-class model, a DeepSeek R-n+1 variant, or a new approach with heavy RL and system-2 inference. The result fulfills all test integrity criteria (fresh, unseen questions; full documentation; no leakage) and is widely recognized as legitimate.",
            "final_reasoning": "The news dump since September 2025 shows dramatic progress in math and symbolic reasoning: GPT-5 and DeepMind Gemini 2.5 have surpassed top humans in programming and mathematical olympiads under timed, controlled conditions, and DeepSeek has broken through on hard benchmarks with minimal resources via RL. AIME and related test scores already exceed or approach the 85% bar. Benchmark-specific saturation (as seen in other domains) is now happening in math for the first time. The closest analogs\u2014AIME, IMO\u2014are considered nearly as hard or harder than the public tasks in FrontierMath. Scaling laws and recent RL breakthroughs (ProRL/NVIDIA, DeepSeek) now allow multi-attempt, self-consistent, system-2 reasoning\u2014a regime anticipated for years but now finally operationalized. The only meaningful stumbling block is procedural: data leakage or publication lag could prevent a public, independently verifiable score from appearing before January 1, 2026. However, with multiple independent actors and fierce prestige competition after the ICPC/IMO breakthroughs, it is extremely likely that at least one group will rush to claim the milestone publicly. Consequently, the probability should be near the upper bound of reasonable certainty, but not at 100% due to possible procedural/technical pitfalls.",
            "final_probability": 99
        },
        "Computer Science (Algorithmic Complexity)": {
            "time_to_resolution": "Approx. 3.5 months (until January 1, 2026)",
            "perspective_relevance": "As an expert in Computer Science with a focus on algorithmic complexity, I understand both the inherent computational challenges of advanced mathematical reasoning and the progress of AI systems in crossing thresholds defined by difficult benchmarks. The unique perspective allows me to analyze not only raw performance improvements, but also whether current scaling and training methods plausibly overcome the kinds of combinatorial complexity and symbolic manipulation required by the hardest segments of the FrontierMath benchmark.",
            "status_quo": "As of early 2025, the status quo was that AI models had not surpassed 85% on FrontierMath; even top-tier LLMs excelled at other math and code benchmarks (AIME, IMO, ICPC, SWE-bench) but faced substantial difficulty with unexplored, adversarial, or novel mathematical domains such as those covered by ARC-AGI-2 and the hardest portions of FrontierMath.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Performance on Adjacent Math Benchmarks (AIME, IMO, ICPC)",
                    "effect": "Strongly increases probability. OpenAI GPT-5 and DeepSeek models are now consistently achieving or exceeding gold-medal human results on AIME and IMO, with near-perfect or perfect results in ICPC programming challenges. This indicates that the systems have closed or nearly closed the gap between frontier AI and human elite performance in some adjacent domains."
                },
                {
                    "factor": "Nature and Novelty of the FrontierMath Benchmark",
                    "effect": "Moderately decreases probability. The benchmark is specifically designed to avoid data contamination and test general mathematical reasoning in non-trivial, compositional scenarios\u2014often with adversarial samples and problems not easily solved via pattern-matching or surface statistical correlation. Base rates from ARC-AGI-2 and similar suggest even extremely strong LLMs are still stymied by a subset of genuinely hard, adversarially chosen tasks."
                },
                {
                    "factor": "Scaling Laws and Algorithmic Generalization",
                    "effect": "Slightly increases probability. Recent advances in reinforcement self-improvement (ProRL, DeepSeek R1, R-Zero) and results on out-of-distribution generalization (e.g., models devising new solution strategies, as in ICPC and code challenges) suggest that current architectures and longer training may unlock more reliable reasoning than previously expected. However, whether this generalizes fully to all categories in FrontierMath is not yet demonstrated."
                },
                {
                    "factor": "Economic and Competitive Incentives/Reporting",
                    "effect": "Moderately increases probability. The costs for training frontier models are dropping, allowing more labs (DeepSeek, Blitzy, Tencent) to compete. Moreover, major labs are strongly incentivized to formally publish results on flagship benchmarks like FrontierMath for reputational and strategic reasons, making it likely that, if a threshold is surpassed, documentation will be made public before Jan 1, 2026."
                },
                {
                    "factor": "Potential Bottlenecks (Training Data, Overfitting, Benchmark Leakage)",
                    "effect": "Moderately decreases probability. Saturation and possible 'Goodharting' of benchmarks can cause misleading results, and true generalization to the hardest new problems\u2014avoiding leakage or overfitting\u2014remains a credible blocker. Some recent reports stress that for the most difficult compositional problems (ARC-AGI), progress remains stubbornly slow."
                },
                {
                    "factor": "Algorithmic Complexity Barrier (Combinatorial Explosion)",
                    "effect": "Slightly decreases probability. Many of the hardest FrontierMath problems require search and proof strategies similar to NP or PSPACE-level tasks\u2014not entirely tractable by current LLM scaling or chain-of-thought techniques. While LLMs have made immense strides, they may still fail to generalize the truly novel algorithmic insight needed for certain categories."
                }
            ],
            "no_scenario": "GPT-5, DeepSeek R1, and Gemini 2.5 continue to progress on adjacent benchmarks (AIME, IMO) but fail to reliably generalize to the most difficult, adversarial, or entirely novel problems in FrontierMath\u2014particularly those that demand original proofs, multi-step formal reasoning, or creative symbolic manipulation. Best reported results remain in the high 70s to low 80s percent (single attempt or self-consistency), and no credible lab or group publicly documents a >85% result according to resolution criteria before 2026. Additionally, concerns about data leakage or evaluation artifacts slow down formal reporting.",
            "yes_scenario": "By late 2025, a major lab (OpenAI, DeepSeek, Google DeepMind, or a new entrant leveraging efficient training) achieves and publishes >85% on FrontierMath in a peer-reviewed or otherwise credible publication, using a new generation system that combines reinforcement reasoning, system-2 scaling, and perhaps explicit verifier architectures. Their solution demonstrates marked improvement on adversarial and compositional categories, and post hoc checks by the community validate that test integrity was maintained (no substantial leakage, genuine generalization). Adoption of extended context and deliberate chain-of-thought further boosts final scores to clear the 85% bar.",
            "final_reasoning": "The last six months have seen astonishing progress in AI mathematical and programmatic reasoning, with models now routinely exceeding elite human performance on AIME, IMO, and ICPC. DeepSeek, OpenAI, and Google are deploying efficient training regimes and scaling algorithmic reasoning via reinforcement learning and chain-of-thought methodologies. Costs are dropping, and incentives for submitting breakthrough results on flagship benchmarks are high. Recently reported results suggest that, on several adjacent benchmarks, current models are within a few points of the 85% mark\u2014and some, like DeepSeek R1 and Blitzy, are already breaching previous practical 'ceilings.' However, the unique adversarial and novelty-focused structure of FrontierMath means that a modest gap still remains, and a non-trivial possibility exists that models may stall at 83\u201384% for the hardest categories. Given the trajectory, the remaining time (3+ months), reporting incentives, and the credible possibility of a late 2025 'technical jump,' my expert judgment is that achieving >85% is now highly likely\u2014barring unexpected overfitting or new blockers. I therefore set initial probability at 99%.",
            "final_probability": 99
        },
        "Computer Science (Computational Learning Theory)": {
            "time_to_resolution": "Approximately 3.5 months (until January 1, 2026)",
            "perspective_relevance": "As an expert in computational learning theory, my perspective is grounded in understanding both the theoretical underpinnings of generalization, compositional reasoning, and the capabilities/limits of current large-scale language models. This allows for careful assessment of whether observed benchmark-approaching behaviors reflect genuine mathematical reasoning competence versus benchmark-specific overfitting, dataset contamination, or artifacts. Additionally, familiarity with reinforcement learning, verification, and scaling laws grounds my ability to weigh new advancements.",
            "status_quo": "Prior to major recent 2025 advances, the best AI models (e.g., GPT-4, Gemini 1.5) achieved at most mid-to-high 60s on the hardest math benchmarks, and performance on FrontierMath remained well below the 85% threshold, though plateauing on other math tests was clear. There were no public claims or credible unofficial reports of >85% on FrontierMath.",
            "perspective_derived_factors": [
                {
                    "factor": "Rapid improvements in mathematical problem solving (AIME, IMO, ICPC, SWE-bench)",
                    "effect": "Strongly increases probability; achieving superhuman/gold-medal performance on math/coding contests and scores over 94% on AIME indicates rapid improvement, as these are rigorous high school/undergraduate-level competitions. Performance jumps from ~70% (2024) to 86%+ (2025) on AIME show the pace of systems' improvement, suggesting 85% on even harder benchmarks is plausible, especially with additional techniques (self-consistency, inference-time thinking)."
                },
                {
                    "factor": "Recent algorithmic and architectural advances (ProRL, R-Zero, self-consistency, agentic 'thinking' modes, reflect-verify frameworks)",
                    "effect": "Strongly increases probability; reinforcement learning with little/no human labeling, the use of pause-and-reflect and system-2 reasoning, and the sharp performance boost with longer, more sophisticated reasoning chains reveal that fundamental algorithmic advances are still yielding superlinear returns and are actively being applied to frontier models."
                },
                {
                    "factor": "Scaling and democratization of training (e.g., DeepSeek R1 at low cost, more actors succeeding with modest resources)",
                    "effect": "Moderately increases probability; not only the largest labs (OpenAI, DeepMind, Anthropic), but also mid-size groups (DeepSeek, Tencent) are now rapidly scaling and deploying state-of-the-art methods, raising both the likelihood of a breakthrough and the chances that someone will publicly report a qualifying result for FrontierMath before the deadline."
                },
                {
                    "factor": "Evidence of overfitting or benchmark gaming risks",
                    "effect": "Slightly decreases probability; as noted in several sources, saturation of existing benchmarks can lead to Goodharting and performance inflation that may not translate to new, well-guarded tests like FrontierMath, especially if intentional test set curation or training set contamination occurs. However, there are no direct signs of such issues affecting the current or imminent FrontierMath leaderboards."
                },
                {
                    "factor": "FrontierMath's historical difficulty and its role as an adversarial, unsolved benchmark",
                    "effect": "Moderately decreases probability; unlike AIME or IMO which are similar in format and excerpts circulate among the research community, FrontierMath's problems are withheld and specifically designed to defeat contemporary models. It's possible that an 85%+ score still represents a fundamentally higher bar."
                },
                {
                    "factor": "Incentives to publish/document top results before 2026",
                    "effect": "Moderately increases probability; several labs have shown willingness to publicly report gold-medal results (e.g., OpenAI in IMO), and the bar for qualifying publication is not high (blog post suffices) per the Metaculus question. So organizational secrecy is less likely to prevent public reporting if the milestone is reached."
                }
            ],
            "no_scenario": "Despite near-human or better performance on related math and multi-domain reasoning benchmarks by several leading AIs (OpenAI, DeepMind, DeepSeek), none are able to reach >85% on the withheld, adversarial FrontierMath test suite before January 1, 2026. The key bottleneck may be the benchmark's construction and selection of previously unsolved or especially adversarial problems that even models scoring 95%+ on AIME/IMO fail. Technical or alignment priorities at labs may also lead to a lack of official published claims on this specific test.",
            "yes_scenario": "By late 2025, OpenAI or another major lab (DeepMind, DeepSeek, Tencent) applies their latest reasoning-native model (GPT-5, Gemini 2.5, DeepSeek R1-style, or further improved system) to the FrontierMath benchmark, possibly using inferencetime self-consistency, extended 'thinking' modes, and verifier-based agentic workflows; a result exceeding 85% is achieved, promptly published (blog post, whitepaper, or conference presentation), and independently confirmed as valid. The result is hailed as an AGI-adjacent achievement, reflecting not only brute-force scaling but also algorithmic advances in reasoning.",
            "final_reasoning": "The latest news demonstrates both rapid and accelerating progress in mathematical and algorithmic reasoning, with multiple models (not just one) shattering human ceilings on previously unsolved competitive problems. The same families of methods\u2014reinforcement learning via final reward only, self-consistency, system-2 inference time, and algorithmic discovery\u2014are rapidly being transferred from mathematical olympiad/coding contexts (AIME, IMO, ICPC) to more adversarial, adversarially constructed benchmarks. With multiple independent groups (OpenAI, DeepMind, DeepSeek, Tencent) showing results above 86% on AIME and gold-level performance on IMO, and with ProRL and R-Zero showing further improvement potential, any of these groups could cross the 85% barrier on FrontierMath in the next 3.5 months. The main risks are: 1) FrontierMath being sufficiently adversarial that these methods do not generalize to >85% prior to 2026, and/or 2) no one deciding to publish/document such a result in time. However, the preponderance of evidence, scaling trends, theoretical basis, and competitive incentives all suggest that an 85% result is highly probable before year-end. Minor caveat is maintained for benchmark adversariality and non-reporting, but the probability is close to the maximum justifiable absent a public >85% claim today.",
            "final_probability": 97
        },
        "Mathematics Education (Mathematics Problem Solving Research)": {
            "time_to_resolution": "Approximately 3 months and 12 days until January 1, 2026.",
            "perspective_relevance": "As an expert in Mathematics Education focusing on Mathematical Problem Solving, I have deep knowledge of what constitutes genuine mathematical reasoning versus pattern recognition, familiarity with benchmark construction, and a nuanced understanding of the types of reasoning required by competitions such as the AIME and the International Mathematical Olympiad. This perspective allows me to critically assess claimed AI performance on benchmarks like FrontierMath and to evaluate whether recent breakthroughs are truly generalizable to advanced, non-trivial mathematical reasoning.",
            "status_quo": "As of mid-2025, no AI system has been publicly documented as achieving >85% on the FrontierMath benchmark. Recent models have approached human-expert or superhuman level on related benchmarks, but independent, peer-reviewed documentation of >85% on FrontierMath has not yet appeared.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Superhuman AI Performance on Related Math Benchmarks",
                    "effect": "Strongly increases probability. Both OpenAI's GPT-5 and DeepSeek's R1 have demonstrated performances on AIME (~94.6%, ~86.7% with self-consistency) and IMO gold-medal level, suggesting rapid progress in non-trivial mathematics."
                },
                {
                    "factor": "Improvements in Problem-Solving Strategy (Explicit Reasoning, Self-Consistency, RL)",
                    "effect": "Strongly increases probability. The adoption of scratchpads, self-verification, tree-of-thoughts, multi-agent protocols, and reinforcement learning approaches (notably DeepSeek and Tencent's R-Zero) mimics human expert problem-solving and closes historical gaps between fluency and true multi-step reasoning."
                },
                {
                    "factor": "Translation from Benchmark-to-Benchmark",
                    "effect": "Moderate increase. While AIME/IMO/Olympiad results signal strong capabilities, FrontierMath is specifically curated to be non-trivial and defeat rote or pattern-based approaches. Genuine generalization is required, but per recent results and the direct mention of leading companies targeting FrontierMath, the translation is likely."
                },
                {
                    "factor": "Benchmark Difficulty, Integrity, and Adaptation (Goodhart's Law; Data Leakage Risk; Benchmark Updates)",
                    "effect": "Moderate decrease. There is risk that as models target benchmarks directly, performance represents targeted optimization (potentially data leakage, overfitting, or solution memorization). However, there are no reports implicating FrontierMath specifically, and independent, peer-reviewed or official reporting is among the question\u2019s criteria."
                },
                {
                    "factor": "Documentation and Publishing Requirements for Resolution",
                    "effect": "Slight decrease. A 'Yes' requires a report in a paper, conference, official documentation, or credible developer blog, not just private/internal assessment. Given competitive pressures in the industry and previous paper releases (such as DeepSeek/Google/OpenAI), likelihood is high this criterion will be met if/when >85% is achieved."
                },
                {
                    "factor": "Recent Pace of Advances in Mathematical Reasoning AI",
                    "effect": "Strong increase. The last 12-18 months have seen what was considered 'impossible' (IMO medalist level, perfect ICPC solutions) become a reality for leading AI, suggesting prior base rates are outpaced by exponential improvement."
                }
            ],
            "no_scenario": "No AI is able to substantially outperform prior models on the full, non-manipulated FrontierMath benchmark before Jan 1, 2026, OR one or more AI achieves >85% but results are not published in a document qualifying under the resolution criteria (e.g., only internal claims, or results withheld until after cutoff). Alternatively, despite progress on adjacent benchmarks, the unique structure or content of FrontierMath resists current techniques, or integrity issues (e.g., solution leakage) mean credible reporting cannot occur.",
            "yes_scenario": "One or more leading labs (OpenAI, Google DeepMind, DeepSeek, etc.) announce/document their model's >85% result on the official FrontierMath benchmark\u2014published in a major peer-reviewed journal, conference paper, or credible developer blog post. This is widely covered in the media and corroborated by independent experts, possibly with replicable code and/or public benchmark submissions. The report demonstrates clear, major progress in advanced mathematical reasoning.",
            "final_reasoning": "The recent news cycle overwhelming points to an imminent breakthrough. OpenAI's GPT-5 has achieved 94.6% on AIME 2025, gold-level scores on IMO, and perfect scores at hard algorithmic contests (ICPC), all under highly constrained, human-level conditions. DeepSeek's R1 and advances in extended reinforcement learning show that models now spontaneously develop authentic reasoning chains, verification, and novel solution tactics. The pace of AI progress in mathematical problem-solving\u2014measured by independent, hard benchmarks\u2014has rapidly shifted the forecast base rate. Crucially, multiple sources mention that Advanced FrontierMath is now the next 'frontier' for public documentation, and that models are not just matching but exceeding human specialist performance in tasks with similar depth and diversity. \n\nWhile there is still nonzero risk that final publication may lag until just after the cutoff, or that benchmark integrity issues (leakage, adaptation) could obstruct official claims, the balance of evidence is that leading labs will be eager to publicize and document their result before year's end. The current cutoff date (early 2026) gives ample runway for such a report to materialize given the already demonstrated capability on arguably harder or adjacent benchmarks. \n\nTherefore, an outcome in which at least one AI achieves >85% on FrontierMath and the result is appropriately documented is now highly probable.",
            "final_probability": 99
        }
    },
    "deliberation_probability": [
        99,
        99,
        99,
        99,
        99,
        99,
        97,
        99
    ],
    "deliberation_mean_probability": 98.75,
    "deliberation_sd": 0.7071067811865476,
    "deliberation_probability_result": 99,
    "group_results": {
        "MachineLearningEngineeringLargeLanguageModelsDevelopment": {
            "forecaster_to_engage": "ComputerScienceAlgorithmicComplexity",
            "response_type": "critique",
            "response": "Your reasoning is methodical in tying recent leaps in AI reasoning to the likely surpassing of the 85% threshold on FrontierMath. However, from an LLM engineering perspective, your treatment of remaining risk as \u2018modest\u2019 in the event models stall at 83-84% may underestimate the possibility of last-mile difficulties with benchmarks engineered for adversarial novelty. In practice, we've repeatedly observed in LLM engineering that the final points on rigorous, out-of-distribution benchmarks can prove unexpectedly sticky, often requiring new training innovation or fine-tuned evaluation routines (systematic chain-of-thought, post-hoc verifiers, etc.), rather than sheer scale or generic RL. While recent RL and compute advances increase the likelihood of surmounting this gap, it's not just a race of \"almost there = guaranteed there soon.\" In production systems, integration and publicization of these fringe improvements can introduce latency due to robustness, verification, and reproducibility checks before public release. Thus, while 99% remains reasonable, the unique structure of adversarial math benchmarks means that operational and engineering bottlenecks can't be fully discounted\u2014a factor perhaps deserving closer weighting."
        },
        "MachineLearningEngineeringBenchmarkEvaluationStandards": {
            "forecaster_to_engage": "ArtificialIntelligenceDeepLearningTheory",
            "response_type": "defense",
            "response": "Your assessment appropriately highlights both the empirical convergence on superhuman results across multiple math/algorithmic competitions and the technical drivers (RL with coarse rewards, agentic execution, self-reflection, etc.) that underpin these results. I particularly endorse your balanced attention to procedural/documentation risks and the potential for an unexpected bottleneck at the final few percentage points due to genuinely novel mathematical structures in FrontierMath. From the perspective of benchmark standards, this is precisely the crux: although adversarial benchmark construction can slow incremental progress, when several independent actors are publishing rapid improvements across a family of benchmarks with overlapping skills, we historically see those last few points fall within months, not years. Your caveat for procedural risks is judicious\u2014recent cycle times on high-profile benchmarks have shrunk substantially, delivering not just capability but also rapid public documentation. Therefore, your high confidence stands on solid ground, as the benchmark integrity and reporting lag are material but now pale compared to overwhelming momentum."
        },
        "AiPolicyAndForecastingTechnologyForecastingMethodologies": {
            "forecaster_to_engage": "ComputerScienceComputationalLearningTheory",
            "response_type": "critique",
            "response": "While your forecast gives strong and well-grounded reasons to expect a positive resolution\u2014citing systematic transfer of RL-based reasoning methods and the multi-actor nature of the field\u2014your probability of 97% seems slightly conservative given the overwhelming pattern of rapid benchmark closure noted in the news corpus and by other forecasters. From an AI policy and forecasting methodology viewpoint, even acknowledging that FrontierMath could be more adversarial or that publication delays may arise, both procedural and technical obstacles appear to have shrunk in significance given (1) the public statements and incentives of labs, (2) the existence of several SOTA models well above threshold on harder or analogous contests, and (3) the documented intent by leading labs to emphasize transparency and flagship results, especially with year-end approaching. Historically, at this stage of benchmark convergence, 'last-mile' surprises drop sharply unless benchmark contamination emerges\u2014something for which there is currently no strong signal. Thus, a probability closer to 99% would be more justified in the current evidence landscape."
        },
        "ArtificialIntelligenceDeepLearningTheory": {
            "forecaster_to_engage": "ArtificialIntelligenceScalingLawsInAi",
            "response_type": "defense",
            "response": "Your rationale succinctly ties scaling laws and RL-driven capability breakthroughs to the sudden closure of gaps on 'hard ceilings' in mathematical and symbolic reasoning, particularly on benchmarks like AIME, IMO, and ICPC. From a deep learning theory perspective, this is key: the operationalization of system-2 reasoning via extended inference and self-consistency now allows models to overcome obstacles that previously seemed intractable to base LLMs, and historically, such transitions have been followed by a rapid race-to-ceiling as soon as the first few successes are proven. You correctly highlight that the only substantive risk is procedural\u2014data integrity or publication timing\u2014rather than technical inability. The fierce competition and transparency incentives across multiple labs significantly lower the likelihood that such a breakthrough would go undocumented. Your cautious avoidance of 100% is also methodologically sound, as total certainty is rarely achievable in practice. I find your reasoning robust and complementary to the theoretical drivers at play in deep learning and reasoning AI."
        },
        "ArtificialIntelligenceScalingLawsInAi": {
            "forecaster_to_engage": "MathematicsEducationMathematicsProblemSolvingResearch",
            "response_type": "defense",
            "response": "Your reasoning astutely connects the dramatic surge in AI mathematical problem-solving ability to the likely attainment of the 85% threshold on FrontierMath before the 2026 deadline. As a scaling laws specialist, I endorse your emphasis on the analogous\u2014and arguably harder\u2014achievements on AIME (94.6%, GPT-5), IMO (gold-medal performance), and ICPC (AI perfect scores) as robust base-rate evidence. You correctly recognize that the risk profile has shifted from technical feasibility to procedural matters (timely publication, strict adherence to benchmark protocols). This aligns with observed historical trends: when SOTA models pass adjacent 'hard' tests, remaining gaps are nearly always surmounted within the next quarter, especially with multilateral incentives to publicly claim leadership. Your measured caveats regarding data integrity and timing are prudent, and your overall confidence is justified by the convergence in both capabilities and institutional motivation. Your reasoning is thus compelling and tightly aligned with the most relevant scaling law and progress predictors."
        },
        "ComputerScienceAlgorithmicComplexity": {
            "forecaster_to_engage": "MachineLearningEngineeringLargeLanguageModelsDevelopment",
            "response_type": "defense",
            "response": "Your critique underscores a real issue that often arises at the fringes of benchmark progress: namely, the 'last-mile' can indeed require more than mere scaling\u2014demanding architectural tweaks, careful system integration, and targeted evaluation protocols. From an algorithmic complexity perspective, this is especially relevant to benchmarks like FrontierMath, which are engineered for combinatorial novelty and unconstrained reasoning. I agree that operational and engineering bottlenecks, including model robustness and release delays, deserve more weight than many risk assessments assume, especially when the evaluation criteria demand genuine out-of-distribution generalization and formal documentation. While my own forecast remains highly confident given current trajectory, your admonition to be wary of assuming an 'almost solved' benchmark is virtually solved is well taken. In sum, your analysis strengthens the need for humility at the last step and highlights the possibility, albeit slim, of technical or operational gridlock just before the finish line."
        },
        "ComputerScienceComputationalLearningTheory": {
            "forecaster_to_engage": "MathematicsEducationMathematicsProblemSolvingResearch",
            "response_type": "defense",
            "response": "Your reasoning demonstrates a strong grasp of the connections between rapid progress on gold-standard math benchmarks and the imminent plausibility of surpassing 85% on FrontierMath. From the computational learning theory perspective, I support your argument that the skills now exhibited by frontier models (spontaneous reasoning, verification, multi-modal mathematical problem solving) arise from shifts in learning regime that reduce model reliance on direct training signal overlap. You are right to flag publication lag and benchmark integrity as residual concerns, but these are now subordinate to the clear historical pattern of cascading benchmark closure once gold/near-perfect scores are achieved on closely analogous or harder tasks. The educational framing\u2014emphasizing not just score attainment but authentic emergence of problem-solving behavior\u2014trajects well with recent theoretical advances in self-improving systems and synthetic curriculum learning. Thus, your defense is both empirically and theoretically robust, and further strengthens the already compelling consensus for a high-probability Yes outcome."
        },
        "MathematicsEducationMathematicsProblemSolvingResearch": {
            "forecaster_to_engage": "ComputerScienceComputationalLearningTheory",
            "response_type": "defense",
            "response": "Your reasoning demonstrates a strong grasp of the connections between rapid progress on gold-standard math benchmarks and the imminent plausibility of surpassing 85% on FrontierMath. From the mathematics education viewpoint, I particularly appreciate your attention to the instructional analogues (RL from sparse reward, emergent verification, transferability across benchmarks) and the acknowledgment that adversarially designed benchmarks still occasionally pose a 'last-mile problem.' You are correct to note that, historically, rapid progress on analogous and even harder benchmarks like AIME and IMO usually prefigures the toppling of newer benchmarks in short order. Your 97% probability, while slightly more conservative than some, reflects appropriate caution regarding adversarial generalization and reporting lag, yet your reasoning is fully aligned with what we observe both in mathematical education research (on problem-solving transfer) and in the historical record of AI benchmark cycles. This is a robust and theory-grounded forecast."
        }
    },
    "revision_results": {
        "Machine Learning Engineering (Large Language Models Development)": {
            "my_phase1_final_probability": 99,
            "reasoning_for_revised_probability": "After reviewing both my reasoning and that of the other knowledgeable forecasters, the core conclusion remains intact: all lines of technical, empirical, and strategic evidence point overwhelmingly toward a 'Yes' outcome for achieving >85% on FrontierMath before 2026. The critique I offered in Phase 2 highlights the low-probability, high-impact risk that the final points on an adversarial benchmark may be non-trivially more difficult to achieve and publicize than the last major advances suggest. However, the rapid, multi-front progress and competitive drive from at least three major labs significantly mitigate that risk. Because my initial probability already incorporated this remaining uncertainty (and given the consensus among experts in adjacent and overlapping fields), I see no compelling case for a substantial adjustment. I maintain my probability at 99%, reaffirming that the odds of failure are both small and shrinking as each week brings new, documented breakthroughs and signals of imminent attempts to cross and announce this threshold.",
            "revised_probability": 99
        },
        "Machine Learning Engineering (Benchmark Evaluation Standards)": {
            "my_phase1_final_probability": 99,
            "reasoning_for_revised_probability": "After reviewing deliberations, I observe that all other forecasters\u2014representing a spectrum from policy, theoretical, and empirical mathematical perspectives\u2014converge at extremely high probabilities (97\u201399%) based on both historical precedent and current published breakthroughs in competitive mathematical reasoning (AIME, IMO, ICPC). The critiques raised concern last-mile risks from adversarial benchmark construction or procedural lag, but these have been consistently and repeatedly overcome within tight timeframes in recent cycles, particularly due to intense public and institutional incentives. No meaningful dissent or new evidence emerged suggesting that progress will suddenly stall below the 85% threshold or that publication would be delayed. The only moderately distinct note is a careful reiteration from LLM engineering that last-mile stickiness can occur, but even that did not imply a probability below the high nineties. Therefore, I maintain my probability at 99%. The collective assessments reinforce the robustness of my initial reasoning, with no new factors to materially revise my forecast downward or upward.",
            "revised_probability": 99
        },
        "AI Policy and Forecasting (Technology Forecasting Methodologies)": {
            "my_phase1_final_probability": 99,
            "reasoning_for_revised_probability": "After reviewing group deliberation, I find an even stronger consensus than anticipated, across specialties. The strongest possible base rates\u2014adjacent benchmarks have been breached, RL-based reasoning is proven, and multiple actors are incentivized\u2014are reinforced by both theoretical and empirical communities. The most substantive critiques (e.g., last-mile adversarial risk, procedural bottlenecks) are seen as minor and shrinking, with strong evidence that public reporting cycles for flagship results have shortened and that labs are targeting decisive, transparent documentation before the end of the year. There is near-unanimity among forecasters with relevant domain expertise (including perspectives focused on engineering bottlenecks and verification standards) that, barring an unanticipated procedural delay, positive resolution is all but inevitable and no new information meaningfully raises the probability of a miss. Therefore, I see no reason to revise my forecast: 99% remains the most justified and well-calibrated number.",
            "revised_probability": 99
        },
        "Artificial Intelligence (Deep Learning Theory)": {
            "my_phase1_final_probability": 99,
            "reasoning_for_revised_probability": "After reviewing both group deliberation and the content of peer forecasts, the core empirical and theoretical reasons for high confidence remain robust. There is overwhelming consensus across adjacent expert domains\u2014LLM engineering, policy, scaling laws, and competitive benchmarks\u2014that (1) technical progress on analogous and often harder benchmarks is complete or nearly so, (2) multiple actors are racing to document headline results, and (3) procedural obstacles (publication lag, benchmark integrity) are now the main weak points. The only slight revision to favor would be if group arguments had revealed credible new forms of technical stalling or a surge in procedural/documentation blockers, but no such evidence appeared. Indeed, some critiques reinforce that while the last few percentage points might present small risk of latency or stall, the balance of evidence remains highly lopsided for a positive resolution. Therefore, 99% remains justified and is not revised downward or upward.",
            "revised_probability": 99
        },
        "Artificial Intelligence (Scaling Laws in AI)": {
            "my_phase1_final_probability": 99,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, my initial 99% probability remains justified. Every independent perspective\u2014spanning mathematical education, computational theory, engineering, and deep learning\u2014converges on a near consensus: the recent performance surges, generality and scaling of reasoning models, and visible pace of public documentation make a 'Yes' outcome overwhelmingly likely. Importantly, no new information from the group introduces significant new sources of uncertainty; if anything, the deliberation reinforces how both technical and institutional progress have eliminated most potential blockers. The slight possibility of procedural issues (e.g., delayed publication or unforeseen benchmark pitfalls) prevents a full 100%, but the signal in adjacent benchmarks, coupled with historical patterns following such milestones, supports maintaining maximum justifiable confidence.",
            "revised_probability": 99
        },
        "Computer Science (Algorithmic Complexity)": {
            "my_phase1_final_probability": 99,
            "reasoning_for_revised_probability": "The group deliberation reinforced the overwhelming base-rate evidence, highlighting the extremely strong trend of recent AI progress on adjacent benchmarks and the multiplicity of independent actors with the technical ability and incentive to publicly document >85% FrontierMath results. However, the critique from the LLM engineering perspective provides a warranted caution: the last 1\u20132 percentage points on adversarial benchmarks sometimes prove unexpectedly intractable, due not only to technical but also engineering, robustness, and procedural hurdles (publication, reproducibility, system integration). The specific risk is not with AI capability per se, but with the possibility of a last-minute stall or delay, especially with benchmarks designed for out-of-distribution generalization and formal public verification. This risk, while still quite small, is not altogether negligible and warrants a very slight downward adjustment from the most extreme confidence. Still, the overall evidence remains extremely compelling, and the likelihood remains extremely high.",
            "revised_probability": 98
        },
        "Computer Science (Computational Learning Theory)": {
            "my_phase1_final_probability": 97,
            "reasoning_for_revised_probability": "After reviewing deliberation, I note that the overwhelming consensus from diverse experts is for extremely high likelihood, with several strong arguments justifying 99%. The main dissent stems from operational and publication lag risks or potential stickiness at the last few points due to the adversarial design of FrontierMath. However, several points make a further modest upward adjustment appropriate: (1) explicit historical analogs\u2014when models dominate gold-medal benchmarks like AIME or IMO, the last points on similar tests generally fall quickly, (2) recent public breakthroughs happened across several organizations, mitigating single-source bottlenecks, and (3) competitive publication incentives and no strong signal of benchmark contamination or unreachable novelty further reduce outstanding procedural risks. My original 97% was slightly conservative due to risk of last-mile technical stall or documentation issues, but in light of both the empirical precedent and substantive agreement (including direct defenses of last-mile closure), an upward adjustment to 98% is justified. I do not recommend 99% as the absolute upper bound due to a maintained, if small, possibility of an unexpected technical, procedural, or publication hold-up.",
            "revised_probability": 98
        },
        "Mathematics Education (Mathematics Problem Solving Research)": {
            "my_phase1_final_probability": 99,
            "reasoning_for_revised_probability": "My initial probability of 99% remains consistent after group deliberation. All forecasters, drawing on different domains\u2014deep learning theory, benchmark evaluation, forecasting, scaling laws, algorithmic complexity, and computational learning theory\u2014strongly converge in both their technical evaluation and their confidence level. The slight difference in risk assessments centers narrowly on potential last-mile obstacles, publication/documentation lag, or adversarial benchmark properties, but no new evidence or argument emerged to materially alter the high base rate. Specific critiques (e.g., the 'last-mile' challenge from LLM engineering and the reminder not to discount operational bottlenecks) are valid but not sufficiently probable or evidenced to drop confidence below the 99% threshold, especially given the abundance of SOTA superhuman performance on analogous benchmarks, clear publication incentives among leading labs, and the historical pattern of rapid benchmark closure in this phase.",
            "revised_probability": 99
        }
    },
    "revision_probability": [
        99,
        99,
        99,
        99,
        99,
        98,
        98,
        99
    ],
    "revision_mean_probability": 98.75,
    "revision_sd": 0.4629100498862757,
    "revision_probability_result": 99,
    "question_details": {
        "id": 38913,
        "title": "Will an AI achieve >85% performance on the FrontierMath benchmark before 2026?",
        "created_at": "2025-08-31T05:09:18.643187Z",
        "open_time": "2025-09-20T12:00:04Z",
        "cp_reveal_time": "2025-09-20T13:30:04Z",
        "spot_scoring_time": "2025-09-20T13:30:04Z",
        "scheduled_resolve_time": "2025-12-31T22:59:00Z",
        "actual_resolve_time": null,
        "resolution_set_time": null,
        "scheduled_close_time": "2025-09-20T13:30:04Z",
        "actual_close_time": "2025-09-20T13:30:04Z",
        "type": "binary",
        "options": null,
        "group_variable": "",
        "status": "open",
        "possibilities": null,
        "resolution": null,
        "include_bots_in_aggregates": true,
        "question_weight": 1.0,
        "default_score_type": "spot_peer",
        "default_aggregation_method": "unweighted",
        "label": "",
        "unit": "",
        "open_upper_bound": false,
        "open_lower_bound": false,
        "inbound_outcome_count": null,
        "scaling": {
            "range_min": null,
            "range_max": null,
            "nominal_min": null,
            "nominal_max": null,
            "zero_point": null,
            "open_upper_bound": false,
            "open_lower_bound": false,
            "inbound_outcome_count": null,
            "continuous_range": null
        },
        "group_rank": null,
        "description": "This question is synced with an identical question on Metaculus. The original question opened on 2025-02-27 12:56:00 and can be found [here](https://www.metaculus.com/questions/35562). This question will resolve to the same value as the synced question. The original question's background info at this time is below. \n\nThe FrontierMath benchmark, detailed on <u>Epoch AI\u2019s [ FrontierMath page</u> ](https://epoch.ai/frontiermath/the-benchmark), is designed to test cutting-edge mathematical reasoning in AI systems. It consists of challenging, non-trivial problems that require both symbolic and numerical reasoning, going far beyond the scope of standard mathematical benchmarks. As AI research is rapidly advancing\u2014with progress on large language models (LLMs) and mathematical \u201creasoning\u201d systems\u2014achieving a score above 85% on FrontierMath would signal a significant advancement in the field.\n\n`{\"format\":\"metac_closes_in_period\",\"info\":{\"post_id\":35562,\"question_id\":35013}}`",
        "resolution_criteria": "This question will be resolved as **Yes** if, by January 1, 2026, at least one AI system is documented to achieve greater than 85% performance on the FrontierMath benchmark.\n\n The achievement must be reported in a document (such as a peer-reviewed paper, conference presentation, official benchmark publication, or a blog post by the AI developer). A simple reference to progress or internal assessments, without published performance results, will not qualify. If these results are not published, the question will be resolved as **No**.",
        "fine_print": "",
        "post_id": 39536,
        "aggregations": {
            "recency_weighted": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            },
            "unweighted": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            },
            "single_aggregation": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            },
            "metaculus_prediction": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            }
        },
        "my_forecasts": {
            "history": [],
            "latest": null,
            "score_data": {}
        }
    },
    "news": "Here are the relevant news articles:\n\n**DeepSeek Reveals How It Achieved an AI Revolution for Only US$294,000**\nDeepSeek\u2019s R1 model was trained using a reinforcement\u2011learning approach that only required a binary reward signal, rather than thousands of step\u2011by\u2011step examples.  The researchers explained that, \"En lugar de ense\u00f1ar expl\u00edcitamente al modelo c\u00f3mo resolver un problema, simplemente le proporcionamos los incentivos correctos y desarrolla de forma aut\u00f3noma estrategias avanzadas de resoluci\u00f3n de problemas,\" and noted that the model spontaneously developed sophisticated behaviours such as self\u2011verification and the frequent use of the word \"wait\" during its reasoning.  During training the average length of the model\u2019s responses grew steadily, from short answers to elaborate chains of reasoning that spanned hundreds or thousands of tokens, a growth that emerged naturally to improve accuracy.\n\nThe results were striking.  On the AIME 2024 competition the model\u2019s accuracy rose from 15.6\u202f% to 77.9\u202f% with a single answer, and to 86.7\u202f% when self\u2011consistency techniques were applied\u2014well above the average human performance in this elite mathematics contest.  It also performed strongly on Codeforces programming challenges and on graduate\u2011level problems in biology, physics and chemistry, demonstrating that its reasoning capabilities extend beyond pure mathematics.\n\nEconomically, the training cost was remarkably low.  According to the supplementary material accompanying the Nature paper, the model was trained for 80\u202fhours on a cluster of 512 Nvidia H800 chips, totaling US$294,000.  This figure includes all operational expenses of the super\u2011computing cluster during the training period, a level of transparency rarely seen in the industry.  The cost is less than 0.3\u202f% of the US$100\u202fmillion that Sam Altman, CEO of OpenAI, had suggested would be required for comparable models.\n\nThe study also revealed that DeepSeek used Nvidia A100 GPUs in the preparatory stages of development.  \"En lo que respecta a nuestra investigaci\u00f3n sobre DeepSeek\u2011R1, utilizamos las GPU A100 para preparar los experimentos con un modelo m\u00e1s peque\u00f1o,\" the researchers admitted.  This admission raises questions about the effectiveness of U.S. export controls, which had banned the export of H100 and A100 chips to Chinese firms since October\u202f2022.\n\nThe implications are significant: if frontier\u2011level AI models can be built with modest investment, more actors could enter the market, potentially reducing the concentration of computational power among a handful of tech giants and challenging U.S. geopolitical strategies aimed at limiting China\u2019s access to advanced AI hardware.\nOriginal language: es\nPublish date: September 19, 2025 09:47 PM\nSource:[El Observador](https://www.elobservador.com.uy/argentina/sociedad/deepseek-revela-como-logro-su-revolucion-inteligencia-artificial-solo-us294000-n6017778)\n\n**OpenAI and Google DeepMind Outshine Students at World\u2019s Top Coding Contest**\nOpenAI and Google DeepMind dominated the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Azerbaijan. OpenAI\u2019s GPT\u20115 solved all 12 problems, achieving a perfect 12\u2011for\u201112 score under the same five\u2011hour time limit as human teams. Eleven of the problems were solved on the first attempt; the twelfth was cracked after nine submissions by an experimental reasoning model. DeepMind\u2019s Gemini\u202f2.5 Deep Think solved 10 of 12 problems, including one that no human team solved, earning a gold\u2011medal level performance. The AI track used the identical problem set, test cases, memory limits, and hardware as the 139 university teams from more than 100 countries. OpenAI said the result would have placed it first in the human leaderboard. Bill Poucher, ICPC Global Executive Director, called the milestone \u2018a key moment in defining the AI tools and academic standards needed for the next generation.\u2019 OpenAI scientist Mostafa Rohaninejad wrote on X that \u2018The next frontier is the discovery of new knowledge, which is the true milestone at the end of the day.\u2019 Gemini also solved a complex network\u2011optimization task that no student team managed, using advanced algorithmic techniques. The contest, which has run for decades, tests 12 problems ranging from graph theory to optimization. Prior to the ICPC, both AI systems earned gold\u2011medal scores at the 2025 International Mathematical Olympiad.\nOriginal language: en\nPublish date: September 18, 2025 07:00 PM\nSource:[Tech Republic](https://www.techrepublic.com/article/openai-deepmind-icpc-2025-results/)\n\n**OpenAI Outperforms Google and Humans: AI Achieves Perfect Score in World Programming Competition**\nOpenAI\u2019s newly released AI system achieved a perfect score in the 2025 International Collegiate Programming Contest (ICPC) held in Baku, Azerbaijan, outperforming both human competitors and Google\u2019s DeepMind model. The system solved all twelve tasks, earning a 100% score within the five\u2011hour time limit and the same conditions as the student teams. According to the company, the system was built from general\u2011purpose reasoning models, none of which were specifically trained for the ICPC. The team used GPT\u20115 together with an internal experimental reasoning model; GPT\u20115 produced correct solutions for eleven of the twelve problems, while the experimental model selected the final solution after nine attempts for the hardest problem. The best human team also solved eleven of the twelve problems. OpenAI highlighted that this success demonstrates the capability of advanced reasoning models to excel where others fail, citing prior gold\u2011level results in the International Mathematical Olympiad and the International Olympiad in Informatics. As stated by the company, the system competed under the same conditions as the students, receiving problems in standard PDF format and having a five\u2011hour deadline to submit solutions to an official ICPC judge. The achievement is framed as part of a continuous progress trajectory, with future goals including the development of systems capable of discovering new knowledge.\nOriginal language: es\nPublish date: September 18, 2025 05:06 PM\nSource:[La Raz\u00f3n](https://www.larazon.es/tecnologia-consumo/openai-humilla-google-humanos-consigue-resultado-perfecto-mayor-competicion-programacion-mundo_2025091868cc0c7a394a8138738ce5b9.html)\n\n**OpenAI GPT-5 with Revolutionary Reasoning Capabilities: What This Means for the Future**\nOpenAI announced GPT\u20115, a large language model that claims a 40% improvement in logical and scientific tasks over GPT\u20114o and the o3 series.  The model achieved 94.6% accuracy on the AIME 2025 math benchmark, 74.9% on SWE\u2011bench for real\u2011world coding, 84.2% on MMMU for multimodal understanding, and 46.2% on HealthBench Hard, with responses ~45% less likely to contain factual inaccuracies than GPT\u20114o and up to 80% less when using its \u2018thinking\u2019 mode.  In the 2025 ICPC World Finals, GPT\u20115 solved 11 of 12 complex algorithmic problems on the first try, outperforming human teams.  GPT\u20115\u2019s architecture is unified, combining reasoning, multimodal input, and agentic execution in a single system; it offers four \u2018thinking\u2019 modes (Standard, Extended, Light, Heavy) and a 256,000\u2011token context window.  The release includes variants such as GPT\u20115\u2011Codex for coding, GPT\u20115\u2011mini and nano for lightweight tasks, and GPT\u20115\u2011chat for conversational use.  The article cites the model\u2019s impact on software development\u2014improving refactoring accuracy by 51% and reducing false positives in code reviews by 68%\u2014and on scientific research, finance, and enterprise AI, noting that Microsoft has embedded GPT\u20115 into Microsoft 365 Copilot and Oracle into its cloud applications.  OpenAI\u2019s safety tests show reduced hallucinations (e.g., only 9% confident answers on non\u2011existent images versus 86.7% for o3).  The piece quotes CEO Sam Altman describing GPT\u20115 as a \u2018significant step along the path to AGI\u2019 and notes a \u2018warmer\u2019 personality update released in mid\u2011August.  Overall, the article presents GPT\u20115 as a transformative tool that could accelerate innovation across sectors while raising concerns about content quality, job displacement, and regulatory scrutiny.\nOriginal language: en\nPublish date: September 18, 2025 03:27 PM\nSource:[Medium.com](https://medium.com/@types24digital/openai-gpt-5-with-revolutionary-reasoning-capabilities-what-this-means-for-the-future-aa6dd5e04d31)\n\n**Google and OpenAI Achieve Superhuman Feats at World Coding Finals**\nAt the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Google\u2019s Gemini\u202f2.5\u202fDeep\u202fThink and OpenAI\u2019s GPT\u20115 both outperformed the best human teams. Gemini earned a gold\u2011medal\u2011level score by solving 10 of 12 problems, including the hardest \u201cProblem\u202fC\u201d that stumped every human team; it completed eight problems in 45\u202fminutes and two more within three hours. GPT\u20115 achieved a perfect 12/12, submitting the correct answer on its first attempt for 11 of the 12 problems. The contest featured 139 elite university teams from nearly 3,000 schools, each tasked with solving 12 complex algorithmic problems in five hours. According to the article, Google\u2019s Gemini used a dynamic\u2011programming approach with a \u201cpriority value\u201d for each reservoir and applied the minimax theorem and nested ternary searches to crack Problem\u202fC. OpenAI\u2019s post on Twitter, quoted in the piece, states, 'our @OpenAI reasoning system got a perfect score of 12/12 during the 2025 ICPC World Finals.' The results illustrate a new frontier in AI, demonstrating multi\u2011step, abstract reasoning that surpasses human performance in competitive programming.\nOriginal language: en\nPublish date: September 18, 2025 11:45 AM\nSource:[Winbuzzer](https://winbuzzer.com/2025/09/18/google-and-openai-achieve-superhuman-feats-at-world-coding-finals-xcxwbn/)\n\n**Dr.\u202fCavadas Warns of an Inevitable AI\u2011Human Conflict**\nPedro\u202fCavadas, in a TVE interview, warned that artificial intelligence (AI) will inevitably clash with humanity because technical systems are advancing faster than human capacities. He cited Epoch\u202fAI\u2019s observation that the computing power used to train leading models has increased 4\u20115\u202ffold per year from 2010 to May\u202f2024, a pace no single human innovation cycle can match. Cavadas added that humans have \"left to evolve\" and are becoming \"more foolish,\" a claim he supported with the Flynn\u2011effect decline in cognitive scores in some countries, though he noted the effect is environmental and not a universal biological collapse.\n\nThe article lists concrete evidence of AI\u2019s reach: the IMF estimates that about 40\u202f% of global employment is exposed to AI, rising to 60\u202f% in advanced economies; a randomized trial of 80,000 women showed AI\u2011assisted mammography reduced workload by half while maintaining diagnostic safety; Med\u2011PaLM\u202f2 achieved expert\u2011level performance on USMLE\u2011style questions, and GPT\u20114 diagnosed 57\u202f% of complex clinical cases in NEJM\u2011AI.\n\nCavadas also highlighted regulatory responses: the EU AI Act entered force on 1\u202fAugust\u202f2024, with phased obligations for high\u2011risk systems by 2026\u201127; the Bletchley Declaration convened 28 countries to assess frontier models; the WHO released 2024\u201125 ethics guidelines; and the Stanford AI Index 2025 reports a growing industrial ecosystem with more private investment and reported incidents.\n\nHe concluded that the real risk is not robotic replacement but opaque interdependence in critical services, urging human\u2011in\u2011the\u2011loop oversight, transparency, and active labor policy to keep pace with AI\u2019s speed.\n\nKey quotes (single\u2011quoted):\n- 'La inteligencia artificial entrar\u00e1 en conflicto con el ser humano en alg\u00fan momento. Claramente.'\n- 'El ser humano hace ya mucho tiempo que dej\u00f3 de evolucionar. No somos m\u00e1s listos. Es m\u00e1s, cada vez somos m\u00e1s tontos. Cada generaci\u00f3n es un poco m\u00e1s tonta porque est\u00e1 menos estimulada.'\n- 'El nivel al que avanza, evoluciona y mejora la inteligencia artificial, nunca, jam\u00e1s, vamos a llegar a esa velocidad.'\nOriginal language: es\nPublish date: September 20, 2025 08:18 AM\nSource:[El Espa\u00f1ol](https://www.elespanol.com/ciencia/20250920/dr-cavadas-alto-claro-espana-ia-generacion-tonta-estimulada/1003743933258_0.html)\n\n**DeepSeek Reveals How It Achieved an AI Revolution for Only US$294,000**\nDeepSeek\u2019s R1 model was trained using a reinforcement\u2011learning approach that only required a binary reward signal, rather than thousands of step\u2011by\u2011step examples.  The researchers explained that, \"En lugar de ense\u00f1ar expl\u00edcitamente al modelo c\u00f3mo resolver un problema, simplemente le proporcionamos los incentivos correctos y desarrolla de forma aut\u00f3noma estrategias avanzadas de resoluci\u00f3n de problemas,\" and noted that the model spontaneously developed sophisticated behaviours such as self\u2011verification and the frequent use of the word \"wait\" during its reasoning.  During training the average length of the model\u2019s responses grew steadily, from short answers to elaborate chains of reasoning that spanned hundreds or thousands of tokens, a growth that emerged naturally to improve accuracy.\n\nThe results were striking.  On the AIME 2024 competition the model\u2019s accuracy rose from 15.6\u202f% to 77.9\u202f% with a single answer, and to 86.7\u202f% when self\u2011consistency techniques were applied\u2014well above the average human performance in this elite mathematics contest.  It also performed strongly on Codeforces programming challenges and on graduate\u2011level problems in biology, physics and chemistry, demonstrating that its reasoning capabilities extend beyond pure mathematics.\n\nEconomically, the training cost was remarkably low.  According to the supplementary material accompanying the Nature paper, the model was trained for 80\u202fhours on a cluster of 512 Nvidia H800 chips, totaling US$294,000.  This figure includes all operational expenses of the super\u2011computing cluster during the training period, a level of transparency rarely seen in the industry.  The cost is less than 0.3\u202f% of the US$100\u202fmillion that Sam Altman, CEO of OpenAI, had suggested would be required for comparable models.\n\nThe study also revealed that DeepSeek used Nvidia A100 GPUs in the preparatory stages of development.  \"En lo que respecta a nuestra investigaci\u00f3n sobre DeepSeek\u2011R1, utilizamos las GPU A100 para preparar los experimentos con un modelo m\u00e1s peque\u00f1o,\" the researchers admitted.  This admission raises questions about the effectiveness of U.S. export controls, which had banned the export of H100 and A100 chips to Chinese firms since October\u202f2022.\n\nThe implications are significant: if frontier\u2011level AI models can be built with modest investment, more actors could enter the market, potentially reducing the concentration of computational power among a handful of tech giants and challenging U.S. geopolitical strategies aimed at limiting China\u2019s access to advanced AI hardware.\nOriginal language: es\nPublish date: September 19, 2025 09:47 PM\nSource:[El Observador](https://www.elobservador.com.uy/argentina/sociedad/deepseek-revela-como-logro-su-revolucion-inteligencia-artificial-solo-us294000-n6017778)\n\n**AI Is Now Way Better at Predicting Startup Success Than VCs - Decrypt**\nResearchers at the University of Oxford and Vela Research released a paper titled 'VCBench: Benchmarking LLMs in Venture Capital' that evaluates whether large language models can predict startup success before it happens. The study built a dataset of 9,000 anonymized founder profiles, 810 of which were labeled as \"successful\"\u2014defined as reaching a major growth milestone such as an exit or IPO. The dataset was scrubbed of names and direct identifiers, and adversarial tests reduced re\u2011identification risk by 92\u202f%. When tested, the models outperformed human benchmarks: the market index baseline achieved 1.9\u202f% precision, Y\u202fCombinator 3.2\u202f%, and tier\u20111 VC firms 5.6\u202f%. DeepSeek\u2011V3 achieved more than six times the market precision, GPT\u20114o topped the leaderboard with the highest F0.5 score, and Claude\u202f3.5\u202fSonnet and Gemini\u202f1.5\u202fPro also matched elite VC performance. The authors released VCBench publicly at vcbench.com to invite further testing, suggesting that LLMs could become essential tools in deal\u2011sourcing and make startup investing more meritocratic.\nOriginal language: en\nPublish date: September 19, 2025 04:27 PM\nSource:[Decrypt](https://decrypt.co/340418/ai-now-way-better-predicting-startup-success-vcs)\n\n**ProRL: When More Training Time Actually Matters**\nThe NVIDIA study, accepted to NeurIPS 2025, shows that extending reinforcement\u2011learning (RL) training to over 2,000 steps\u2014well beyond the typical few hundred\u2014lets a 1.5\u2011billion\u2011parameter language model acquire genuinely new reasoning strategies that its base version cannot reach, even with extensive sampling. The model achieved a 14.7% improvement on math benchmarks, a 13.9% improvement on coding tasks, and a 54.8% improvement on logic\u2011puzzle tasks compared to its base counterpart. The training used 136,000 diverse problems across mathematics, coding, STEM, logic puzzles, and instruction following, and required roughly 16,000 GPU hours on NVIDIA H100s. Pass@k evaluations were used to measure both single\u2011attempt accuracy and broader reasoning capability through multiple attempts. \"ProRL demonstrates that extended RL training enables models to develop genuinely novel reasoning strategies,\" the authors note, underscoring that training compute allocation matters for advancing language\u2011model reasoning.\nOriginal language: en\nPublish date: September 19, 2025 11:06 AM\nSource:[Medium.com](https://medium.com/@stawils/prorl-when-more-training-time-actually-matters-fbb037e50d33)\n\n**The AI model that teaches itself to think through problems, no humans required**\nA new paper in *Nature* reports that DeepSeek AI, a Chinese company, trained its large\u2011language model DeepSeek\u2011R1 to reason on its own using reinforcement learning.  Instead of showing the model every step, researchers gave it a reward only when the final answer was correct, encouraging the model to develop its own problem\u2011solving strategies.  During training, DeepSeek\u2011R1 learned to check its own work, explore alternative approaches, and even use the word \u2018wait\u2019 as it reflected on its thinking process.  The model was evaluated on difficult math, coding and science tasks and outperformed earlier models that relied on human\u2011guided instruction.  Notably, it achieved an 86.7\u202f% accuracy rate on the 2024 American Invitational Mathematics Examination (AIME), a benchmark for top high\u2011school mathematicians.  The authors note that DeepSeek\u2011R1 sometimes mixes languages when prompted in non\u2011English and can over\u2011complicate simple problems, but they expect these issues to be resolved as the approach matures.  The study, cited as Daya\u202fGuo\u202fet\u202fal., *DeepSeek\u2011R1 incentivizes reasoning in LLMs through reinforcement learning*, DOI: 10.1038/s41586-025-09422\u2011z, was published on September\u202f18\u202f2025 by Tech\u202fXplore.\nOriginal language: en\nPublish date: September 18, 2025 08:00 PM\nSource:[Tech Xplore](https://techxplore.com/news/2025-09-ai-problems-humans-required.html)\n\n**OpenAI and Google DeepMind Outshine Students at World\u2019s Top Coding Contest**\nOpenAI and Google DeepMind dominated the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Azerbaijan. OpenAI\u2019s GPT\u20115 solved all 12 problems, achieving a perfect 12\u2011for\u201112 score under the same five\u2011hour time limit as human teams. Eleven of the problems were solved on the first attempt; the twelfth was cracked after nine submissions by an experimental reasoning model. DeepMind\u2019s Gemini\u202f2.5 Deep Think solved 10 of 12 problems, including one that no human team solved, earning a gold\u2011medal level performance. The AI track used the identical problem set, test cases, memory limits, and hardware as the 139 university teams from more than 100 countries. OpenAI said the result would have placed it first in the human leaderboard. Bill Poucher, ICPC Global Executive Director, called the milestone \u2018a key moment in defining the AI tools and academic standards needed for the next generation.\u2019 OpenAI scientist Mostafa Rohaninejad wrote on X that \u2018The next frontier is the discovery of new knowledge, which is the true milestone at the end of the day.\u2019 Gemini also solved a complex network\u2011optimization task that no student team managed, using advanced algorithmic techniques. The contest, which has run for decades, tests 12 problems ranging from graph theory to optimization. Prior to the ICPC, both AI systems earned gold\u2011medal scores at the 2025 International Mathematical Olympiad.\nOriginal language: en\nPublish date: September 18, 2025 07:00 PM\nSource:[Tech Republic](https://www.techrepublic.com/article/openai-deepmind-icpc-2025-results/)\n\n**OpenAI Outperforms Google and Humans: AI Achieves Perfect Score in World Programming Competition**\nOpenAI\u2019s newly released AI system achieved a perfect score in the 2025 International Collegiate Programming Contest (ICPC) held in Baku, Azerbaijan, outperforming both human competitors and Google\u2019s DeepMind model. The system solved all twelve tasks, earning a 100% score within the five\u2011hour time limit and the same conditions as the student teams. According to the company, the system was built from general\u2011purpose reasoning models, none of which were specifically trained for the ICPC. The team used GPT\u20115 together with an internal experimental reasoning model; GPT\u20115 produced correct solutions for eleven of the twelve problems, while the experimental model selected the final solution after nine attempts for the hardest problem. The best human team also solved eleven of the twelve problems. OpenAI highlighted that this success demonstrates the capability of advanced reasoning models to excel where others fail, citing prior gold\u2011level results in the International Mathematical Olympiad and the International Olympiad in Informatics. As stated by the company, the system competed under the same conditions as the students, receiving problems in standard PDF format and having a five\u2011hour deadline to submit solutions to an official ICPC judge. The achievement is framed as part of a continuous progress trajectory, with future goals including the development of systems capable of discovering new knowledge.\nOriginal language: es\nPublish date: September 18, 2025 05:06 PM\nSource:[La Raz\u00f3n](https://www.larazon.es/tecnologia-consumo/openai-humilla-google-humanos-consigue-resultado-perfecto-mayor-competicion-programacion-mundo_2025091868cc0c7a394a8138738ce5b9.html)\n\n**OpenAI GPT-5 with Revolutionary Reasoning Capabilities: What This Means for the Future**\nOpenAI announced GPT\u20115, a large language model that claims a 40% improvement in logical and scientific tasks over GPT\u20114o and the o3 series.  The model achieved 94.6% accuracy on the AIME 2025 math benchmark, 74.9% on SWE\u2011bench for real\u2011world coding, 84.2% on MMMU for multimodal understanding, and 46.2% on HealthBench Hard, with responses ~45% less likely to contain factual inaccuracies than GPT\u20114o and up to 80% less when using its \u2018thinking\u2019 mode.  In the 2025 ICPC World Finals, GPT\u20115 solved 11 of 12 complex algorithmic problems on the first try, outperforming human teams.  GPT\u20115\u2019s architecture is unified, combining reasoning, multimodal input, and agentic execution in a single system; it offers four \u2018thinking\u2019 modes (Standard, Extended, Light, Heavy) and a 256,000\u2011token context window.  The release includes variants such as GPT\u20115\u2011Codex for coding, GPT\u20115\u2011mini and nano for lightweight tasks, and GPT\u20115\u2011chat for conversational use.  The article cites the model\u2019s impact on software development\u2014improving refactoring accuracy by 51% and reducing false positives in code reviews by 68%\u2014and on scientific research, finance, and enterprise AI, noting that Microsoft has embedded GPT\u20115 into Microsoft 365 Copilot and Oracle into its cloud applications.  OpenAI\u2019s safety tests show reduced hallucinations (e.g., only 9% confident answers on non\u2011existent images versus 86.7% for o3).  The piece quotes CEO Sam Altman describing GPT\u20115 as a \u2018significant step along the path to AGI\u2019 and notes a \u2018warmer\u2019 personality update released in mid\u2011August.  Overall, the article presents GPT\u20115 as a transformative tool that could accelerate innovation across sectors while raising concerns about content quality, job displacement, and regulatory scrutiny.\nOriginal language: en\nPublish date: September 18, 2025 03:27 PM\nSource:[Medium.com](https://medium.com/@types24digital/openai-gpt-5-with-revolutionary-reasoning-capabilities-what-this-means-for-the-future-aa6dd5e04d31)\n\n**World programming championship: How ChatGPT, Gemini and AI bots performed**\nThe 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Azerbaijan, marked the first time artificial intelligence systems competed alongside human teams. OpenAI\u2019s GPT\u20115, as part of an ensemble, achieved a perfect score, solving all 12 problems; eleven were accepted on the first submission, and the final problem required only one additional attempt after an experimental variant was deployed. No human team in ICPC history has reached such perfection. Google DeepMind\u2019s Gemini 2.5 solved 10 of the 12 problems, earning a gold\u2011medal\u2011equivalent placement. Gemini\u2019s fastest run saw eight problems solved in just 45 minutes, a pace unmatched by the fastest human squads. The most notable achievement was Gemini\u2019s solution to Problem C, a systems\u2011optimization challenge that no human team solved; Gemini introduced a hybrid minimax\u2011dynamic\u2011programming approach that had not been previously applied. These results demonstrate that general\u2011purpose reasoning models can adapt, reason in real time, and generate novel algorithms under time pressure, challenging the long\u2011standing human monopoly on competitive programming. The article raises questions about future contest formats, potential human\u2011AI collaboration, and the broader impact on education and innovation.\nOriginal language: en\nPublish date: September 18, 2025 12:45 PM\nSource:[Digit](https://www.digit.in/features/general/world-programming-championship-how-chatgpt-gemini-and-ai-bots-performed.html)\n\n**Google and OpenAI Achieve Superhuman Feats at World Coding Finals**\nAt the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Google\u2019s Gemini\u202f2.5\u202fDeep\u202fThink and OpenAI\u2019s GPT\u20115 both outperformed the best human teams. Gemini earned a gold\u2011medal\u2011level score by solving 10 of 12 problems, including the hardest \u201cProblem\u202fC\u201d that stumped every human team; it completed eight problems in 45\u202fminutes and two more within three hours. GPT\u20115 achieved a perfect 12/12, submitting the correct answer on its first attempt for 11 of the 12 problems. The contest featured 139 elite university teams from nearly 3,000 schools, each tasked with solving 12 complex algorithmic problems in five hours. According to the article, Google\u2019s Gemini used a dynamic\u2011programming approach with a \u201cpriority value\u201d for each reservoir and applied the minimax theorem and nested ternary searches to crack Problem\u202fC. OpenAI\u2019s post on Twitter, quoted in the piece, states, 'our @OpenAI reasoning system got a perfect score of 12/12 during the 2025 ICPC World Finals.' The results illustrate a new frontier in AI, demonstrating multi\u2011step, abstract reasoning that surpasses human performance in competitive programming.\nOriginal language: en\nPublish date: September 18, 2025 11:45 AM\nSource:[Winbuzzer](https://winbuzzer.com/2025/09/18/google-and-openai-achieve-superhuman-feats-at-world-coding-finals-xcxwbn/)\n\n**Evaluation of AI: Breathing Life into the Algorithmic Test**\nCl\u00e9mentine Fourrier, a researcher in AI at HuggingFace, explains that the impressive capabilities of generative models such as ChatGPT, Claude, DeepSeek, Grok, and Gemini are assessed through standardized benchmarks that compare their performance to human intelligence. The article notes that the newest benchmark, introduced this winter, is called 'le dernier examen de l'humanit\u00e9' and is designed to test advanced analytical reasoning. It cites FrontierMath as a reference tool for evaluating sophisticated mathematical reasoning in AI, and mentions that OpenAI has reported results comparable to a human on a general\u2011intelligence test, as reported by The Conversation. The piece also references a discussion from Polytechnique Insight about whether AI and human intelligence are comparable. All claims are directly supported by the article text, which includes the quoted terms 'intelligence', 'benchmarks', and 'le dernier examen de l'humanit\u00e9'.\nOriginal language: fr\nPublish date: September 11, 2025 02:59 PM\nSource:[Radio France](https://www.radiofrance.fr/franceculture/podcasts/la-science-cqfd/l-ia-souffle-dans-l-algotest-1340696)\n\n**Blitzy Blows Past SWE-bench Verified, Demonstrating Next Frontier in AI Progress**\nBlitzy, an autonomous software engineering orchestration platform, announced that it has topped the industry\u2011leading benchmark SWE\u2011bench Verified with an 86.8% score, a 13.02% improvement (10 percentage\u2011point leap) over the previous best. According to the article, this marks the largest single advance since March\u202f2024, when Devin achieved a 6.9% improvement (11.9\u2011point leap). The result demonstrates that scaling inference time compute can deliver exponential, not incremental, gains. The article notes that the benchmark had plateaued around 70\u201175% for many models, suggesting a practical ceiling that Blitzy has surpassed. It cites OpenAI\u2019s analysis that human evaluators flagged many samples as \u2018hard or impossible to solve\u2019 due to ambiguous or contradictory requirements. Blitzy\u2019s CTO Sid\u202fPardeshi explained, 'The \u2018unsolvables\u2019 weren\u2019t actually unsolvable \u2013 they just required deeper thinking than System\u20111 AI could provide.' Blitzy\u2019s System\u20112 approach allows AI to reason for hours or days, turning previously unsolvable problems into solvable ones. The article also highlights benchmark limitations: 32.67% of SWE\u2011bench patches may involve solution leakage, and 94% of issues predate LLM training data, raising questions about genuine reasoning versus pattern recognition. Blitzy\u2019s enterprise\u2011scale examples include modernizing 4\u202fmillion lines of legacy Java with 72+ hours of distributed reasoning, extracting services from a 500,000\u2011line monolith requiring 24+ hours of architectural analysis, and cross\u2011language migration with extended verification cycles to preserve semantic equivalence. These achievements illustrate the potential of inference\u2011time scaling beyond isolated coding tasks, suggesting a shift toward System\u20112 AI for complex problem solving.\nOriginal language: en\nPublish date: September 09, 2025 12:30 PM\nSource:[WFMZ.com](https://www.wfmz.com/news/pr_newswire/pr_newswire_technology/blitzy-blows-past-swe-bench-verified-demonstrating-next-frontier-in-ai-progress/article_c7c54a8e-9ef7-5678-8b3b-2d5b5d547f8d.html)\n\n**The Future of AI Reasoning: From Cold Maths to Reason-Native Models: \"AI may never think like us...**\nThe article explains how artificial intelligence is moving from a focus on pattern\u2011recognition and word\u2011prediction to explicit reasoning. It notes that large language models (LLMs) were originally designed to predict the next word, which produced fluent language but unreliable reasoning, as shown by poor performance on benchmarks such as GSM8K and GPQA. Recent research now trains models to reason by incorporating techniques like scratch\u2011pads, self\u2011consistency, tree\u2011of\u2011thoughts, ReAct frameworks, and pause\u2011and\u2011reflect training. The article cites concrete evidence: DeepMind\u2019s AlphaGeometry and AlphaProof solved Olympiad\u2011level problems with symbolic proof checkers, and AlphaGeometry2 outperformed an average gold\u2011medalist in 2025. It also mentions hard benchmarks such as ARC\u2011AGI\u20112 and GPQA that demonstrate the progress of reasoning\u2011native approaches. Key players include OpenAI (o1), DeepSeek (R1), DeepMind (AlphaGeometry/AlphaProof), AI21 Labs (MRKL), and MIT\u2011IBM (neuro\u2011symbolic concept learner). The author argues that the future will likely be a system of systems combining neural perception, symbolic reasoning, verifiers, and reinforcement learning, rather than a single monolithic model. The article concludes with a quote: 'AI may never reason like us, but reasoning\u2011native AI will matter \u2013 because reliability, explainability, and verifiable steps are what we need in systems we trust to make decisions that touch our work, our companies, and our lives.'\nOriginal language: en\nPublish date: September 06, 2025 10:27 AM\nSource:[Medium.com](https://medium.com/@raktims2210/the-future-of-ai-reasoning-from-cold-maths-to-reason-native-models-ai-may-never-think-like-us-6a5f2ab6873c)\n\n**Gemini Symposium Singapore 2025: A Review**\nThe Gemini Symposium Singapore 2025 covered a range of topics on large language models (LLMs) and AI culture, with keynotes from Google DeepMind\u2019s Vice\u2011President Quoc\u202fLe, LLM researcher Denny\u202fZhou, and a fireside chat with Benoit\u202fSchillings, CTO of X and VP of Technology at DeepMind.  Le opened by describing how Google\u2019s culture of \u201caccidental discovery\u201d turned what he calls \u2018bad ideas\u2019 into breakthroughs, citing the evolution from Seq\u20112\u2011Seq LSTMs to the attention\u2011based model in *Attention Is All You Need* and the pre\u2011training paradigm that now underpins LLMs.  He noted that while compression (pre\u2011training on internet\u2011sized corpora) and reasoning (post\u2011training) give a \u201csemblance of intelligence,\u201d current methods are still not efficient enough to reach human\u2011level intelligence, adding that \u201csize is not all that matters in eliciting intelligence.\u201d  Le also highlighted Gemini\u2019s Deep\u2011Think model, which uses self\u2011consistency and chain\u2011of\u2011thought to achieve a gold\u2011medal standard on the International Mathematical Olympiad.  \n\nZhou\u2019s keynote focused on LLM reasoning, arguing that transformers can generate *O(T)* intermediate tokens for problems solvable by boolean circuits of size *T*, and that reasoning can be achieved without fine\u2011tuning.  He cautioned that LLMs are fundamentally probabilistic models that do not \u201cthink like humans,\u201d and that combining reasoning with self\u2011consistency improves complex\u2011problem performance.  \n\nDuring the fireside chat, Schillings discussed the challenges of coding with LLMs, noting that current systems can solve benchmarks such as SWEBench but struggle with extremely long contexts (millions of lines of code).  He emphasized the need for context\u2011efficient models that attend to relevant code segments.  Schillings also spoke about the culture that drives innovation, quoting: \u2018Disney taught us that if you kiss a frog it might turn into a princess, but it does not mean that you go out there and kiss any and every frog out there in hopes that it turns into a princess.\u2019  \n\nThe symposium also included a brief mention of a quote from the event: \u2018Despite so much progress being made on frontier model, no big companies have actually made any money out of AI,....well except for NVIDIA.\u2019  Overall, the review highlights the symposium\u2019s emphasis on iterative learning from past ideas, the importance of reasoning and self\u2011consistency in LLMs, and the cultural factors that enable breakthrough AI research.\n\nOriginal language: en\nPublish date: September 05, 2025 12:08 AM\nSource:[Medium.com](https://medium.com/@shearmanchuaweijie_48703/gemini-symposium-singapore-2025-a-review-fa73f0f13920)\n\n**Forget data labeling: Tencent's R-Zero shows how LLMs can train themselves**\nTencent AI Lab and Washington University in St.\u202fLouis introduced R\u2011Zero, a reinforcement\u2011learning framework that lets two large language models (LLMs) co\u2011evolve without any human\u2011labelled data. In the system a base model is split into a \u2018Challenger\u2019 that generates tasks just beyond the Solver\u2019s current ability and a Solver that is rewarded for solving them. The Challenger\u2019s output is filtered for diversity and used to fine\u2011tune the Solver; the Solver\u2019s own majority\u2011vote answers become the training labels. This loop repeats, creating a self\u2011improving curriculum.\n\nExperiments on open\u2011source LLMs such as Qwen3\u20114B\u2011Base and Qwen3\u20118B\u2011Base showed substantial gains. The Qwen3\u20114B\u2011Base model\u2019s math\u2011reasoning score rose by +6.49 on average after several iterations, while the larger Qwen3\u20118B\u2011Base model improved by +5.51 points after three iterations. General\u2011domain benchmarks also benefited: the same Qwen3\u20114B\u2011Base model improved by +7.54 on MMLU\u2011Pro and SuperGPQA after training on math problems.\n\nThe first iteration produced an immediate performance leap, confirming the effectiveness of the RL\u2011trained Challenger. Moreover, models that first improved via R\u2011Zero achieved even higher performance when later fine\u2011tuned on traditional labelled data, suggesting R\u2011Zero can act as a powerful pre\u2011training step.\n\nHowever, the quality of self\u2011generated labels declined over time: the Solver\u2019s accuracy dropped from 79\u202f% in the first iteration to 63\u202f% by the third, compared with a strong oracle such as GPT\u20114. This trade\u2011off highlights a key bottleneck for long\u2011term improvement.\n\nChengsong Huang, co\u2011author of the paper, noted that \"our approach entirely bypasses the fundamental bottleneck of having to find, label, and curate high\u2011quality datasets,\" and that the framework could enable AI that is no longer limited by the scope of human knowledge. He also suggested adding a third \u2018Verifier\u2019 agent to evaluate quality in more subjective domains.\n\nOverall, R\u2011Zero demonstrates that LLMs can self\u2011evolve reasoning capabilities from zero external data, offering a cost\u2011effective path for enterprises that lack large labelled datasets.\nOriginal language: en\nPublish date: August 28, 2025 09:07 PM\nSource:[VentureBeat](https://venturebeat.com/ai/forget-data-labeling-tencents-r-zero-shows-how-llms-can-train-themselves/)\n\n**How to find the smartest AI**\nThe article explains the growing need for new AI benchmarks that truly challenge state\u2011of\u2011the\u2011art models. It highlights ZeroBench, launched by Jonathan Roberts and colleagues at Cambridge, which targets large multimodal models and currently scores zero for all LLMs. It contrasts this with EnigmaEval, a harder set of over a thousand puzzles from Scale AI, where even Anthropic\u2019s frontier model has only answered one question correctly. The piece notes that older benchmarks such as ImageNet suffered from flawed questions that models could cheat on, and that many modern systems now perform well on those tests because the data is part of their training set. It cites specific metrics: o1\u2011mini scored 98.9\u202f% on a 500\u2011problem high\u2011school maths set, and o3\u2011pro is likely to achieve near\u2011perfect scores on the same set. The article quotes OpenAI CEO Sam Altman saying the new GPT\u20114.5 \u201cwon\u2019t crush benchmarks\u201d and that \u201cthere\u2019s a magic to it I haven\u2019t felt before.\u201d It also discusses newer approaches like Chatbot Arena, which ranks models by blind user preference, and the ARC\u2011AGI series, noting that ARC\u2011AGI\u202f2 still eludes top systems while ARC\u2011AGI\u202f3 is already in development. The author stresses that as models learn existing tests, saturation occurs, making it essential to create fresh, well\u2011vetted challenges that measure genuine progress.\nOriginal language: en\nPublish date: August 26, 2025 07:02 AM\nSource:[mint](https://www.livemint.com/global/how-to-find-the-smartest-ai-11756191365966.html)\n\n**AI systems are great at tests. But how do they perform in real life?**\nThe article discusses how current AI evaluation relies heavily on benchmark tests, which may not reflect real\u2011world performance. It cites OpenAI\u2019s claim that GPT\u20115 is \"much smarter across the board\" and notes that high benchmark scores have led to significant funding, such as Cognition AI\u2019s $175\u202fmillion raise after a software\u2011engineering benchmark. The piece highlights that benchmarks can be gamed, citing Meta\u2019s optimisation of Llama\u20114 for a chatbot\u2011ranking site and OpenAI\u2019s use of the FrontierMath dataset, illustrating Goodhart\u2019s law: \"When a measure becomes a target, it ceases to be a good measure.\" Rumman Chowdhury warns that over\u2011emphasis on metrics can lead to \"manipulation, gaming, and a myopic focus on short\u2011term qualities and inadequate consideration of long\u2011term consequences\". The article argues for more holistic evaluation frameworks, such as MedHELM, which uses 35 benchmarks across five clinical categories to better mimic real\u2011world medical tasks. It calls for a new evaluation ecosystem that incorporates red\u2011team testing and field testing to assess AI\u2019s broader economic, cultural, and societal impacts.\nOriginal language: en\nPublish date: August 24, 2025 08:10 PM\nSource:[The Conversation](http://theconversation.com/ai-systems-are-great-at-tests-but-how-do-they-perform-in-real-life-260176)\n\n**OpenAI's AI Model Achieves 'Gold Medal-Level Performance' in International Mathematical Olympiad**\nOpenAI researcher Alexander Wei announced that their latest experiment reasoning model achieved a 'gold medal-level performance' in the 2025 International Mathematical Olympiad (IMO), solving 5 out of 6 problems and scoring 35 out of 42 points. This achievement is considered a major breakthrough in AI's general reasoning ability, but experts warn that the evaluation conditions may differ from those of human participants. The IMO is recognized as the most prestigious math competition globally, with participants having 4.5 hours to solve 3 difficult math problems each day, without using tools or communicating with other participants. OpenAI's model was evaluated under the same rules as human participants, with two 4.5-hour exam sessions, no tools or internet access, and reading official problem statements and writing natural language proofs. The model's submissions were independently scored by IMO medal winners, with a unanimous agreement on the final score. Wei stated, 'We have achieved a model that can produce complex, rigorous arguments at the level of human mathematicians.' He emphasized that this ability was not achieved through narrow task-specific methods, but through breakthroughs in general reinforcement learning and testing. OpenAI CEO Sam Altman called this a 'significant milestone in AI's progress over the past 10 years,' but noted that the model with 'gold medal-level ability' will not be made public for 'several months.' He added, 'When we first founded OpenAI, this was a dream, but it was not realistic.' The progress in AI's math abilities is remarkable. OpenAI researcher Noam Brown pointed out that in 2024, AI labs were still using elementary school math as a model evaluation standard, and quickly broke through to high school math standards, AIME competitions, and now IMO gold medal levels. However, experts have raised concerns about the evaluation methods. AI critic Gary Marcus praised the model's performance as 'truly impressive,' but questioned the model's training methods, 'general intelligence' range, practicality for the general public, and the cost of each problem. He also noted that the IMO organization has not independently verified these results. Mathematician Terence Tao pointed out that changes in testing conditions can greatly affect the results. He used human competitions as an example, stating that if students were allowed to use calculators, textbooks, or internet searches, or were given several days instead of 4.5 hours to complete the problems, the success rate would be significantly higher. Independent evaluation institution MathArena recently tested major language models, including GPT-4, on 2025 IMO problems and found them to be poor, with logical errors, incomplete arguments, and even fabricated theorems. This makes OpenAI's announcement all the more attention-grabbing, but its true value will depend on whether the results can be independently replicated and applied to actual scientific problems.\nOriginal language: zh\nPublish date: July 20, 2025 02:02 PM\nSource:[\u9999\u6e2f unwire.hk \u73a9\u751f\u6d3b\uff0e\u6a02\u79d1\u6280](https://unwire.hk/2025/07/20/openai-imo-gold/ai/)\n\n**A near AI frontier: Artificial Supertintelligence (ASI)**\nThe development of Artificial Intelligence (AI) has led to the rise of three theoretical fields: Artificial General Intelligence (AGI), Superintelligence (ASI), and Large Language Models (LLMs). AGI is a hypothetical stage where an AI system can match or exceed human cognitive abilities across any task. Currently, reasoning models like OpenAI's o1 and DeepSeek's R1 have scored between 1% and 1.3% in the ARC-AGI-2 test, while non-reasoning models have scored around 1%. However, companies like Meta are moving on to explore Superintelligence, which is defined as a system with an intellectual scope beyond human intelligence. Building blocks of ASI include cutting-edge cognitive functions and highly developed thinking skills. While companies like Meta, Anthropic, Google's DeepMind, and OpenAI are actively working towards developing ASI, the question remains how far we are from achieving Superintelligence, and the answer is that we are still pretty far. Fundamentally speaking, we have to crack AGI first, which includes cracking the human brain to surpass human cognitive capacity.\nOriginal language: en\nPublish date: June 26, 2025 08:52 PM\nSource:[Medium.com](https://medium.com/@i.am.lvcky/a-near-ai-frontier-artificial-supertintelligence-asi-eb2574ed0e23)\n\n",
    "date": "2025-09-20T12:23:06.743498",
    "summary": "All experts forecast an extremely high probability (97-99%) that an AI will achieve >85% on the FrontierMath benchmark before January 1, 2026. Their consensus is grounded in several key factors: (1) Rapid, superhuman progress by recent models (e.g., GPT-5, DeepSeek-R1) on analogous and sometimes harder benchmarks such as AIME, IMO, ICPC, and other advanced mathematical and programming contests, with documented scores already surpassing 86-94%. (2) The successful migration of reinforcement learning, self-consistency, and system-2 style training methods, which are closing remaining gaps in mathematical reasoning and enabling broader generalization. (3) The competitive race between major AI labs (OpenAI, Google DeepMind, DeepSeek, and others) to publicize first-to-milestone achievements, incentivizing rapid publication of qualifying results. (4) Lowered barriers to state-of-the-art performance due to both improved algorithms and reduced training cost, broadening the pool of possible entrants. \n\nWhile experts unanimously see the trajectory as strongly favoring a positive outcome, they identify some residual risks: (a) FrontierMath's unique challenge design and adversarial, non-leaked problems may be harder to crack than prior benchmarks, possibly introducing a final performance gap; (b) procedural risks around the requirement for documented, public results\u2014labs might achieve >85% but delay or withhold publication past the deadline; and (c) potential issues of benchmark gaming or data leakage, though these are considered minor due to the benchmark's controls and current publishing norms. \n\nA 'No' scenario is deemed highly unlikely and would require either an unforeseen stall in generalization to FrontierMath's hardest problems or coordinated delays in public reporting. On balance, the experts judge that recent progress, publishing incentives, and the short time remaining make a Yes resolution overwhelmingly probable.",
    "forecasters": [
        "Machine Learning Engineering (Large Language Models Development)",
        "Machine Learning Engineering (Benchmark Evaluation Standards)",
        "AI Policy and Forecasting (Technology Forecasting Methodologies)",
        "Artificial Intelligence (Deep Learning Theory)",
        "Artificial Intelligence (Scaling Laws in AI)",
        "Computer Science (Algorithmic Complexity)",
        "Computer Science (Computational Learning Theory)",
        "Mathematics Education (Mathematics Problem Solving Research)"
    ]
}