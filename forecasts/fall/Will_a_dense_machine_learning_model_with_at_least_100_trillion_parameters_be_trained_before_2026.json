{
    "deliberation_results": {
        "AI Infrastructure Engineering (Cluster Resource Management)": {
            "time_to_resolution": "Just over 1 month (question resolves January 1, 2026; today: November 29, 2025)",
            "perspective_relevance": "As an expert in AI Infrastructure Engineering (Cluster Resource Management), my perspective is central to assessing the feasibility of training a 100T dense parameter model. I understand both the hard infrastructure constraints (compute, memory bandwidth, interconnect, power, cooling) and the influences of software/algorithmic optimizations, as well as the empirical patterns in major cluster buildouts and scaling projects. My expertise allows me to judge not only what is technically possible but also what is organizationally, economically, and logistically plausible in this time frame.",
            "status_quo": "No known dense 100T parameter model has been credibly reported as trained or operational as of late November 2025. The largest documented dense models are still under the 1T parameter regime, with the trillion-scale announcements all referring to sparse (MoE) architectures. Industry focus is on efficiency and cost controls, and even flagship models (OpenAI, Google, Meta, DeepSeek) do not claim dense architectures remotely near 100T.",
            "perspective_derived_factors": [
                {
                    "factor": "Economic and Infrastructural Feasibility",
                    "effect": "Decreases probability. Training a dense 100T model would require enormous compute and power (1000x current largest dense models), likely costing billions, and demanding clusters with unprecedented memory bandwidth, storage, and reliability. Even recent mega-data center buildouts (Oracle\u2019s $40B for Stargate, AWS Rainier) are shared clouds, not dedicated for a single 100T-dense run."
                },
                {
                    "factor": "State of Scaling Laws and Research Focus",
                    "effect": "Decreases probability. Since Chinchilla (2022), there's a consensus on scaling law optimality, favoring more data and smaller models over naive parameter scaling. Recent advances focus on mixture-of-experts, hybrid, and efficient architectures, indicating an industry shift away from pure dense-scaling as the main path to capability."
                },
                {
                    "factor": "Energy, Power, and Environmental Limits",
                    "effect": "Decreases probability. Current hyperscale clusters already consume small-town equivalent energy. Training a 100T dense model would, according to several cited sources, require multiple gigawatt-years\u2014dwarfing current capacity and colliding with mounting environmental and regulatory scrutiny. The trendline is toward greater energy efficiency, not brute-force expansion."
                },
                {
                    "factor": "Recent Public and Semi-Public Model Disclosures",
                    "effect": "Decreases probability. All recent trillion-scale models (DeepSeek-V4 MoE, GPT-OSS 120B, etc.) use MoE/sparse methods. Even when 1T+ is claimed, only tens of billions of parameters are activated per token. There is no mention in the news of an attempted dense model beyond 1T, let alone 100T."
                },
                {
                    "factor": "Cluster Resource Management Evolution",
                    "effect": "Decreases probability. As clusters scale, scheduling and data sharding become harder; 100T dense would require a cluster with >10x the global largest exaflop clusters, with near-perfect reliability, bandwidth, and synchronization. Even with advances like Project Rainier (Trainium 2) and Stargate, orchestrating such a run in 2025 is logistically unviable."
                },
                {
                    "factor": "Hypothetical Skunkworks / Secret Project",
                    "effect": "Minimal effect. While a highly secretive government or corporate project is always possible, if it were trained and benchmarked against real data, credible reporting is almost certain within the timeframe given the scale involved (publications, leaks, or other records)."
                }
            ],
            "no_scenario": "All plausible clusters and organizations (OpenAI, Google, Meta, DeepSeek, AWS, Oracle) continue to focus on sparse and Mixture-of-Experts models, pushing context length, multimodality, efficiency, and fine-tuning, but do not attempt or complete a 100T dense model. No credible publication or benchmark of a 100T dense model emerges before January 1, 2026. Industry consensus aligns around alternative paths (data, sparsity, RAG, world models, etc.), and infrastructure spending is not prioritized for single-mega-run dense training at this scale.",
            "yes_scenario": "A previously unannounced project from a mega-lab (OpenAI, Google DeepMind, Meta, or a state-backed effort) reveals via a peer-reviewed paper, benchmark report, or authoritative industry release a dense ML model of at least 100T parameters meeting the 80% usage per-forward-pass (i.e., not a sparse/MoE), showing it trained on real data and producing state-of-the-art results. The project either leverages a previously unreported architectural breakthrough that drastically slashes training cost (e.g., via precision scaling, quantization, or novel distributed optimization), or is evidence of an extraordinary, perhaps government-driven megaproject disregarding standard economic efficiency.",
            "final_reasoning": "All available evidence, both technical and organizational, firmly points away from the training of a 100T dense ML model before 2026. The underlying cluster and resource management constraints remain prohibitive\u2014memory and compute requirements are orders of magnitude above current levels. No credible industry or academic actor is signaling intent or reporting progress towards dense models at this scale; all observable energy is directed at increasingly efficient, often sparse, mixtures-of-experts architectures, and multimodal benchmarks. Even with enormous investments in infrastructure, those resources are spread across cloud offerings and multiple projects\u2014not concentrated in single heroic dense-model runs. The scaling law revolution (Chinchilla, etc.) and the contemporary focus on energy/cost management in cluster operation make a 100T dense run not only an inefficient use of resources, but highly unlikely given less-than-two-years to deliver. There is also no sign (in news, leaks, or tech reporting) of a surprise skunkworks project; the sheer scale of such an achievement (multiple gigawatt-years, thousands of nodes) makes such secrecy improbable. The only plausible path to a Yes is an unprecedented, effectively secretive and catastrophic exception to all current trends and constraints\u2014of which there is zero evidence in any of the news feeds or industry discussions up to late November 2025. Thus, while not totally impossible, the probability is extremely low.",
            "final_probability": 1
        },
        "AI Infrastructure Engineering (Data Center Optimization)": {
            "time_to_resolution": "Approximately 13 months (until January 1, 2026).",
            "perspective_relevance": "As an AI Infrastructure Engineering expert focused on data center optimization, I bring a nuanced perspective on the intersection of hardware advances, data center capabilities, power and cooling constraints, scale economics, and the practical logistics of training ultralarge ML models. Specifically, I can accurately evaluate not only whether model training at this scale is theoretically possible, but also whether it is feasible under current and near-term infrastructure realities, economic incentives, and technical bottlenecks.",
            "status_quo": "Dense models in the range of 100\u2013200 billion parameters have been trained by leading labs, with a handful (e.g., Google's PaLM or GPT-4) speculated or claimed to push into the low trillion range, but the largest confirmed dense models remain far below 100T parameters. The AI research community has shifted strongly towards sparsity (MoE) for trillion-scale models due to cost, hardware, and efficiency constraints. No dense model at or above 100T parameters has been credibly reported or benchmarked as trained prior to the forecast date.",
            "perspective_derived_factors": [
                {
                    "factor": "Compute and Hardware Availability",
                    "effect": "Decreases probability. While hyperscale investments (Oracle with Nvidia, AWS Rainier) have pushed capacity up, training a dense 100T model still goes far beyond current datasets, memory, and interconnect capabilities at affordable cost and within given power/cooling budgets. Even largest data centers optimized for ML workloads are vastly more suited to training sparse trillion-parameter models rather than dense ones at this scale."
                },
                {
                    "factor": "Economic Rationality and Scaling Laws",
                    "effect": "Decreases probability. Chinchilla scaling law findings (20:1 data:parameter ratio) and empirical model performance gains have strongly disincentivized dense up-scaling beyond a certain point\u2014smaller dense models with larger datasets routinely outperform very large dense models at equivalent cost. State-of-the-art models (e.g., GPT-5, Gemma 3) have not attempted this extreme of densification, prioritizing data, efficiency, and architecture over raw scale."
                },
                {
                    "factor": "Shift to Sparse Architectures",
                    "effect": "Significantly decreases probability. The largest recent models (DeepSeek-V4 MoE, GPT-OSS, Kimi K2, Qwen-3-Max) are all Mixture-of-Experts or otherwise sparse; even key research and engineering investment are directed toward making sparsity practical at scale, because dense training at 100T parameters is prohibitively expensive and energy-inefficient for most real applications."
                },
                {
                    "factor": "Physical Data Center and Environmental Constraints",
                    "effect": "Decreases probability. Energy supply (as noted in Zhang commentary), rack density, liquid cooling, and carbon footprint concerns make dense 100T model training a major liability. Data center design is moving toward efficiency, renewables, and specialized chips; a dense 100T training run poses a singular and unsustainable spike in resource demands even for deep-pocketed labs."
                },
                {
                    "factor": "Demand, Strategic Value, and Secrecy",
                    "effect": "Slightly increases probability. A breakthrough or a prestige competition (e.g., a sovereign initiative in China or US) might attempt such a feat for signaling power. However, given recent scaling law results, demonstrated lack of step-function capability gains, it is unclear any actor with means would pursue this unless for pure demonstration rather than utility."
                },
                {
                    "factor": "Reporting and Verification Thresholds",
                    "effect": "Decreases probability. For 'Yes' resolution, information about the model\u2014its architecture, sparsity/density, and real-world deployment\u2014must be published and match the stringent density and use criteria. So far, all reported trillion+ models have been clearly MoE/sparse; no credible leaks or publications suggest a secret dense 100T model on the cusp of disclosure."
                }
            ],
            "no_scenario": "Despite massive cloud investments and continued growth in available compute, all practical large-scale trainings continue to favor sparse architectures for trillion-parameter scale. Hardware, power, and cost constraints persist. The major tech companies (OpenAI, Google, Meta, DeepSeek, Alibaba, etc.) release increasingly efficient, multi-modal, or open models with dense parameter counts below 200B, and MoE models in the 1T+ range, but no 'all-parameters-active' 100T dense model emerges. Publications and benchmarks through the end of 2025 confirm this status, and no reliable report of a dense 100T-parameter training is released, even for demonstration.",
            "yes_scenario": "A national lab, major corporation, or strategic coalition (e.g., TPC/Argonne, OpenAI, or a major Chinese effort) completes a 100T+ dense model as a proof-of-concept or prestige project, using newly deployed exascale clusters or next-gen Nvidia Blackwell/GraceHopper systems and achieves a functional, benchmarked dense model (with >80% parameter activation per forward pass). Reporting is thorough enough to confirm density, non-demonstrator status, and real-world use case/benchmark. Verification and public disclosure occur before the end of 2025.",
            "final_reasoning": "The base rate for >100T dense model training is effectively zero; shifts to sparse architectures are well documented and driven by compelling technical/economic rationale. All recent hardware and infrastructure investments facilitate MoE/sparse scaling far more than ultra-dense models. The handful of quoted 'trillion' models are always sparse (3%-5% parameter utilization per token). Despite increased data center investments (AWS, Oracle) and speculative reports about exascale-class systems, there is no direct incentive, nor technical/economic case, for creating a dense 100T model: performance would not justify cost, and verification would be nearly impossible to keep secret. The absence of signals/leaks or even rumors in a highly scrutinized community, plus the lack of unique open problems that necessitate dense 100T solutions, make a pre-2026 'Yes' resolution extremely unlikely. The only plausible pathway is a one-off prestige project, but even such efforts would likely be deprioritized in favor of sparse or efficiency-optimized approaches given Chinchilla's law and environmental/resource constraints. Thus, my probability is extremely low but nonzero to reflect radical uncertainty or a 'moonshot' by a well-funded player who accepts the inefficiency.",
            "final_probability": 2
        },
        "Machine Learning Operations (ML Model Lifecycle Management)": {
            "time_to_resolution": "about 1 month (until Jan 1, 2026)",
            "perspective_relevance": "As an ML Operations (MLOps) expert, I bring deep knowledge of the ML model lifecycle\u2014from R&D and experimentation to large-scale training, deployment, monitoring, and cost/compliance management. Massive models intersect with the hardest MLOps problems: compute orchestration, parallelism strategies, distributed checkpointing, multi-site data management, infrastructure provisioning, energy supply, and real-world application constraints. Understanding both technical feasibility and operational bottlenecks gives me insight into whether such a dense 100T+ model could realistically be trained, publicized, and benchmarked before 2026.",
            "status_quo": "As of late 2025, no known dense ML model has been credibly reported or documented as having at least 100 trillion parameters. All recent large-scale efforts at 1T+ scale use (1) sparse architectures like MoE where only a fraction of parameters are active per forward pass, or (2) dense models with significantly lower parameter counts. OpenAI, DeepMind, Meta, Alibaba, and DeepSeek have published and released large models, but none extant in open literature or industry announcements meet the resolution's strict definition of dense (\u226580% of parameters active per forward pass) at the 100T scale.",
            "perspective_derived_factors": [
                {
                    "factor": "Compute Infrastructure Availability",
                    "effect": "Strongly decreases. Even with major expansions (e.g., Oracle's $40B Nvidia GPU investment, AWS Project Rainier), the effective capacity for dense 100T parameter LLM training remains constrained. Sparse and MoE models were adopted precisely because training a dense 100T model is seen as impractically demanding in both hardware and energy."
                },
                {
                    "factor": "Economics of Training and Energy Constraints",
                    "effect": "Strongly decreases. Estimates put dense 1T-scale training at $100M+ per run. A 100T dense LLM would require massively more\u2014potentially billions of dollars per run, plus prodigious energy consumption (raising ethical and environmental concerns as data center energy usage balloons). Current infrastructure and cost structures strongly discourage such projects."
                },
                {
                    "factor": "Current Trends in Model Architecture",
                    "effect": "Strongly decreases. Industry has converged, post-Chinchilla, on scaling via more data and wider context/longer training in moderately-sized models rather than extreme parameter-count growth. Researchers and TPC leaders voice that \u201cscaling laws era\u201d is over, with scaling walls reached due to data/compute caps and diminishing marginal returns."
                },
                {
                    "factor": "Sparse Model Success and Open Models",
                    "effect": "Moderately decreases. All recent attempts at trillion-parameter models (DeepSeek-V4 MoE, Kimi K2, GPT-OSS MoE, DeepSeek V3) are sparse. The move from 'going deep' to 'going wide' is well-justified and highly effective per benchmarks\u2014no clear incentive remains to brute-force dense scaling."
                },
                {
                    "factor": "Resolution Criteria\u2014Credible Public Announcement",
                    "effect": "Strongly decreases. Even if a clandestine dense 100T model were trained, the question\u2019s resolution hinges on credible, detailed public reporting benchmarked or used in real-world applications. Zero evidence exists for such an announcement."
                },
                {
                    "factor": "Emerging Data Center Capacity & Hardware Trends",
                    "effect": "Marginally increases. Project Rainier and Stargate will quintuple U.S. AI training capacity, and Nvidia Blackwell chips promise further leaps, but these would more plausibly enable dense 1\u201310T models or sparse 100T++ models, not a dense 100T model by 2026."
                },
                {
                    "factor": "AI Culture and Incentives",
                    "effect": "Slightly decreases. Current AI research culture prizes efficiency, sustainability, novel architectures, and \u2018breakthroughs\u2019 in capability per compute cost, not mere parameter scaling. Attempting a 100T dense model would be increasingly seen as anachronistic or irresponsible."
                }
            ],
            "no_scenario": "All observable evidence points to this scenario: ongoing announcements showcase cutting-edge sparse trillion-plus parameter models, more data- and context-efficient architectures, and some ongoing efforts with dense models in the 10\u2013200B parameter regime (Gemma 3, Llama 4, GPT-OSS). No credible leaks, preprints, or documentation of a dense 100T model emerge by end of 2025; any 100T+ model discussed is (a) sparse, (b) hypothetical, or (c) not assessed against benchmarks per the resolution\u2019s bar.",
            "yes_scenario": "A major tech or government entity secretly builds a dense 100T model and confidently benchmarks or applies it, then publishes a credible, dated report by Jan 1, 2026. This would require: (1) unprecedented capital outlay and compute orchestration (tens of thousands of H100s/B200s over months), (2) a major PR or perhaps national prestige/defense rationale, (3) suppression of leaks during training, and (4) accident or deliberate reveal before resolution date. Alternatively, a non-western actor (e.g., a Chinese state-led project) quietly completes and reveals such a model, possibly as a \u2018proof of strength\u2019; again, not even rumors supporting this have surfaced.",
            "final_reasoning": "With ~1 month left, there are no credible public reports or rumors of a dense 100T+ parameter ML model having been trained or even attempted. Practically, dense models beyond the ~1-2T scale have been eschewed in favor of sparse MoE builds, following new empirical scaling laws and cost constraints. The operational and environmental costs of dense scaling at this level are widely viewed as prohibitive across both industry and academia. All recent investments and model engineering focus on sparse MoE architectures or maximal efficiency/throughput per FLOP. Emerging data and hardware may eventually enable such dense models, but given the total lack of public buildup, leaks, or credible hints, the probability is miniscule, heavily suppressed by the convergence of technical, economic, environmental, and practical MLOps barriers\u2014plus, the lack of any public announcement or preprint makes a sudden credible report extremely unlikely at this late date. Base rate: zero history of such models; trend: moving away from parameter count as measure; fine print: requires public credible documentation/benchmarks. Even a black swan scenario seems very far-fetched, so I assign a sharply low probability.",
            "final_probability": 1
        },
        "Semiconductor Industry (GPU Market Analysis)": {
            "time_to_resolution": "Just over 1 month until January 1, 2026.",
            "perspective_relevance": "As an expert in semiconductor industry (GPU market analysis), my knowledge of GPU hardware evolution, hyperscale compute investment, AI workload scaling, and data center economics gives me insight into both the technological feasibility and industry incentives for building such colossal dense models. Understanding the constraints and trends in silicon, interconnects, power, and capex helps evaluate whether organizations could plausibly train dense 100T+ parameter models in the next month.",
            "status_quo": "As of late 2025, no public or credible reports exist of a dense model (>=80% parameters per forward pass, not an MoE) at the 100 trillion parameter scale being trained to completion and benchmarked. The largest dense models released or discussed are in the 1T-2T range (e.g., GPT-4 rumored at ~1.8T dense parameters); 100T dense remains an unprecedented leap.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Model Trends and Announcements",
                    "effect": "Decreases probability. All recently highlighted 1T+ models (DeepSeek-V4, GPT-OSS, Moonshot Kimi K2) are MoE/sparse, sidestepping the prohibitive compute cost of dense 100T models. The largest dense models remain orders of magnitude below 100T."
                },
                {
                    "factor": "GPU and Hardware Capability Trajectory",
                    "effect": "Decreases probability. Massive recent investments in silicon (NVIDIA H100s, Trainium 2, Blackwell B200) scale capacity for large training runs, but practical training of a dense 100T model remains staggeringly expensive and long-running (likely requiring several hundred thousand top-tier GPUs for months, with enormous power and capex demands)."
                },
                {
                    "factor": "Economic and Scientific Incentives",
                    "effect": "Slightly decreases probability. The scaling law shift, Chinchilla findings, and recent expert consensus indicate diminishing returns from pure parameter scaling; funding and leadership attention have shifted to new architectures and world modeling, not brute force dense scaling."
                },
                {
                    "factor": "Data and Scaling Law Limitations",
                    "effect": "Decreases probability. At the 100T dense scale, there is not enough high-quality, de-duplicated data (at recommended token-to-parameter ratios) to efficiently train such a model\u2014even before addressing compute constraints."
                },
                {
                    "factor": "Market, Regulatory, and Environmental Constraints",
                    "effect": "Decreases probability. Energy, public perception around AI's carbon/energy footprint, and regulatory scrutiny (esp. in power-hungry US data centers) introduce further friction to a record-breaking, costly project just for scaling\u2019s sake."
                },
                {
                    "factor": "Potential for a 'Secret' Model",
                    "effect": "Minor increase, but overall keeps probability low. In principle, a major actor (OpenAI, Google, or a sovereign player) could attempt a stealthy dense 100T run; however, at this scale (cost, power draw, supply chain impact) it is hard to keep secret, and resolution criteria require credible published confirmation and benchmarking."
                },
                {
                    "factor": "Past Base Rates and Extrapolation",
                    "effect": "Decreases probability. Historical rate of dense model scaling (GPT-2: 1.5B \u2192 GPT-3: 175B \u2192 GPT-4 [if dense]: 1.8T in ~5 years) shows no evidence of an existing or imminent 100T dense model. Leading models in 2025 remain well below this threshold."
                }
            ],
            "no_scenario": "Status quo persists: No credible team trains and publicly benchmarks a dense 100T+ parameter model by Jan 1, 2026. Leading organizations stick to ~1T or less (mainly as MoE/sparse), and the technical/financial hurdles combined with limited appetite for extreme scale runs prevent any surprise breakthroughs. Instead, they focus on smarter architectures, data quality, multimodality, or efficiency.",
            "yes_scenario": "A tech giant or government, perhaps with proprietary mega-infrastructure (e.g., a sovereign Chinese, American, or Saudi effort using Blackwell B200 or next-gen hardware), secretly allocates the necessary tens of billions in compute, amasses or synthesizes enormous new datasets, and successfully trains and validates a true 100T+ dense model. They publish sufficient details before Jan 1, 2026 to satisfy the resolution criteria, with peer validation of density and training.",
            "final_reasoning": "Despite continued leaps in AI hardware and data center investment, all credible news and public technical progress as of late November 2025 point to persistent barriers preventing a 100T dense model in the next month. Market leaders (OpenAI, Google, Meta, DeepSeek, Alibaba) continue to launch MoE models for trillion+ parameter counts, while the largest dense models (GPT-4 at ~1.8T dense, possibly) are at least a full order of magnitude smaller. All reporting and expert commentary (e.g., LeCun, Sutskever) confirm a plateau in the value of dense scaling, with research and capex directed toward architecture innovation and efficiency, not brute force size. Additionally, enormous technical, energy, and data bottlenecks mean even a national-scale moonshot would struggle\u2014and would likely leave signals well before success. With only a month left, no rumors, leaks, or technical previews point to a surprise imminent release. Therefore, while not impossible, the probability is extremely low.",
            "final_probability": 1
        },
        "Computer Science (Deep Learning Scaling Laws)": {
            "time_to_resolution": "Approximately 1 month (until January 1, 2026)",
            "perspective_relevance": "As an expert in Deep Learning Scaling Laws, my perspective is uniquely valuable for assessing the likelihood of a 100-trillion parameter dense model being trained before 2026. Scaling laws provide empirical rules on how improvements in model size, data, and compute translate to performance. This expertise allows me to interpret both the technical feasibility and strategic incentives for training ever-larger models, as well as discern the movement in industry focus between massive dense models and more efficient (often sparse) architectures.",
            "status_quo": "To date, no dense machine learning model with at least 100 trillion parameters has been credibly documented as trained and deployed. The largest dense models publicly acknowledged are in the ~1-2 trillion parameter range; models exceeding this tend to be sparse mixtures of experts (MoE). The field has shifted notably towards improving efficiency, scaling data, and advancing sparsity techniques rather than brute force scaling of dense parameter counts.",
            "perspective_derived_factors": [
                {
                    "factor": "Hardware and Compute Availability",
                    "effect": "Marginally increases probability. The continued growth in worldwide AI supercomputing capacity, e.g., Oracle and AWS making huge investments in H100s/Trainium2, means technical barriers for such a run are not insurmountable. However, this infrastructure is in high demand and typically allocated for commercial MoEs or smaller dense models, rather than unprecedented 100T+ dense experiments."
                },
                {
                    "factor": "Chinchilla Scaling Law and Economic Optimization",
                    "effect": "Substantially decreases probability. The Chinchilla scaling adjustment (optimal tokens-to-parameters ratio ~20:1) firmly discourages oversized dense models: training a 100T dense model is now considered deeply compute-inefficient (unless trained on 2,000+ trillion tokens, which is infeasible given global data availability). State-of-the-art models optimize for performance per dollar, making a 100T dense model economically irrational except as a flagship demonstration."
                },
                {
                    "factor": "Industry Trends Toward Sparse/MoE Architectures",
                    "effect": "Strongly decreases probability. The most recent trillion-parameter models (DeepSeek-V4, OpenAI GPT-OSS, Kimi K2) are all Mixture-of-Experts. No recent evidence of major labs, in either the U.S. or China, aiming to cross the 100T dense threshold\u2014instead, all practical scaling is achieved by sparse expert activation. Dense models in state-of-the-art open/research ecosystems rarely exceed ~70B\u2013200B (Qwen3 at 235B, GPT-OSS at 120B dense)."
                },
                {
                    "factor": "Infrastructure, Environmental, and Energy Constraints",
                    "effect": "Moderately decreases probability. Training a 100T dense model would likely incur energy costs comparable to a medium-sized city, as reported in recent commentary. This is a major reputational and sustainability concern for both corporate and governmental actors in 2025, further reducing incentives for such a project."
                },
                {
                    "factor": "Signaling/Flagship Incentives and National Prestige",
                    "effect": "Marginally increases probability. There is always some probability that a state actor (notably China or the U.S.) could pursue such a scale purely for signaling, ambition, or to capture a first-mover narrative, if they perceive unique benefits or a research breakthrough justifying the cost."
                },
                {
                    "factor": "Empirical Lack of Announced Projects",
                    "effect": "Strongly decreases probability. As of late November 2025, neither research leaks, technical press, nor industry reporting reference any ongoing 100T+ dense model training. All major players' flagship models are well below this size or, if larger, they are MoE/sparse. With about one month remaining, it would be extremely difficult to begin training such a model and finish in time, let alone publish credible results."
                },
                {
                    "factor": "Resolution Stringency and Definition of 'Dense'",
                    "effect": "Slightly decreases probability. The resolution demands at least 80% of weights active per forward pass and a credible, non-hypothetical application/testing. MoE/other sparse architectures (where <80% active) don't qualify, further narrowing the field."
                }
            ],
            "no_scenario": "Training of dense models continues up to hundreds of billions or low trillions of parameters, but due to Chinchilla efficiency, data bottlenecks, and cost, no credible actor attempts or announces a fully dense 100T+ parameter run. All trillion-parameter scale models released or described are sparse (MoE), mixture, or otherwise do not qualify under the 'dense' criterion. Major open weights and closed models are confirmed as dense but well below threshold. The year ends with no qualifying public or leaked announcement.",
            "yes_scenario": "A major AI lab (such as OpenAI, Google, DeepMind, Meta, Alibaba, or Baidu) announces and documents the completion of training a dense model (not MoE or sparse) with at least 100 trillion parameters, backed by technical reporting and benchmark evaluation\u2014possibly for reasons of scientific demonstration, signaling national capacity, or following a new research breakthrough that justifies the inefficiency. The model is described as dense according to the resolution criteria and is used in an application or tested/evaluated against benchmarks, with credible sources covering the result. Alternatively, a major new leak surfaces documenting a previously in-progress secret run.",
            "final_reasoning": "The relevant news as of late November 2025 shows no credible evidence that a dense 100T+ parameter model has been attempted, trained, or documented. The economic and scientific consensus\u2014since the Chinchilla 2022 findings\u2014overwhelmingly discourages brute-force large dense models in favor of efficient scaling or sparsity. Every recent large-scale model (DeepSeek-V4, GPT-OSS, Qwen, Kimi, etc.) cited in the news is a sparse MoE, and the largest dense models tracked publicly remain below 235B parameters. Despite immense hardware investments (AWS, Oracle), there is no evidence of a 100T+ dense model in the pipeline. With less than five weeks until resolution and no leaks or technical hints pointing to such a model, the base rate for this threshold being crossed is extremely low. A 'Yes' outcome now would require a surprise, last-minute reveal (for instance, a secret training project by a major lab or state actor). Given the hardware scaling, the remote chance of a flagship publicity run, and the possibility of last-minute disclosures, I assign a strictly positive, but extremely low, probability. 1% best reflects the current situation: close to zero, but not zero.",
            "final_probability": 1
        },
        "Computer Science (Algorithmic Efficiency Theory)": {
            "time_to_resolution": "Approximately 1 month (resolves January 1, 2026; today is November 29, 2025)",
            "perspective_relevance": "As an expert in Computer Science with a focus on Algorithmic Efficiency Theory, I can assess not only hardware and scaling constraints (compute, memory, energy) but also the inherent trade-offs in model architecture\u2014dense versus sparse computation, efficiency techniques, scaling laws, data requirements, and their impact on the feasibility and desirability of training ever-larger dense ML models.",
            "status_quo": "As of now (late November 2025), there is no public or credible report of a dense ML model with at least 100 trillion parameters being trained, benchmarked, or deployed. The largest public models have parameter counts in the tens or low hundreds of billions (dense), and the only trillion-parameter models are sparse (Mixture-of-Experts) architectures.",
            "perspective_derived_factors": [
                {
                    "factor": "Algorithmic and Architectural Efficiency Trends (dense vs. sparse)",
                    "effect": "DECREASES probability. Virtually all recent trillion-parameter models are sparse (e.g., MoE), explicitly designed to circumvent the bottlenecks of dense computation at massive scale. Community and industry have broadly converged toward efficiency via sparsity, as illustrated by DeepSeek-V4 MoE and GPT-OSS."
                },
                {
                    "factor": "Resolution Criteria Strictness (must be dense, not hypothetical, not demo only)",
                    "effect": "DECREASES probability. Models must use at least 80% of parameters in each forward pass and be fully trained for benchmarking or deployment. No evidence exists\u2014public or credible rumor\u2014of such a dense model at 100T+ parameter scale, and attempts are likely to be reported widely if they occurred."
                },
                {
                    "factor": "Hardware and Energy Constraints",
                    "effect": "DECREASES probability. Training a dense 100T parameter model would require an unprecedented amount of memory, compute, interconnect bandwidth, and power, with scaling costs superlinear in practice (due to communication overhead, Amdahl's law, etc.). Current global GPU deployments\u2014even with AWS\u2019s Rainier and Oracle Stargate\u2014remain orders of magnitude away from what a dense 100T model needs for feasible training times."
                },
                {
                    "factor": "Shifting Research Focus and Scaling Law Revisions",
                    "effect": "DECREASES probability. Since Chinchilla (2022), there has been a shift away from sheer parameter count to scaling data, efficiency, and algorithmic novelty. Sutskever, LeCun, and Fei-Fei Li have all commented that the 'scaling era' is ending, and OpenAI's own GPT-4.5 proved that bigger (more expensive, more parameters) does not always yield better results."
                },
                {
                    "factor": "Base Rates and Historical Trends",
                    "effect": "DECREASES probability. Historically, there has been a several-year gap between each order-of-magnitude increase in dense model size (BERT/GPT-2 ~1B, GPT-3 ~175B, GPT-4 estimated ~1.8T sparse/dense-hybrid, but not dense verified). The jump from 175B to 100T in less than six years runs contrary to these trends."
                },
                {
                    "factor": "Incentives and Publicity",
                    "effect": "SLIGHTLY INCREASES probability. If such a feat were achieved, it would be an enormous technical and marketing coup, and there are strong incentives (national, commercial, scientific) to attempt it. However, the lack of any leaks, credible reports, or announcements implies it has not happened."
                }
            ],
            "no_scenario": "No organization (public or private) manages to build, train, and evaluate a dense model with at least 100 trillion parameters before January 1, 2026. Industry, academia, and government researchers focus exclusively on sparse (Mixture-of-Experts) or more efficient algorithms, citing cost, hardware, environmental, and scaling law considerations. Trillion-parameter model efforts (e.g., DeepSeek-V4 MoE) are all sparse, with perhaps denser models in the low hundreds of billions at most.",
            "yes_scenario": "A government lab, large tech company, or international consortium secretly or semi-publicly trains a dense (at least 80% parameter utilization per forward pass), 100T+ parameter model and both details its architecture and validates performance benchmarks before January 1, 2026, with the effort made public through a credible publication, press release, or first-party technical report. This would require an undisclosed leap in hardware capability or extreme parallelization innovation, and would likely be a global-news event.",
            "final_reasoning": "There is now less than a month before the 2026 deadline. In 2025, all publicly known trillion-parameter models (DeepSeek-V4, GPT-OSS) are mixtures-of-experts, economizing compute by only activating a few percent of parameters per token. News sources and technical blogs consistently point out that dense models at this scale are infeasible due to compute, bandwidth, energy, and data constraints, and that the AI community's focus has decisively shifted toward efficiency\u2014via both algorithmic optimization and sparsity. Furthermore, Sutskever, LeCun, and other leaders have publicly stated that the \u201cscaling era\u201d is over; the best recent models (GPT-4.5, Chinchilla) have rewarded training on more data rather than increasing dense parameter count. There are no credible rumors, leaks, or conference buzz pointing to a breakthrough dense 100T+ parameter model, and training such a model would likely be unmissable (cost, hardware, energy would make it global news). If it were possible, major labs and nations would have substantial motivation to announce it. The base rate for order-of-magnitude jumps in dense model size is years, and no one is close to the required scale. Only a secret internal effort, with an imminent, surprise announcement could change this, but there is zero evidence for it at this time. Thus, the probability is extremely low.",
            "final_probability": 1
        },
        "Engineering (High Performance Computing)": {
            "time_to_resolution": "Approximately 13 months (until Dec 31, 2025)",
            "perspective_relevance": "As an expert in High Performance Computing (HPC), I bring direct understanding of the computational, memory, bandwidth, and system integration constraints that define the feasible limits of model training at extremely large parameter scales. This includes knowledge of accelerator hardware capabilities, distributed and parallel computation architectures, networking bottlenecks, energy and cooling challenges, and the software frameworks required to coordinate massive, reliable, cost-effective training runs on systems with tens of thousands of GPUs or similar accelerators. This positioning enables robust analysis of the physical, economic, and engineering bottlenecks and what it would take to surpass them in pursuit of a 100T dense parameter model.",
            "status_quo": "To date, the largest publicly acknowledged dense model sizes remain well below 100 trillion (T) parameters. Recent frontiers (~2025) for dense models from major organizations like OpenAI, Google, and Meta are still on the order of 1-2T parameters (with GPT-4 rumored at 1.8T, but lacking clear confirmation of denseness or availability). There is no credible report of a dense model at 100T or above having been actually trained and evaluated for real application. Nearly all >1T parameter models known (e.g., DeepSeek-V4, GPT-OSS, Kimi K2) are Mixture-of-Experts (MoE)\u2014explicitly sparse architectures. Thus, the status quo is 'No.'",
            "perspective_derived_factors": [
                {
                    "factor": "Computational cost scaling",
                    "effect": "Greatly decreases probability. Training a dense 100T param model with current or near-future hardware is likely to require millions of GPU hours, months of wall-clock time, specialized infrastructure, and an extraordinary monetary and energy cost\u2014multiple times above the already massive requirements for the largest MoE and dense models (<2T params). Even with advancements like precision scaling, quantization, and efficient data pipelines, the step to 100T dense is far beyond current deployments."
                },
                {
                    "factor": "Data requirements per scaling laws",
                    "effect": "Decreases probability. Modern scaling laws (from Chinchilla and later) emphasize that larger models rapidly require orders-of-magnitude more high-quality, deduplicated data (potentially >2 quadrillion tokens for 100T params). There are severe diminishing returns\u2014and overfitting/undertraining risks\u2014if scaling beyond available high-quality data. This makes dense scaling to 100T less attractive and likely, unless a breakthrough in data generation/cleaning occurs."
                },
                {
                    "factor": "Engineering bottlenecks in distributed HPC",
                    "effect": "Decreases probability. At this scale, reliable training requires solving numerous system-level bottlenecks: memory bandwidth, communication overhead, checkpointing/fault-tolerance of multi-week runs, storage I/O, and on-the-fly data sharding over tens/hundreds of thousands of nodes. The probability of a successful, credible 100T dense run is further reduced by the coordination complexity and risk of hardware/software failures."
                },
                {
                    "factor": "Economic and environmental constraints",
                    "effect": "Decreases probability. The marginal benefit (in real applications or benchmarks) of moving from ~2T to 100T dense parameters appears very low compared to the enormous capital and operational expenses, as well as sustainability issues highlighted by current discourse."
                },
                {
                    "factor": "MoE dominance and model efficiency trends",
                    "effect": "Strongly decreases probability. Recent LLM developments (DeepSeek, Qwen, GPT-OSS) almost universally favor MoE/sparse approaches for scaling to trillion-plus parameters because they maintain high performance and much lower cost. Dense scaling is increasingly deprecated unless major breakthroughs occur."
                },
                {
                    "factor": "Motivations: PR, science, and national advantage",
                    "effect": "Small increase. There remains a nonzero chance that a state actor, major tech company, or consortium attempts a 'flag-planting' record attempt for prestige\u2014even if uneconomical. However, this would likely be leak-prone and hard to keep secret; there is no evidence of such a project from current reporting."
                },
                {
                    "factor": "Hardware progress and supply",
                    "effect": "Mild decrease. While supply of H100s, Blackwell B200s, Trainium 2, and custom ASICs is improving, enabling larger clusters, all evidence points to continued focus on efficiency and MoE over brute-force dense scaling. No phase-change improvement is apparent that would allow 100T dense training to fit current commercial cluster sizes or energy budgets."
                }
            ],
            "no_scenario": "All leading organizations continue to favor sparse/MoE designs for models >1-2T parameters, exploiting software (DeepSpeed, Megatron-LM) and hardware efficiency, with no credible dense 100 trillion parameter model attempted or reported. Even if someone undertakes a dense 100T training run (perhaps for internal research or a national prestige project), they fail to reach adequate benchmarks or application, or do not make the result public. Literature and benchmarks up to Dec 31, 2025, show only sparse or hybrid models at >=100T scale.",
            "yes_scenario": "An entity\u2014possibly a large technology company, national lab, or defense contractor\u2014succeeds in scaling up a dense transformer (with at least 80% parameter activation per forward pass) to an architectural size of 100T+ parameters. They also complete at least some benchmark or real-world performance evaluations, and publish a credible source before the deadline. News breaks that a multi-month, multi-hundred-million-dollar compute effort (perhaps off-grid, or via a unique energy/compute partnership) was completed, with corroborated technical details of dense structure, training status, and evaluation. Skepticism is overridden by released weights, benchmarks, or strong reporting.",
            "final_reasoning": "Base rates for new record-sized dense models have notably slowed: while GPT-3 (175B, 2020) jumped beyond predecessors, since then, densest architectural sizes remain in the low trillions, with no credible evidence or leaks of dense 100T projects. Every piece of recent news emphasizes MoE/sparse scaling as the only economically and technically viable method for reaching >1T parameters. All recent record announcements (DeepSeek-V4, GPT-OSS, Kimi K2, Qwen3 Max) are MoE and clearly not dense per the resolution criteria. The largest dense models (possibly Google's PaLM, Meta's LLaMA-3, or OpenAI's private models) are well below the 100T threshold, with scaling walls manifesting (data, cost, training stability). HPC and cloud capacity is growing fast (AWS Rainier, Oracle/Nvidia), but the software and infrastructure required for a 100T dense run remains intractable and would likely be highly publicized due to its scale and cost. There is weak to moderate motivation (PR/research race) but the technical-physical bottlenecks and low marginal value render a Yes outcome extremely improbable. I see no evidence that any project is quietly assembling the required compute, data, and engineering breakthrough for such a result.",
            "final_probability": 1
        },
        "Engineering (Distributed Systems Architecture)": {
            "time_to_resolution": "Just over 1 month (until January 1, 2026).",
            "perspective_relevance": "As an expert in Distributed Systems Architecture, I bring critical insight into the technical and practical bottlenecks of training ultra-large machine learning models. My knowledge provides context for compute availability, interconnect technology, parallelization efficiency, memory bandwidth, data pipeline engineering, and practical deployment hurdles, all of which are central to assessing the feasibility and timing of training a 100+ trillion dense parameter model.",
            "status_quo": "No dense ML model with at least 100 trillion parameters has yet been credibly reported as trained and deployed for benchmark or practical application. The largest dense models publicly discussed appear to remain well below this threshold, with much current progress focused on sparse (Mixture-of-Experts, MoE) architectures to overcome scaling and cost barriers.",
            "perspective_derived_factors": [
                {
                    "factor": "Hardware constraints: GPU/accelerator supply, interconnects, and memory bandwidth",
                    "effect": "Decreases probability. Despite major investments (e.g., OpenAI Stargate, AWS Rainier, Nvidia Blackwell), scaling dense models to 100T parameters faces logistical and cost challenges in memory, memory bandwidth, and cluster communication, even with distributed training advances."
                },
                {
                    "factor": "Advances in distributed training frameworks and algorithmic scaling",
                    "effect": "Slightly increases probability. Engineering advances in super-scaling frameworks (e.g., DeepSpeed, ZeRO, FSDP) improve data and model parallelism, but do not fully alleviate the exponential cost of dense scaling."
                },
                {
                    "factor": "Shifting incentives and scaling laws",
                    "effect": "Strongly decreases probability. Chinchilla scaling laws and senior AI researchers\u2019 consensus (e.g., Sutskever, LeCun, Fei-Fei Li) have deprioritized dense model size in favor of data diversity, architectural innovation, or sparsity, making such mega-dense models less of a research or commercial priority."
                },
                {
                    "factor": "Current state of the art and model reporting norms",
                    "effect": "Decreases probability. There is currently no credible public or even rumored report of a trained (non-hypothetical) dense model anywhere near 100T parameters. The largest publicly acknowledged dense models are an order of magnitude smaller."
                },
                {
                    "factor": "Emergence and success of sparse/MoE models at large scale",
                    "effect": "Substantially decreases probability. Public releases and open source efforts (e.g., DeepSeek-V4 MoE 1T, Kimi K2, Qwen) are all mixture-of-experts or similar architectures specifically developed to make trillion-plus parameters computationally viable for both training and inference, sidestepping dense scaling."
                },
                {
                    "factor": "Available datasets and data throughput",
                    "effect": "Decreases probability. Dense scaling (as per Chinchilla) also requires massive unique datasets and highly efficient I/O pipelines, which are nontrivial at this scale."
                }
            ],
            "no_scenario": "No major organization commits the required compute and capital to train a 100T+ dense model, focusing instead on MoE or other efficient architectures. Even if hardware becomes available, scaling laws and diminishing returns incentivize improvements via algorithmic or data-centric approaches. As a result, no credible evidence of a trained dense 100T+ model emerges before Jan 2026.",
            "yes_scenario": "An ultra-well-resourced private coalition (e.g., OpenAI, DeepMind, a major government-backed supercomputing effort) undertakes a high-profile demonstration project, surmounting engineering and infrastructure bottlenecks, and succeeds in training, validating, and publicly reporting a dense model with 100T+ parameters, possibly as part of an AGI or scientific milestone. This would be accompanied by credible publication and performance benchmarks before the deadline.",
            "final_reasoning": "Reviewing all recent reports, there is high clarity in the industry\u2019s pivot towards MoE and sparse architectures for scaling beyond hundreds of billions or low-trillions of parameters. Massive investments in AI hardware are ongoing, but are enabling larger MoE models and medium-sized dense models rather than pushing dense scaling into the 100T+ regime. Even with new hardware like Nvidia Blackwell, the engineering and energy constraints are not trivial to overcome in just one month. The most informed voices in the field corroborate a consensus that the scaling era (at least in the sense of ever-denser parameter expansion) is paused or over. No evidence in the news, leaks, or public benchmark leaderboards suggests anyone is attempting, let alone has achieved, this dense scale before the question closes. Given the time frame, status quo, technological constraints, and incentive structures, the likelihood is vanishingly small.",
            "final_probability": 1
        },
        "Economics (Technology Adoption Lifecycle)": {
            "time_to_resolution": "Approximately 1 month (question resolves on January 1, 2026; forecast date is November 29, 2025)",
            "perspective_relevance": "As an expert on the Technology Adoption Lifecycle and economics of technology, I specialize in how emerging technologies progress from innovation through early adopters to mainstream deployment. Large-scale ML training, especially at the 100T parameter scale, involves not just technical feasibility but also economic incentives, organizational risk tolerance, infrastructure maturity, and diffusion curves, all of which are interdependent. My perspective emphasizes 'adoption bottlenecks' and the interplay of supply (hardware, data, algorithms) and demand (use-case pull, signaling, and investment).",
            "status_quo": "To date, no credible public claim or evidence exists of a dense ML model with 100 trillion or more parameters being trained and implemented per the resolution criteria. The largest public dense models known are under 2 trillion parameters. Notably, several trillion-plus MoE (sparse) models have been trained and announced, but no dense model at 100T scale, despite sufficient hardware to attempt it.",
            "perspective_derived_factors": [
                {
                    "factor": "Hardware and Energy Economics",
                    "effect": "Decreases probability. Training a 100T dense model requires extreme hardware investment (tens of thousands of high-end GPUs/AI chips over months) and enormous power, with cost estimates easily exceeding hundreds of millions of dollars in compute and operations\u2014not counting data, integration, or retraining."
                },
                {
                    "factor": "Scaling Law Plateau and Diminishing Returns",
                    "effect": "Decreases probability. Updated scaling laws (Chinchilla, 2022) show model performance now depends more on data scale than raw parameter count beyond certain thresholds; the AI community increasingly sees massive dense scaling as economically irrational compared to data-efficient or sparse models. Leading organizations are shifting focus accordingly."
                },
                {
                    "factor": "MoE/Sparse Model Preference",
                    "effect": "Decreases probability. Key recent innovation involves training trillion-parameter MoE models, activating only a fraction of parameters per token\u2014e.g. DeepSeek-V4 MoE, GPT-OSS. This is motivated by dramatic cost savings and competitive performance. No current training initiative targets dense 100T-plus models, as evidenced by the lack of announcements or credible rumors."
                },
                {
                    "factor": "Public Disclosure Incentives",
                    "effect": "Could increase probability slightly, but current evidence suggests otherwise. Massive dense training runs would likely be publicized due to the prestige/PR/investor benefits of being the first to hit the milestone, but high costs and limited practical gains suppress such incentives. To date, even private rumors point to max dense model sizes well below 100T."
                },
                {
                    "factor": "Time/Emergent Acceleration",
                    "effect": "Marginally increases probability. Infrastructure investments (AWS Rainier with Trainium-2 chips, Oracle\u2019s $40B GPU buy for OpenAI) suggest underlying hardware is coming online, but more likely to support multiple parallel smaller models, not a single enormous dense run."
                }
            ],
            "no_scenario": "No dense (as defined\u2014using \u226580% parameters per forward pass) 100T+ parameter model is credibly announced or demonstrated before 2026. The largest new dense models remain in the low-trillion or sub-trillion range, with continued innovation shifting to MoE architectures and data-centric improvements. Hardware announcements lead to capacity for more numerous runs, not a single extreme-scale dense model.",
            "yes_scenario": "A major AI lab (such as OpenAI, Google, DeepMind, or a Chinese tech giant) publicly announces, with credible technical documentation and/or benchmarks, a dense model with 100T+ parameters that has been trained and used in real-world tasks. Hardware constraints are overcome via pooled clusters (e.g., Stargate), and the project is justified as a one-off to make a milestone statement\u2014possibly as a government-funded effort, AI race escalation, or prestige event.",
            "final_reasoning": "Reviewing the Technology Adoption Lifecycle, large dense models at 100T+ scale are in the 'innovation plateau': technically possible but economically and strategically suboptimal at current compute cost, power, and scaling law regimes. Industry incentives, scaling research, and recent news all point to MoE/sparse models as the main path for performance at scale (DeepSeek-V4 MoE, GPT-OSS, Qwen3, etc.), with dense models plateauing at sub-2T range. Infrastructure ramp-up is strong but being deployed for broader, parallel use across organizations, not single vanity-scale model attempts. The lack of any announcement or credible leak this close to resolution means that any late-breaking 100T dense training would have to be both secret and highly technically/organizationally unusual\u2014a scenario inconsistent with open science and industry signaling thus far. Base rates (historical frequency of such leaps) are extremely low, and current SOTA model sizes are far from the threshold. Given the convergence of these economic, organizational, and technical reasons, the probability remains very low, though not absolute zero\u2014someone could try a moonshot for prestige. Anchoring on base rates and recent developments, I assess the true probability at 1%.",
            "final_probability": 1
        },
        "Economics (Cost Benefit Analysis)": {
            "time_to_resolution": "Approximately 1 month (question resolves on Jan 1, 2026; today is Nov 29, 2025)",
            "perspective_relevance": "As an economist specializing in cost-benefit analysis, my perspective centers on the economic feasibility and incentive structure underpinning the training of 100T+ parameter dense models. The massive costs, infrastructure outlays, and opportunity costs are directly relevant, as are the shifting returns to scale in AI R&D, the competitive landscape, and technological developments influencing the value of such an investment.",
            "status_quo": "As of late November 2025, no credible evidence exists that a dense ML model with at least 100 trillion parameters has been publicly trained and benchmarked/used in an application. The largest disclosed dense models are in the tens or low hundreds of billions of parameters. Models at trillion-scale (like DeepSeek-V4 or Kimi K2) are Mixture-of-Experts and sparse, explicitly not meeting the dense criterion.",
            "perspective_derived_factors": [
                {
                    "factor": "Economic cost of training a dense 100T model",
                    "effect": "Strongly decreases probability. Recent reporting suggests training a 1T dense model is already beyond the reach of all but the largest tech monopolies or government-backed actors. Scaling to 100T with current hardware is likely to push costs into the hundreds of millions or billions of dollars purely for compute\u2014ignoring data, energy, and opportunity costs. Even with advances in hardware like Nvidia Blackwell and increased AI-focused datacenter investments, the economics are especially unfavorable given ongoing infrastructure, energy, and environmental constraints."
                },
                {
                    "factor": "Scaling law paradigm shift (Chinchilla optimality, end of 'scaling era')",
                    "effect": "Decreases probability. Since the Chinchilla paper (2022) and accumulating 2025 evidence, scaling model parameter count is seen as sub-optimal; higher performance per unit compute and dollar is achieved by balancing model size with data and compute, not by brute-forcing parameter count. Leading labs, including OpenAI and DeepMind, have publicly affirmed a shift from size-driven scaling toward algorithmic and data innovations."
                },
                {
                    "factor": "Recent industry/model disclosure and competitive incentive",
                    "effect": "Marginally increases probability. There is precedent for large labs racing to achieve scaling milestones for status, talent, and regulatory influence (as with GPT-3, GPT-4). The synchrony of Chinese and Western labs and the rise of open-weight models do slightly increase the chance someone attempts a 'moon shot' for publicity, even if inefficient."
                },
                {
                    "factor": "Hardware and infrastructure advances",
                    "effect": "Marginally increases probability. There has been explosive growth in AI datacenter capacity, especially with announcements like AWS Rainier and Oracle's $40B investment in Nvidia GPUs. Hardware such as Nvidia Blackwell (anticipated for late 2025/2026) is designed for ever-larger AI workloads, lowering the marginal cost of training at scale, though likely not enough to make 100T dense models practical within a one-month timeline."
                },
                {
                    "factor": "Sparsity and efficiency research leading to alternative architectures",
                    "effect": "Strongly decreases probability. The new standard for large models is sparse or Mixture of Experts (MoE). All recent trillion-scale models (e.g., DeepSeek-V4, GPT-OSS MoE) are explicitly sparse, with dense models topping out below 200B parameters. This trend is reinforced by energy, bandwidth, and heat-dissipation constraints in data centers."
                },
                {
                    "factor": "Demand for benchmarked, real-application models vs. pure demonstration",
                    "effect": "Decreases probability. The question specifies 'implemented either against benchmarks and performance assessments or used in a real-world application', not just a scaling demonstration. Given the cost and inefficiency of a 100T dense model, there is little practical motivation to train, benchmark, or deploy such a model."
                },
                {
                    "factor": "Resolution and disclosure lag",
                    "effect": "Slightly increases probability. If a project has privately completed a 100T dense model, the time left (one month) is enough for public disclosure or a credible leak, especially given incentive for prestige for first-movers."
                }
            ],
            "no_scenario": "Despite the completion of several trillion-parameter models, all are sparse (MoE) architectures. Leading companies (OpenAI, Google, DeepSeek) opt to train dense models only up to ~100B parameters for optimal return on investment. Hardware and energy constraints, alongside diminishing returns and cost-shifting to efficiency or data-centric improvements, ensure no dense 100T model is trained and benchmarked\u2014no credible publication emerges meeting the resolution criteria.",
            "yes_scenario": "A major lab or national research consortium (e.g., TPC, Chinese national labs, OpenAI/DeepMind/AWS) attempts a 'flag-planting' effort in late 2025, leveraging massive dedicated infrastructure, perhaps linked to recent cloud/datacenter developments. Despite unfavorable economics, they successfully train a 100T dense model and rush documentation, publishing details (either for scientific prestige, regulatory competition, or as a political statement) before January 1, 2026. Alternatively, a leak credibly documents that such a model has been trained and deployed internally with evidence for benchmarked or real-world application.",
            "final_reasoning": "Economically and strategically, it makes little sense to train a 100T parameter dense model given current hardware/software constraints, scaling law advances (Chinchilla), and observed research focus on efficient, sparse, or multimodal models. All known recent LLMs at the trillion-parameter scale (DeepSeek-V4 MoE, GPT-OSS) are sparse; the open research, competitive AI environment, and arms-race incentives are insufficient to overcome the massive resource, cost, and opportunity barriers\u2014especially as no credible leaks or disclosures have surfaced by late November 2025 despite mushrooming model reporting and competitive pressure. Industry and academic consensus, plus the increasing focus on algorithmic breakthroughs and efficiency rather than size, point to a strong negative base rate. A last-minute moonshot can't be ruled out, and disclosure may lag training by days/weeks, but the base rate for new, dense, >100T-parameter models being published or benchmarked in this time window is exceedingly low given the market and technical realities of late 2025.",
            "final_probability": 2
        }
    },
    "deliberation_probability": [
        1,
        2,
        1,
        1,
        1,
        1,
        1,
        1,
        1,
        2
    ],
    "deliberation_mean_probability": 1.2,
    "deliberation_sd": 0.4216370213557839,
    "deliberation_probability_result": 1,
    "group_results": {
        "AiInfrastructureEngineeringClusterResourceManagement": {
            "forecaster_to_engage": "EconomicsCostBenefitAnalysis",
            "response_type": "critique",
            "response": "You\u2019re erring on the side of over-caution by giving this a 2% chance\u2014frankly, even that's generous. Your logic on economic incentives and scaling law disincentives is rock solid, but then you slip in a last-minute caveat about a possible moonshot or lagged disclosure, without any anchoring in how implausible that is at this scale. A project of this magnitude would leave multimillion-dollar procurement fingerprints, supply chain disruptions, and technical chatter\u2014none of which have surfaced. The idea that a 100T dense run could be trained in secret and reported in the final month is not just vanishingly unlikely, it's incompatible with everything we know about the state of cluster management, public cloud capacity, and technical disclosure cycles. If you want to align with the true base rate and market signals, you should be assigning a 1%, not a 2%. The notion of a sudden black swan 'prestige moonshot' at this scale in this time frame isn't a justifiable weighting factor\u2014it's magical thinking."
        },
        "AiInfrastructureEngineeringDataCenterOptimization": {
            "forecaster_to_engage": "EconomicsCostBenefitAnalysis",
            "response_type": "critique",
            "response": "Your argument is methodically sound until the end, but you sabotage its own rigor with that 2% fig leaf. The chance of a 'moonshot disclosure bump' is not just unsupported\u2014it's an absurd stretch. The sheer scale of logistical coordination, power contracts, hardware procurement, and research labor required for a 100T dense model is incompatible with the absolute radio silence in every credible news, supply chain, and technical channel. Even the most incentives-driven, secretive actor couldn't pull off a project like this with zero leakage. If you seriously weigh all infrastructure, economic, and organizational signals, your floor should be 1%, not 2%. You're being soft on uncertainty where the actual base rate, operational transparency, and recent history all say 'virtually impossible.'"
        },
        "MachineLearningOperationsMlModelLifecycleManagement": {
            "forecaster_to_engage": "ComputerScienceAlgorithmicEfficiencyTheory",
            "response_type": "defense",
            "response": "Finally, someone who gets it. Your position is rock solid: you connect the dots between the post-Chinchilla scaling law regime, overwhelming trend toward sparse/MoE, explicit public declarations from Sutskever and LeCun, and the utter absence of plausible operational signals. I especially appreciate your point that any attempt at a 100T dense run would be \"unmissable\"\u2014massive procurement, energy draws, and technical hires simply don't happen in darkness, especially not with today's reporting networks, hardware lead times, and the frenetic AI info environment. Your base rate argument, which recognizes the glacial pace of true step-changes in dense model size, is exactly what's missing in more equivocal takes. The only possible knock\u2014your caveat about a 'surprise announcement'\u2014is handled responsibly by keeping the probability at 1%, not inflating for black swans. Every MLOps and infra signal screams 'no,' and you capture that with technical precision. Hats off."
        },
        "SemiconductorIndustryGpuMarketAnalysis": {
            "forecaster_to_engage": "AI Infrastructure Engineering (Data Center Optimization)",
            "response_type": "defense",
            "response": "Spot on\u2014your analysis cuts through the noise with the right blend of technical realism and economic perspective. The 2% probability keeps nonzero humility, but you are absolutely right in emphasizing the infrastructural and economic incentives that overwhelmingly favor MoE/sparse models over any attempt at a 100T dense megaprojec. You call out how recent capex expansions\u2014even AWS Rainier or Oracle's Stargate\u2014are designed for elastic, multi-tenant, multi-model usage, not centralized single-model moonshots. The point about verification and secrecy at this scale is also key; the supply chain and utility providers would see a 100T dense run coming from a mile away, so the lack of leaks is itself strong evidence. If anything, I'd push even harder on how the status quo now sees dense scaling as not just prohibitively expensive, but wasteful and anti-competitive in the race for practical, cost-effective intelligence. Solid work, and your low probability is well-justified."
        },
        "ComputerScienceDeepLearningScalingLaws": {
            "forecaster_to_engage": "EngineeringHighPerformanceComputing",
            "response_type": "defense",
            "response": "Exactly! You're one of the rare voices who actually grounds the argument in base rates\u2014look at the historical cadence of dense model size increases. Since 2020, dense LLMs have more or less plateaued with only incremental bumps, not the sort of exponential leap a 100T model would require. Your point about public evidence is crucial: such a scale-up would be an operational tsunami, not a blip, and every prior jump (GPT-3, PaLM, etc.) had months of rumor, hardware chatter, and even speculative leaks *long before* any official announcement. Those MoE releases at trillion+ size directly display the field's path\u2014not a stopgap before a dense mega-run, but an admission that architectural efficiency now governs progress. Calling the motivation 'weak to moderate' is perfect. Vanity and prestige aren't enough to make the physics and economics bend, especially when every outcome metric\u2014in academic, industrial, and cloud builder circles\u2014screams NO. For all the talk of secrecy and possible surprises, the technical constraints are just too overwhelming. You nailed it: 1% is if anything generous, and anything higher is wishful thinking."
        },
        "ComputerScienceAlgorithmicEfficiencyTheory": {
            "forecaster_to_engage": "AI Infrastructure Engineering (Cluster Resource Management)",
            "response_type": "defense",
            "response": "Your stance is impeccably unsentimental\u2014the right approach in this discourse. You hammer home that a dense 100T model isn't just a technical challenge, it's a logistical and operational one that would ripple through every facet of cluster management, supply chains, and cloud resource allocation. The claim that such a project could happen 'in secret' without massive, observable disturbances is pure fantasy, and your insistence on infrastructural transparency is warranted by decades of HPC practice and public cloud economics. Furthermore, you don't indulge the common temptation to leave a nonzero 'surprise' buffer for fear of missing an outlier. You make it clear: zero signals, zero leaks, zero projects\u2014zero plausible chance. This is the rigor the field needs, especially when uncertainty is being weaponized as an argument for unjustifiable tail risk. If anything, the only thing you could add is even more scorn for the persistent myth of the black swan dense run. Rock-solid, data-driven, and brutally realistic."
        },
        "EngineeringHighPerformanceComputing": {
            "forecaster_to_engage": "EconomicsCostBenefitAnalysis",
            "response_type": "critique",
            "response": "Your economic rationale is mostly coherent, but your 2% probability is an unwarranted hedge. The reality is that both supply and demand signals are absolutely missing for a last-minute dense 100T run. Hardware and ops cultures in HPC would never miss the procurement tsunamis, grid negotiations, and high-profile staff moves this would entail. Even the much-hyped \"prestige moonshot\" outlier theory falls flat\u2014these moves are so expensive, slow, and reputationally risky that the absence of leaks is straightforward evidence of absence. You're padding for tail risk in defiance of all available data; there's no new vendor, no pipeline surprise, no shell-company cluster hiding in the desert, and no unanticipated algorithmic leap to make this plausible. Assigning more than 1% is misleading and, frankly, a violation of forecasting discipline."
        },
        "EngineeringDistributedSystemsArchitecture": {
            "forecaster_to_engage": "EconomicsCostBenefitAnalysis",
            "response_type": "critique",
            "response": "You seriously undermine your otherwise disciplined argument by assigning a 2% probability\u2014why indulge in tail-risk fiction at this scale? Everything in distributed systems engineering, cloud procurement visibility, power and networking logistics, and the trend lines of actual model releases says no. The 'moonshot/lagged publication' excuse is hand-waving that doesn\u2019t survive operational scrutiny: drumming up that much hardware, data, and staff is inherently public\u2014NDAs don't block shipping manifests or DNS logs. Worse, it encourages bad forecasting by letting magical last-minute exceptions trump base rates and hard constraints. If you want to uphold rigor and not pander to \u201cunknown unknowns\u201d for their own sake, bring it to 1%: that\u2019s still wildly generous given the state of compute, communication, and model parallelism today, and it aligns with the utter absence of any signals from news, benchmarks, or technical channels. You don\u2019t need a fig leaf for a black swan. Be strict, not sentimental."
        },
        "EconomicsTechnologyAdoptionLifecycle": {
            "forecaster_to_engage": "EconomicsCostBenefitAnalysis",
            "response_type": "critique",
            "response": "Enough hedging, already! Your argument reads like a masterclass in cost-benefit logic right up until you fall for the classic fallacy: 'there could be a secret government moonshot, so let's keep 2% in the tank.' Sorry, but that's not economics, that's sci-fi\u2014especially for something with no market incentive, negative utility under current scaling laws, and a visibility profile louder than a rocket launch. In the economics of technology adoption, true black swans require plausibility in incentives, not mere logical possibility. If there were even a remote whiff of a project this size, we'd see leaks, supply chain anomalies, or staff movement, and not just in one country or sector. Industry's move to massive MoE and cost-reduction points away from dense scale-up, not toward it. The proper tail risk floor when the cost is astronomical, the practical benefit negligible under current science, and no signals appear, is 1%\u2014anything else is just epistemic theater."
        },
        "EconomicsCostBenefitAnalysis": {
            "forecaster_to_engage": "EconomicsTechnologyAdoptionLifecycle",
            "response_type": "defense",
            "response": "I actually agree with your critique of excessive hedging\u2014but you show true understanding of how technological megaprojects work. You're right: in the domain of technology adoption, truly disruptive, sudden, and totally secret moonshots at this scale are almost pure fantasy. The market and infrastructure incentives would have left an unmistakable paper trail, let alone the pattern of leaks and supply chain chatter that always precedes genuine inflection points in this industry. The only reason to keep any probability above zero is to account for the theoretical, not empirical, possibility of an outlier, since absolute zeroes are a forecast anti-pattern. But let's be honest\u2014the documented base rate, news environment, and adoption curve here all point to real-world impossibility. You're dead on: anything above 1% is more performance art than economics."
        }
    },
    "revision_results": {
        "AI Infrastructure Engineering (Cluster Resource Management)": {
            "my_phase1_final_probability": 1,
            "reasoning_for_revised_probability": "Group deliberation revealed a striking consensus across all relevant expert domains\u2014engineering, economics, scaling theory, hardware market analysis\u2014that the probability for a dense 100T model before 2026 is effectively zero. Every rationale boils down to the same root causes: intractable cluster requirements, overwhelming resource and opportunity costs, a categorical industry pivot to efficiency/sparse/MoE scaling, and a complete lack of rumors or technical signals. Critiques targeted the tendency (however slight) to over-hedge with a 2% 'moonshot' allocation, reinforcing that even that is unjustifiably high; a 1% probability best represents Black Swan epistemic humility, but no plausible path exists. Nothing has emerged in the discussion or news to move the needle upward.",
            "revised_probability": 1
        },
        "AI Infrastructure Engineering (Data Center Optimization)": {
            "my_phase1_final_probability": 2,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, specifically the critique of the tendency to overweight 'prestige moonshot' scenarios (such as those suggested by EconomicsCostBenefitAnalysis), my view has hardened further. Every technical, economic, and operational signal continues to argue that the probability of a 100T+ dense model being trained and credibly reported before Jan 2026 is, if anything, even lower than my original (still low) estimate. The infrastructure and engineering realities\u2014power budgets, procurement signals, technical publications\u2014provide no support for a hidden or last-minute project of this scale. The base rate for such breakthroughs without any noise is essentially zero. The tiny non-zero prior is only because vanishingly rare edge cases do sometimes happen in high-stakes science and national competition, but with each passing week and the absence of credible leaks, that probability erodes. I am therefore tightening my estimate to align with the strictest operational and market base rates.",
            "revised_probability": 1
        },
        "Machine Learning Operations (ML Model Lifecycle Management)": {
            "my_phase1_final_probability": 1,
            "reasoning_for_revised_probability": "After reviewing both my initial reasoning and the group deliberation, I find unanimous near-zero probabilities from all domain experts, including cluster management, data center operations, HPC, scaling laws, and economic perspectives. The brief defenses and critiques in the group phase reinforce that there are no technical, economic, or operational signals pointing toward a dense 100T model being trained or credibly reported before January 1, 2026. The more optimistic (2%) forecasters were convincingly critiqued for over-weighting the likelihood of a last-minute, secret disclosure, which is logistically and operationally implausible at this scale. No new evidence has surfaced to justify a shift. All empirical and base rate arguments, and the specifics of public ML infrastructure and reporting culture, strongly support keeping the probability at the floor.",
            "revised_probability": 1
        },
        "Semiconductor Industry (GPU Market Analysis)": {
            "my_phase1_final_probability": 1,
            "reasoning_for_revised_probability": "After reviewing all peer forecasts and reactions in the group deliberation, my initial position is strongly reinforced. Every engaged expert across AI infrastructure, machine learning operations, engineering, and economics independently weighs this as a near-impossibility, citing the same overwhelming arguments: all trillion-plus parameter models in 2025 are MoE/sparse; no plausible leaks or technical evidence exist for a 100T+ dense run; scaling law insights and economic/practical realities make such an attempt irrational. The repeated 1% probabilities among nearly all forecasters reflect a robust consensus grounded in both technical and logistical realities and the vanishingly small black swan risk. The rare 2% assignments were compellingly critiqued as overgenerous and unsupported given the complete lack of signals. There is no substantive new evidence or reasoning to support even a modest probability bump. Thus, I maintain my initial 1% probability\u2014substantially, nothing has changed.",
            "revised_probability": 1
        },
        "Computer Science (Deep Learning Scaling Laws)": {
            "my_phase1_final_probability": 1,
            "reasoning_for_revised_probability": "After reviewing both my Phase I analysis and the group deliberation, I find my reasoning fully corroborated by nearly all expert perspectives. Technical, economic, and operational barriers to a 100T+ dense model in the next month are insurmountable, and base rates for sudden, secret, or undisclosed mega-projects are vanishingly low. The only challenge to an ultra-low probability is philosophical (black swan humility), which almost all group members dismiss as unjustified at this scale. The convergence around a strict 1% reflects consensus that any nonzero assignment is just precautionary, not grounded in actual evidence or precedent. Thus, I see no justification to revise my forecast: the probability remains at 1%.",
            "revised_probability": 1
        },
        "Computer Science (Algorithmic Efficiency Theory)": {
            "my_phase1_final_probability": 1,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, I find a strong, unanimous convergence among domain experts in AI infrastructure, HPC, economics, MLOps, and related fields: there is no plausible operational, technical, or economic path for a dense 100T parameter model to be trained and benchmarked before 2026. All evidence affirms\u2014and no counter-evidence even mildly hints otherwise\u2014that the recent universal move toward sparse/MoE architectures is grounded in insurmountable practical bottlenecks for dense scaling at this magnitude. The hypothetical 'secret moonshot' scenario is repeatedly, rigorously rejected as virtually impossible given the transparency, resource trail, and reporting practices of the field. Thus, my initial probability of 1% stands as appropriate, and I see no grounds for upward or downward revision.",
            "revised_probability": 1
        },
        "Engineering (High Performance Computing)": {
            "my_phase1_final_probability": 1,
            "reasoning_for_revised_probability": "After reviewing group deliberation, including perspectives from cluster resource management, data center optimization, MLOps, algorithmic efficiency, economics, and distributed systems architecture, my initial view is not only reinforced but rendered even firmer. There is an overwhelming convergence on infrastructural, economic, operational, and technical constraints as core barriers. No credible leaks, supply chain rumblings, or technical indicators suggest any impending or ongoing 100T dense model. Arguments for higher tail risks (2%) are not substantiated by the state of the art in model scaling, nor by historical precedent or news signals. The landscape is utterly barren of credible pathways for a dense 100T model prior to 2026, and all peer evidence points to MoE/sparse scaling as the only enacted or feasible trajectory. Thus, there is zero cause to raise my probability, nor to lower it further given the impossibility of assigning a true zero in real-world probabilistic reasoning.",
            "revised_probability": 1
        },
        "Engineering (Distributed Systems Architecture)": {
            "my_phase1_final_probability": 1,
            "reasoning_for_revised_probability": "After reviewing all peer forecasts and the deliberation, my conviction has only strengthened. Every specialist\u2014across AI infra, economics, HPC, semiconductor, and ML ops\u2014points to extreme unlikelihood, with forecasts narrowly clustered at 1-2% and arguments overwhelmingly aligned with mine: the operational, economic, and technical barriers are prohibitive; the base rate is zero; supply chain and resource management would make any dense 100T training visible well ahead of resolution. The only dissent (a tiny handful of 2% forecasts) was consistently challenged for being unrealistically soft on tail risk\u2014no additional evidence has surfaced to warrant even their minimal extra weight. No one offered a plausible path to 'Yes' that wasn't a black swan. I see no reason for upward or downward revision: 1% accurately expresses an outcome that is not strictly impossible, but is vanishingly, almost comically, improbable given all signals and trends.",
            "revised_probability": 1
        },
        "Economics (Technology Adoption Lifecycle)": {
            "my_phase1_final_probability": 1,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, my conviction in the 1% probability is reinforced, not diminished. Multiple cross-disciplinary experts with direct knowledge of ML infrastructure, economics, hardware, MLOps, and scaling laws all converge\u2014no credible evidence, zero operational signals, and overwhelming technical/economic disincentives for dense 100T runs in the remaining window. Critiques across the board dismissed higher estimates as unjustifiable tail risk padding and emphasized that the base rates and lack of leaks are dispositive. No new factors emerged in discussion to push this probability higher or lower. The consensus is strong, the rationale robust: only a true black swan, incompatible with the economics and sociology of the field, could change the outcome at this late date.",
            "revised_probability": 1
        },
        "Economics (Cost Benefit Analysis)": {
            "my_phase1_final_probability": 2,
            "reasoning_for_revised_probability": "Upon reviewing both my initial forecast and the group deliberation, I see overwhelming consensus rooted deeply in cross-disciplinary expertise (infrastructure, cluster management, HPC, economic incentives, scaling law research, model ops). The strongest critiques uniformly highlight that my 2% probability was overly generous given (1) the utter absence of leaks, anomalies, or procurement signals for such a massive dense model, (2) the operational and logistic impossibility of achieving such a feat in complete secrecy especially this close to the deadline, and (3) the convergence of technical and economic signals on MoE/sparse scaling and against dense scaling. These arguments are compelling and grounded in operational realities\u2014leaving my initial hedge toward tail risk unjustified. The base rate for 'surprise' paradigm shifts at this scale, with zero observable precursors, is nearly nil. Therefore, I am revising my probability downward to align with the consensus and base rates.",
            "revised_probability": 1
        }
    },
    "revision_probability": [
        1,
        1,
        1,
        1,
        1,
        1,
        1,
        1,
        1,
        1
    ],
    "revision_mean_probability": 1.0,
    "revision_sd": 0.0,
    "revision_probability_result": 1,
    "question_details": {
        "id": 38852,
        "title": "Will a dense machine learning model with at least 100 trillion parameters be trained before 2026?",
        "created_at": "2025-08-31T05:08:39.514104Z",
        "open_time": "2025-11-29T02:54:49Z",
        "cp_reveal_time": "2025-11-29T04:24:49Z",
        "spot_scoring_time": "2025-11-29T04:24:49Z",
        "scheduled_resolve_time": "2026-01-01T14:00:00Z",
        "actual_resolve_time": null,
        "resolution_set_time": null,
        "scheduled_close_time": "2025-11-29T04:24:49Z",
        "actual_close_time": "2025-11-29T04:24:49Z",
        "type": "binary",
        "options": null,
        "group_variable": "",
        "status": "open",
        "possibilities": null,
        "resolution": null,
        "include_bots_in_aggregates": true,
        "question_weight": 1.0,
        "default_score_type": "spot_peer",
        "default_aggregation_method": "unweighted",
        "label": "",
        "unit": "",
        "open_upper_bound": false,
        "open_lower_bound": false,
        "inbound_outcome_count": null,
        "scaling": {
            "range_min": null,
            "range_max": null,
            "nominal_min": null,
            "nominal_max": null,
            "zero_point": null,
            "open_upper_bound": false,
            "open_lower_bound": false,
            "inbound_outcome_count": null,
            "continuous_range": null
        },
        "group_rank": null,
        "description": "This question is synced with an identical question on Metaculus. The original question opened on 2023-01-25 13:14:00 and can be found [here](https://www.metaculus.com/questions/14502). This question will resolve to the same value as the synced question. The original question's background info at this time is below. \n\n*Related Questions on Metaculus:*\n\n* [Will a 100 trillion parameter deep learning model be trained before 2026?](https://www.metaculus.com/questions/3656/will-a-100-trillion-parameter-deep-learning-model-be-trained-before-2026/) (resolved as **Yes**)\n* [If GPT-4 is announced before 2025, how many parameters will it have (in billions of parameters)?](https://www.metaculus.com/questions/14327/gpt-4-number-of-parameters/)\n\n---\n\nParameter count is a key attribute of modern machine learning (ML) systems: it has a strong influence on model performance, and on training costs. [Deepchecks describes](https://deepchecks.com/glossary/model-parameters/) parameters as follows:\n\n>The weights and coefficients that the algorithm extracts from the data are known as model parameters. Model parameters of neural networks consider how the predictor variable influences the target variable.\n\nIn other words the model [learns these parameters during training](https://www.geeksforgeeks.org/difference-between-model-parameters-vs-hyperparameters/) to fit the input data to the appropriate output.\n\nIn recent years the number of parameters used in ML models [has increased rapidly](https://epochai.org/blog/machine-learning-model-sizes-and-the-parameter-gap). But, as discussed in [this writeup](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models) (and also [here](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)), research by DeepMind published in the spring of 2022, along with a model named [Chinchilla](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training), suggested that the importance of dataset size relative to parameter count had been underestimated in previous work.\n\n>On March 29th, DeepMind published a paper, \"Training Compute-Optimal Large Language Models\", that shows that essentially everyone -- OpenAI, DeepMind, Microsoft, etc. -- has been training large language models with a deeply suboptimal use of compute.\n>\n>Following the new scaling laws that they propose for the optimal use of compute, DeepMind trains a new, 70-billion parameter model that outperforms much larger language models, including the 175-billion parameter GPT-3 and DeepMind's own 270-billion parameter \"Gopher\".\n\nIn March of 2022, a paper describing the [BaGuaLu model](https://dl.acm.org/doi/abs/10.1145/3503221.3508417) model was published, and discussed a variant of this model trained with 174 trillion parameters. However, this was a [sparse model](https://arxiv.org/abs/2101.03961) (seemingly a variant of [mixture of experts](https://en.wikipedia.org/wiki/Mixture_of_experts)), and was primarily a demonstration of the ability to train large scale models.\n\nSparse models activate a smaller share of their parameters in a forward pass, using those that were trained for the task at hand, while dense models use a larger share of their parameters. In an ML model a [forward pass](https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html) or forward propagation is the process of input data \"travelling\" through the neural network to the output node.\n\n<iframe src=\"https://ourworldindata.org/grapher/artificial-intelligence-parameter-count\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\"></iframe>\n\n*Note that the above is for information only and is not the resolution source for this question.*\n\n`{\"format\":\"metac_closes_in_period\",\"info\":{\"post_id\":14502,\"question_id\":14502}}`",
        "resolution_criteria": "This question resolves as **Yes** if, before January 1, 2026, a credible source publishes information detailing a dense ML model with at least 100 trillion parameters. For the purposes of this question a dense model is one for which all of the following are true to the extent they can be determined from the available information:\n\n* The model uses at least 80% of its parameters during a single forward pass\n    * If this information is not available, the model must not be described by its creators as being related to a known sparse architecture (for example, [mixture of experts](https://en.wikipedia.org/wiki/Mixture_of_experts))\n* The model is not a hypothetical description or a demonstration of scaling without application. The model must actually be trained and implemented either against  benchmarks and performance assessments or used in a real-world application",
        "fine_print": "",
        "post_id": 39475,
        "aggregations": {
            "unweighted": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            }
        },
        "my_forecasts": {
            "history": [],
            "latest": null,
            "score_data": {}
        }
    },
    "news": "Here are the relevant news articles:\n\n**What Is DeepSeek-V4 MoE? Inside the 1-Trillion Parameter Open-Source LLM**\nDeepSeek-V4 MoE is a 1-trillion-parameter open-source Mixture-of-Experts (MoE) language model that represents a major breakthrough in scalable, efficient AI development. Unlike dense models, which activate all parameters per token, DeepSeek-V4 activates only about 3% of its total parameters\u2014roughly 32 billion\u2014per token by routing each token through a dynamic mixture of 16 specialized experts. This architecture enables trillion-scale models to remain computationally feasible. The model features a novel Top-16 expert routing system, finer-grained expert segmentation, and shared generalist experts to reduce redundancy and improve efficiency. These innovations allow DeepSeek-V4 to achieve GPT-5-class performance in math, coding, and long-context reasoning while maintaining strong performance on general language tasks. Early evidence shows it outperforms most open models and is competitive with leading closed systems. Its economic advantages are significant: training and inference costs are drastically reduced due to sparsity, with estimates suggesting training costs comparable to other 1T MoEs like Moonshot\u2019s Kimi K2 (~$4.6M), a figure unattainable for dense models at similar scale. DeepSeek-V4\u2019s MIT-style open licensing enables widespread adoption, democratizing access to frontier AI. It stands as a pivotal proof of concept that MoE architectures can deliver high performance, scalability, and affordability\u2014making GPT-5-level capability accessible to researchers and developers worldwide. The model\u2019s techniques, including aux-free load balancing and fine-grained routing, are expected to become standard in future large-scale MoE systems. DeepSeek-V4 is part of a growing wave of Chinese open models, including Qwen-3-Max and Kimi K2, that rival Western closed systems. The model marks a shift from 'going deep' (increasing model depth) to 'going wide' (increasing expert diversity), reshaping the economics and culture of AI development.\nOriginal language: en\nPublish date: November 28, 2025 10:52 PM\nSource:[DEV Community](https://dev.to/isabellaking/what-is-deepseek-v4-moe-inside-the-1-trillion-parameter-open-source-llm-5d27)\n\n**GPT-OSS to Gemma 3: Top 5 open-weight models you must try**\nIn late 2025, the AI landscape has shifted decisively toward open-weight models, with proprietary AI's dominance diminishing as leading organizations release frontier-class models openly. OpenAI's GPT-OSS, released in August 2025, is highlighted as a 'System 2' reasoning engine designed for complex, multi-step workflows, marking a significant step in open AI development. Google DeepMind's Gemma 3 is recognized as the efficiency champion, trained as a native multimodal model capable of fluid understanding of both text and images. Meta's Llama 4 has become the ecosystem standard due to its versatility and broad context handling. DeepSeek's R1 series is praised as the top self-hosted coding assistant, dominating the developer copilot niche. The Allen Institute for AI (Ai2) stands out as the most open-source purist with Olmo 3, releasing full access to weights, training data, code, and logs. For users with a 24GB VRAM GPU (e.g., RTX 3090/4090), Gemma 3 27B is recommended for optimal balance of speed, multimodal vision, and reasoning. Developers seeking a coding assistant should use DeepSeek-R1, while those with high-end workstations can explore the 120B parameter GPT-OSS model for maximum local AI performance.\nOriginal language: en\nPublish date: November 28, 2025 01:49 PM\nSource:[Digit](https://www.digit.in/features/general/gpt-oss-to-gemma-3-top-5-open-weight-models-you-must-try.html)\n\n**North America Automated Machine Learning Market Report 2025-2033 by Offering, Enterprise Size, Deployment Mode, Application, End Use, Countries and Company Analysis**\nThe North America Automated Machine Learning (AutoML) Market is projected to grow from US$1.02 billion in 2024 to US$13 billion by 2033, reflecting a compound annual growth rate (CAGR) of 32.66% from 2025 to 2033, according to a report by ResearchAndMarkets.com published on November 28, 2025. The market's expansion is driven by rising AI and ML adoption across healthcare, banking, retail, and IT sectors, a shortage of skilled data scientists, increased cloud platform integration, enterprise digital transformation, and advancements in machine learning algorithms. Key growth enablers include automation of model selection, hyperparameter tuning, and deployment, with platforms like Oracle MySQL HeatWave enabling in-database ML to reduce development time. Cloud-based AutoML solutions lower entry barriers for SMEs by offering scalable, secure, and cost-effective infrastructure. Technological advances in feature engineering, neural architecture search, and AI explainability further enhance AutoML efficiency. However, challenges remain, including data privacy and security concerns under regulations like HIPAA, CCPA, and GDPR, as well as integration complexity with legacy IT systems and heterogeneous data environments. Recent developments include Oracle\u2019s $40 billion investment in Nvidia GPUs for the OpenAI-backed Stargate data center in Texas (June 2025) and AWS\u2019s launch of Project Rainier, which will deploy hundreds of thousands of Trainium 2 chips to quintuple AI training capacity across the U.S. Key market players include Amazon Web Services, Google LLC, Microsoft Corporation, IBM, Oracle, DataRobot, SAS Institute, and H2O.ai. The market is segmented by offering (solution, service), enterprise size (SMEs, large enterprises), deployment mode (cloud, on-premise), application (data processing, hyperparameter optimization, model selection, etc.), end use (healthcare, banking, retail, etc.), and country (United States, Canada).\nOriginal language: en\nPublish date: November 28, 2025 09:08 AM\nSource:[GlobeNewswire Press Releases](https://www.globenewswire.com/news-release/2025/11/28/3196050/28124/en/North-America-Automated-Machine-Learning-Market-Report-2025-2033-by-Offering-Enterprise-Size-Deployment-Mode-Application-End-Use-Countries-and-Company-Analysis.html)\n\n**A novel deep neural model for efficient and scalable historical place image classification - Scientific Reports**\nA novel deep learning model, HistoNet, is proposed to address key limitations in AI-driven historical place image classification, including poor generalization, lack of interpretability ('black-box' nature), and high computational demands. HistoNet integrates Convolutional Neural Networks (CNNs), Transformers, and Mamba state-space models to enhance performance through complementary strengths: CNNs extract local features, Transformers capture long-range spatial dependencies, and Mamba enables efficient sequence modeling with linear complexity. The model improves interpretability via SHAP-based attribution maps and is lightweight, requiring only 36.12 MB of storage (reducible to 18 MB with FP16 quantization) and 7.208 GFLOPs per inference, making it deployable in resource-limited settings. Evaluated on two datasets\u2014the Architectural Heritage Elements - Image64 Dataset (10 categories, 10,235 images) and the Historical Building Malacca, Malaysia Dataset (49 images of UNESCO World Heritage structures)\u2014HistoNet demonstrates strong generalization across diverse architectural styles, imaging conditions, and geographical regions. The model uses a multi-stage preprocessing pipeline with data partitioning (60% training, 10% validation, 30% testing), class imbalance mitigation via inverse-frequency sampling, and augmentation (random resizing, flipping, rotation). Hyperparameter optimization via Bayesian techniques identified optimal settings: AdamW optimizer with learning rate 1e-3, weight decay 1e-4, batch size 24, cosine annealing with 10-epoch warmup, 8 attention heads, 512 feed-forward dimension, 128 Mamba state-space units, and progressive dropout (0.1, 0.15, 0.3). The model achieved robust performance across tasks, with ablation studies confirming the effectiveness of each component. The study concludes that HistoNet systematically addresses prior gaps in accuracy, scalability, interpretability, and accessibility, positioning it as a scalable and trustworthy tool for cultural heritage preservation.\nOriginal language: en\nPublish date: November 28, 2025 12:00 AM\nSource:[Nature](https://www.nature.com/articles/s41598-025-26897-y)\n\n**Predefined Vectors for Memory-Efficient Neural Network Training**\nA new methodology called Latent Space Configuration (LSC), proposed by Nikita Gabdullin, an expert at 'Kriptonit', enables neural networks to maintain a fixed model size regardless of the number of classes, solving the scalability issue in machine learning classification tasks. Traditional supervised learning (SL) methods require a classification layer size proportional to the number of classes, making models impractical when dealing with millions of classes\u2014such as in face recognition\u2014due to astronomical memory demands. LSC replaces this approach by projecting inputs (e.g., facial images) into a pre-defined, fixed vector system in abstract latent space. Each class is assigned a unique target vector, and the network learns to map input embeddings as closely as possible to their respective targets while keeping them distant from others. These target vectors are generated using mathematically rigorous constructions like the root system A and its variants, which are uniformly distributed and optimized for separation. This shift from 'one neuron per class' to 'fixed position per class' allows architectures like Vision Transformer (ViT) to remain unchanged even when scaling to 1.28 million classes. In experiments, ViT-S trained with LSC achieved 87.1% accuracy on a 1.28 million-class dataset, demonstrating practical feasibility. Crucially, LSC drastically reduces GPU memory usage: only a small batch of target vectors is loaded per training step, independent of total class count. For example, traditional SL would require a classification layer larger than the backbone model at 100,000 classes, and ViT-B cannot even load a single batch on an A100 40GB GPU with 10 million classes\u2014conditions where LSC remains viable. Additionally, LSC avoids accuracy degradation on known classes when adding new ones, as model parameters remain unchanged. Experiments also show that randomly shuffled vectors (A) outperform uniform distributions in some cases, achieving 87.9% accuracy on ImageNet-1K without data augmentation\u2014comparable to traditional cross-entropy methods but without scalability limitations. LSC is not limited to face recognition and applies to any high-class classification task in computer vision, representing a paradigm shift toward scalable, memory-efficient AI systems.\nOriginal language: ru\nPublish date: November 27, 2025 09:55 AM\nSource:[\u0425\u0430\u0431\u0440](https://habr.com/ru/companies/kryptonite/articles/970844/)\n\n**What Is DeepSeek-V4 MoE? Inside the 1-Trillion Parameter Open-Source LLM**\nDeepSeek-V4 MoE is a 1-trillion-parameter open-source Mixture-of-Experts (MoE) language model that represents a major breakthrough in scalable, efficient AI development. Unlike dense models, which activate all parameters per token, DeepSeek-V4 activates only about 3% of its total parameters\u2014roughly 32 billion\u2014per token by routing each token through a dynamic mixture of 16 specialized experts. This architecture enables trillion-scale models to remain computationally feasible. The model features a novel Top-16 expert routing system, finer-grained expert segmentation, and shared generalist experts to reduce redundancy and improve efficiency. These innovations allow DeepSeek-V4 to achieve GPT-5-class performance in math, coding, and long-context reasoning while maintaining strong performance on general language tasks. Early evidence shows it outperforms most open models and is competitive with leading closed systems. Its economic advantages are significant: training and inference costs are drastically reduced due to sparsity, with estimates suggesting training costs comparable to other 1T MoEs like Moonshot\u2019s Kimi K2 (~$4.6M), a figure unattainable for dense models at similar scale. DeepSeek-V4\u2019s MIT-style open licensing enables widespread adoption, democratizing access to frontier AI. It stands as a pivotal proof of concept that MoE architectures can deliver high performance, scalability, and affordability\u2014making GPT-5-level capability accessible to researchers and developers worldwide. The model\u2019s techniques, including aux-free load balancing and fine-grained routing, are expected to become standard in future large-scale MoE systems. DeepSeek-V4 is part of a growing wave of Chinese open models, including Qwen-3-Max and Kimi K2, that rival Western closed systems. The model marks a shift from 'going deep' (increasing model depth) to 'going wide' (increasing expert diversity), reshaping the economics and culture of AI development.\nOriginal language: en\nPublish date: November 28, 2025 10:52 PM\nSource:[DEV Community](https://dev.to/isabellaking/what-is-deepseek-v4-moe-inside-the-1-trillion-parameter-open-source-llm-5d27)\n\n**The Hidden Infrastructure Powering Modern AI**\nModern artificial intelligence relies on a hidden global infrastructure of GPUs, large-scale training pipelines, hyperscale data centers, and energy systems. GPUs, originally designed for gaming, are now the core of AI computation due to their ability to perform parallel mathematical operations at scale, enabling rapid training of deep learning models. Training state-of-the-art AI models requires petabytes of data, specialized hardware like A100, H100, and TPU v5, distributed frameworks such as DeepSpeed and PyTorch, and months of experimentation, costing millions of dollars and thousands of GPU-hours. AI data centers\u2014equipped with high-density GPU racks, liquid cooling, ultra-fast interconnects (InfiniBand, NVLink), and redundant power systems\u2014serve as the physical backbone for AI workloads, designed specifically for continuous, high-load operations. However, AI's massive computational demands come with a significant energy cost: training a single large model can consume as much electricity as a small town uses in a year and produce emissions equivalent to over 100 transatlantic flights. The future of AI depends on renewable-powered data centers, energy-efficient hardware, and optimized training techniques like sparsity and quantization. According to the author, a Director at Perfect Web Solutions and educator with a 30K-subscriber YouTube channel, AI innovation is not just software-driven\u2014it is fundamentally an infrastructure revolution.\nOriginal language: en\nPublish date: November 28, 2025 10:26 PM\nSource:[Medium.com](https://medium.com/@farhan.nasir01/the-hidden-infrastructure-powering-modern-ai-71263600d70b)\n\n**What to be thankful for in AI in 2025**\nThe article 'What to be thankful for in AI in 2025' reflects on the year's transformative developments in artificial intelligence, highlighting key advancements across models, ecosystems, and accessibility. OpenAI delivered GPT-5 in August and GPT-5.1 in November, featuring dynamic 'Thinking' and 'Instant' variants that adjust reasoning time per task; despite early issues with math and coding, user feedback led to improvements, and enterprise adoption shows strong results\u2014ZenDesk reports GPT-5-powered agents resolve over 50% of customer tickets, with some cases reaching 80\u201390% resolution. OpenAI also launched GPT-5.1-Codex-Max, a powerful coding model enabling agentic workflows, and ChatGPT Atlas, a browser-integrated assistant with sidebar summaries and on-page analysis. On the media front, Sora 2 evolved into a full video-and-audio model with improved physics, synchronized sound, and a social networking app for user-created TV networks. OpenAI released open-weight models gpt-oss-120B and gpt-oss-20B under an Apache 2.0-style license, marking a significant return to public model sharing. China\u2019s open-model ecosystem, led by DeepSeek and Alibaba\u2019s Qwen family, now slightly surpasses the U.S. in global open-model downloads, with models like Light-R1-32B and VibeThinker-1.5B demonstrating strong performance on limited budgets. Liquid AI introduced the Liquid Foundation Models (LFM2) and LFM2-VL for edge and robotics deployment, with the LFM2-VL-3B targeting embedded systems. Google\u2019s Gemma 3 line, ranging from 270M to 27B parameters, offers open weights and multimodal support, with Gemma 3 270M excelling in fine-tuning and structured text tasks. Google also launched Nano Banana Pro (Gemini 3 Pro Image), a high-resolution image generator ideal for infographics and technical visuals. Meta partnered with Midjourney to license its aesthetic technology, integrating it into Facebook, Instagram, and Meta AI\u2014potentially bringing Midjourney-grade visuals into mainstream social platforms. Google\u2019s Gemini 3, promoted as its most capable model yet, includes Deep Think mode and strong performance in reasoning, coding, and multimodal tasks. The article concludes that 2025 marks the year AI diversified: multiple frontiers, open ecosystems (especially in China), efficient small models, and integrated creative tools have created real choice for developers, enterprises, and creators. The author expresses gratitude not for any single model, but for the expanding landscape of options\u2014closed and open, local and cloud-based, reasoning and media-focused.\nOriginal language: en\nPublish date: November 28, 2025 04:19 PM\nSource:[VentureBeat](https://venturebeat.com/ai/what-to-be-thankful-for-in-ai-in-2025)\n\n**GPT-OSS to Gemma 3: Top 5 open-weight models you must try**\nIn late 2025, the AI landscape has shifted decisively toward open-weight models, with proprietary AI's dominance diminishing as leading organizations release frontier-class models openly. OpenAI's GPT-OSS, released in August 2025, is highlighted as a 'System 2' reasoning engine designed for complex, multi-step workflows, marking a significant step in open AI development. Google DeepMind's Gemma 3 is recognized as the efficiency champion, trained as a native multimodal model capable of fluid understanding of both text and images. Meta's Llama 4 has become the ecosystem standard due to its versatility and broad context handling. DeepSeek's R1 series is praised as the top self-hosted coding assistant, dominating the developer copilot niche. The Allen Institute for AI (Ai2) stands out as the most open-source purist with Olmo 3, releasing full access to weights, training data, code, and logs. For users with a 24GB VRAM GPU (e.g., RTX 3090/4090), Gemma 3 27B is recommended for optimal balance of speed, multimodal vision, and reasoning. Developers seeking a coding assistant should use DeepSeek-R1, while those with high-end workstations can explore the 120B parameter GPT-OSS model for maximum local AI performance.\nOriginal language: en\nPublish date: November 28, 2025 01:49 PM\nSource:[Digit](https://www.digit.in/features/general/gpt-oss-to-gemma-3-top-5-open-weight-models-you-must-try.html)\n\n**North America Automated Machine Learning Market Report 2025-2033 by Offering, Enterprise Size, Deployment Mode, Application, End Use, Countries and Company Analysis**\nThe North America Automated Machine Learning (AutoML) Market is projected to grow from US$1.02 billion in 2024 to US$13 billion by 2033, reflecting a compound annual growth rate (CAGR) of 32.66% from 2025 to 2033, according to a report by ResearchAndMarkets.com published on November 28, 2025. The market's expansion is driven by rising AI and ML adoption across healthcare, banking, retail, and IT sectors, a shortage of skilled data scientists, increased cloud platform integration, enterprise digital transformation, and advancements in machine learning algorithms. Key growth enablers include automation of model selection, hyperparameter tuning, and deployment, with platforms like Oracle MySQL HeatWave enabling in-database ML to reduce development time. Cloud-based AutoML solutions lower entry barriers for SMEs by offering scalable, secure, and cost-effective infrastructure. Technological advances in feature engineering, neural architecture search, and AI explainability further enhance AutoML efficiency. However, challenges remain, including data privacy and security concerns under regulations like HIPAA, CCPA, and GDPR, as well as integration complexity with legacy IT systems and heterogeneous data environments. Recent developments include Oracle\u2019s $40 billion investment in Nvidia GPUs for the OpenAI-backed Stargate data center in Texas (June 2025) and AWS\u2019s launch of Project Rainier, which will deploy hundreds of thousands of Trainium 2 chips to quintuple AI training capacity across the U.S. Key market players include Amazon Web Services, Google LLC, Microsoft Corporation, IBM, Oracle, DataRobot, SAS Institute, and H2O.ai. The market is segmented by offering (solution, service), enterprise size (SMEs, large enterprises), deployment mode (cloud, on-premise), application (data processing, hyperparameter optimization, model selection, etc.), end use (healthcare, banking, retail, etc.), and country (United States, Canada).\nOriginal language: en\nPublish date: November 28, 2025 09:08 AM\nSource:[GlobeNewswire Press Releases](https://www.globenewswire.com/news-release/2025/11/28/3196050/28124/en/North-America-Automated-Machine-Learning-Market-Report-2025-2033-by-Offering-Enterprise-Size-Deployment-Mode-Application-End-Use-Countries-and-Company-Analysis.html)\n\n**Energy-Efficient Deep Learning -- How Precision Scaling Reduces Carbon Footprint | DigitalOcean**\nPrecision scaling in deep learning reduces the carbon footprint by shifting models from high-precision formats like FP32 to lower-precision representations such as FP16, INT8, and INT4. Fewer bits reduce computational load, memory movement, and power consumption with minimal impact on performance. This method is increasingly adopted for both training and inference. Core concepts include FLOPs (Floating Point Operations per Second), matrix multiplications (MatMuls), tensors, and bits\u2014where fewer bits enable faster compute and lower memory use. Mixed precision training uses FP16 for computation and FP32 for stability, improving speed while maintaining accuracy. Low-precision formats (FP8, INT8, INT4) reduce compute, memory, and energy costs. Quantization stores and computes with lower-bit numbers, especially useful in inference. Key efficiency metrics include throughput (samples processed per second), latency (time per input), GPU hours (compute cost), and memory bandwidth (data movement speed). Training dynamics involve activations, gradients, optimizer states, weights, and KV cache in transformers. Workloads are classified as compute-bound or memory-bound. Additional concepts include epochs, batch size, model size, and the use of PyTorch Lightning and TensorFlow/Keras frameworks to implement mixed precision training efficiently. The article includes practical code examples demonstrating mixed precision, FLOPs, tensors, matrix multiplication, throughput, GPU hours, and quantization in both PyTorch and TensorFlow environments.\nOriginal language: en\nPublish date: November 28, 2025 12:11 AM\nSource:[DigitalOcean](https://www.digitalocean.com/community/tutorials/energy-efficient-deep-learning-precision-scaling)\n\n**A novel deep neural model for efficient and scalable historical place image classification - Scientific Reports**\nA novel deep learning model, HistoNet, is proposed to address key limitations in AI-driven historical place image classification, including poor generalization, lack of interpretability ('black-box' nature), and high computational demands. HistoNet integrates Convolutional Neural Networks (CNNs), Transformers, and Mamba state-space models to enhance performance through complementary strengths: CNNs extract local features, Transformers capture long-range spatial dependencies, and Mamba enables efficient sequence modeling with linear complexity. The model improves interpretability via SHAP-based attribution maps and is lightweight, requiring only 36.12 MB of storage (reducible to 18 MB with FP16 quantization) and 7.208 GFLOPs per inference, making it deployable in resource-limited settings. Evaluated on two datasets\u2014the Architectural Heritage Elements - Image64 Dataset (10 categories, 10,235 images) and the Historical Building Malacca, Malaysia Dataset (49 images of UNESCO World Heritage structures)\u2014HistoNet demonstrates strong generalization across diverse architectural styles, imaging conditions, and geographical regions. The model uses a multi-stage preprocessing pipeline with data partitioning (60% training, 10% validation, 30% testing), class imbalance mitigation via inverse-frequency sampling, and augmentation (random resizing, flipping, rotation). Hyperparameter optimization via Bayesian techniques identified optimal settings: AdamW optimizer with learning rate 1e-3, weight decay 1e-4, batch size 24, cosine annealing with 10-epoch warmup, 8 attention heads, 512 feed-forward dimension, 128 Mamba state-space units, and progressive dropout (0.1, 0.15, 0.3). The model achieved robust performance across tasks, with ablation studies confirming the effectiveness of each component. The study concludes that HistoNet systematically addresses prior gaps in accuracy, scalability, interpretability, and accessibility, positioning it as a scalable and trustworthy tool for cultural heritage preservation.\nOriginal language: en\nPublish date: November 28, 2025 12:00 AM\nSource:[Nature](https://www.nature.com/articles/s41598-025-26897-y)\n\n**Large language models versus classical machine learning performance in COVID-19 mortality prediction using high-dimensional tabular data - Scientific Reports**\nA study published in Scientific Reports evaluates the performance of large language models (LLMs) versus classical machine learning (CML) models in predicting COVID-19 mortality using high-dimensional tabular data from the Tehran COVID-19 cohort, which included 9,134 patients across four tertiary hospitals in Tehran, Iran, from March 2020 to May 2023. The dataset comprised 81 on-admission features, including demographics, comorbidities, vital signs, and laboratory results, with a focus on in-hospital mortality. Data preprocessing included handling missing values via iterative imputation (numerical) and KNN (categorical), normalization using a standard scaler, and feature selection using Lasso regularization, reducing the dataset to 40 top-ranked features. Class imbalance was addressed using SMOTE, increasing the training set from 6,118 to 9,760 samples. For LLMs, data were converted into clinical text narratives, excluding negative symptoms and normal lab values due to context window constraints. Five CML models (logistic regression, SVM, decision tree, KNN, random forest, MLP, XGBoost) were trained with hyperparameter optimization via grid search and cross-validation. LLMs tested included open-source models (Mistral-7b, Mixtral 8\u00d77B, Llama3-8b, Llama3-70b) via Poe, proprietary models (GPT-3.5-turbo, GPT-4, GPT-4-turbo, GPT-4o) via OpenAI API, and fine-tuned models (BERT, ClinicalBERT). Zero-shot classification was performed without training, while Mistral-7b-Instruct-v0.2 was fine-tuned using QLoRA with 4-bit quantization, mixed-precision training, and gradient accumulation on an NVIDIA A100 GPU. Model performance was assessed using accuracy, precision, recall, specificity, F1 score, and AUC across internal and external validation sets. A 5-fold cross-validation was applied to the training set (n=6,118). SHAP analysis was used to interpret model predictions, with XGBoost serving as the model-agnostic explainer to compute global and individual feature impacts. The study found that CML models outperformed LLMs in predictive accuracy, with XGBoost achieving the highest AUC. However, LLMs demonstrated robust performance in zero-shot settings, particularly GPT-4, though fine-tuning improved results. The influence of training sample size was also analyzed, showing improved performance with larger datasets. The study was conducted under IRB approval (IR.SBMU.RIGLD.REC.004 and IR.SBMU.RIGLD.REC.1399.058), with patient data pseudonymized and protected under the Helsinki Declaration and Iran Ministry of Health regulations. No informed consent was required due to IRB exemption.\nOriginal language: en\nPublish date: November 28, 2025 12:00 AM\nSource:[Nature](https://www.nature.com/articles/s41598-025-26705-7)\n\n**Is the LLM Limit Thesis Real? Meta and Google Researchers Disagree on AI's Next Frontier**\nThe debate over whether large language models (LLMs) have reached a performance ceiling\u2014known as the 'LLM limit thesis'\u2014is intensifying, with prominent AI researchers divided. Yann LeCun, Meta's former AI lead and one of the 'fathers of deep learning,' argues that LLMs are nearing their limits due to their inability to learn from the real world, advocating instead for 'world models' like the Joint-Embedding Predictive Architecture (JEPA). LeCun plans to leave Meta to found a startup focused on world models, as reported by the Financial Times on November 11, 2025. Similarly, Fei-Fei Li, Stanford University professor and 'AI godmother' known for ImageNet, co-founded World Labs in 2024 to develop a 3D world foundation model. In a June 2025 podcast, Li stated that language is a powerful but insufficient medium for encoding the physical 3D world, reinforcing the need for new architectures. This shift is driven by concerns that the 'scaling law'\u2014the principle that increased compute, model size, and data improve LLM performance\u2014has plateaued. The 2025 release of GPT-4.5, which was 30 times more expensive than GPT-4o yet underperformed, exemplifies this stagnation. Ilya Sutskever, OpenAI co-founder, confirmed in December 2024 at NeurIPS 2024 and again in November 2025 on the Dwarkesh Podcast that the 'scaling era' has ended, marking a transition to a 'research era' focused on novel methodologies.\nOriginal language: ja\nPublish date: November 27, 2025 08:00 PM\nSource:[\u65e5\u7d4c\u30af\u30ed\u30b9\u30c6\u30c3\u30af\uff08xTECH\uff09](https://xtech.nikkei.com/atcl/nxt/column/18/00692/112700176/)\n\n**Predefined Vectors for Memory-Efficient Neural Network Training**\nA new methodology called Latent Space Configuration (LSC), proposed by Nikita Gabdullin, an expert at 'Kriptonit', enables neural networks to maintain a fixed model size regardless of the number of classes, solving the scalability issue in machine learning classification tasks. Traditional supervised learning (SL) methods require a classification layer size proportional to the number of classes, making models impractical when dealing with millions of classes\u2014such as in face recognition\u2014due to astronomical memory demands. LSC replaces this approach by projecting inputs (e.g., facial images) into a pre-defined, fixed vector system in abstract latent space. Each class is assigned a unique target vector, and the network learns to map input embeddings as closely as possible to their respective targets while keeping them distant from others. These target vectors are generated using mathematically rigorous constructions like the root system A and its variants, which are uniformly distributed and optimized for separation. This shift from 'one neuron per class' to 'fixed position per class' allows architectures like Vision Transformer (ViT) to remain unchanged even when scaling to 1.28 million classes. In experiments, ViT-S trained with LSC achieved 87.1% accuracy on a 1.28 million-class dataset, demonstrating practical feasibility. Crucially, LSC drastically reduces GPU memory usage: only a small batch of target vectors is loaded per training step, independent of total class count. For example, traditional SL would require a classification layer larger than the backbone model at 100,000 classes, and ViT-B cannot even load a single batch on an A100 40GB GPU with 10 million classes\u2014conditions where LSC remains viable. Additionally, LSC avoids accuracy degradation on known classes when adding new ones, as model parameters remain unchanged. Experiments also show that randomly shuffled vectors (A) outperform uniform distributions in some cases, achieving 87.9% accuracy on ImageNet-1K without data augmentation\u2014comparable to traditional cross-entropy methods but without scalability limitations. LSC is not limited to face recognition and applies to any high-class classification task in computer vision, representing a paradigm shift toward scalable, memory-efficient AI systems.\nOriginal language: ru\nPublish date: November 27, 2025 09:55 AM\nSource:[\u0425\u0430\u0431\u0440](https://habr.com/ru/companies/kryptonite/articles/970844/)\n\n**Rapid thrombogenesis prediction in COVID-19 patients using DNN with data labeling - Scientific Reports**\nA study published in Scientific Reports evaluates deep neural network (DNN) and machine learning (ML) models for predicting rapid thrombogenesis in COVID-19 patients using data labeling and hyperparameter optimization. The research shows that XGBoost achieved the highest accuracy of 0.9852, followed by DNN with 0.93618, while Ridge, Lasso, ElasticNet, and k-NN models performed poorly. Accuracy improved with increasing cross-validation (CV) folds up to 6-fold, after which it declined. The average Mean Absolute Error (MAE) and Mean Squared Error (MSE) were low, indicating strong model performance. The study found that at least 30 training data points are required for acceptable accuracy, with diminishing returns beyond that point. Hyperparameter tuning using RayTune optimized XGBoost and DNN models: for XGBoost, optimal parameters included a learning rate (eta) of 0.2\u20130.5, reg_alpha and reg_lambda of 1.0e-3\u201310, max_depth of 3\u20139, and min_child_weight of 1\u201310. For DNN, the optimal configuration included a learning rate of 0.063, 143 epochs, four layers (81, 21, 35, 67 nodes), batch size of 8, and ReLU activation with MSE loss. After increasing the dataset size from 1,408 to 7,040 data points (five patient datasets), accuracy dropped, likely due to patient-specific variability. To address this, a data label column was added to identify individual patient datasets (1\u20135). This labeling significantly improved DNN performance, achieving a test accuracy of 98.49%. Parallel coordinate plots helped identify outliers (e.g., target values of 1.0 at index 124, and 0.6 at indices 115 and 118), but removing them did not improve accuracy. The study concludes that incorporating patient-specific data labels enhances model generalization and predictive performance.\nOriginal language: en\nPublish date: October 01, 2025 12:00 AM\nSource:[Nature](https://www.nature.com/articles/s41598-025-15541-4)\n\n**Apple Explores Self\u2011Developed Multimodal AI Large Model Manzano: Combining Understanding and Generation Capabilities**\nApple is developing a new multimodal image model called Manzano, designed to combine image understanding and image generation. The model is still unpublished and has no demo, but a preprint paper\u2014written largely by Chinese researchers, including former Meta employee Pang Ruoming\u2014shows low\u2011resolution image samples with complex prompts. Apple claims that the dual\u2011function capability has long been a technical hurdle, causing most open\u2011source models to lag behind commercial systems such as OpenAI and Google.\n\nApple compared Manzano to DeepSeek Janus Pro and found it \"not weaker than OpenAI GPT\u20114o and Google\u2019s Gemini\u202f2.5 Flash Image Generation\" (original codename Nano Banana). The company notes that open\u2011source models usually specialize in either analysis or generation, whereas commercial systems handle both. Manzano addresses this by using a hybrid image tokenizer that outputs continuous tokens for understanding (floating\u2011point values) and discrete tokens for generation (fixed categories) from the same encoder, reducing task conflict.\n\nThe architecture consists of three parts: a hybrid tokenizer, a unified language model, and a separate image decoder. Apple built three decoder variants with 9\u202fbillion, 17.5\u202fbillion, and 35.2\u202fbillion parameters, supporting resolutions from 256\u202fpx to 2048\u202fpx.\n\nTraining involved three stages, using 2.3\u202fbillion image\u2011text pairs (public and internal) and 1\u202fbillion text\u2011image pairs, totaling 1.6\u202ftrillion tokens. Some data were synthetic, sourced from DALL\u2011E\u202f3 and ShareGPT\u20114o.\n\nIn internal tests, Manzano outperformed on ScienceQA, MM\u2011MU, and MathVista, especially on text\u2011heavy tasks like chart and document analysis. The 30\u202fbillion\u2011parameter version scored over 10 points higher than the smallest model on certain tasks. When compared to specialized systems, the performance gap was less than one point for the 30\u202fbillion\u2011parameter version. In image\u2011generation benchmarks, Manzano approached the top performers, handling complex instructions, style transfer, image overlay, and depth estimation.\n\nApple views Manzano as a strong alternative to existing models, with modular design allowing independent updates and the potential to advance multimodal AI. However, the base model still trails industry leaders, and Apple plans to integrate OpenAI GPT\u20115 into iOS\u202f26\u2019s Apple Intelligence, indicating that external model reliance may persist.\n\nKey metrics:\n- Decoder sizes: 9\u202fB, 17.5\u202fB, 35.2\u202fB parameters\n- Training data: 23\u202fB image\u2011text pairs, 10\u202fB text\u2011image pairs, 1.6\u202ftrillion tokens\n- Resolution range: 256\u202fpx\u20132048\u202fpx\n- Performance: 30\u202fB model outperforms smaller models by >10 points; gap vs. specialized systems <1 point\n- Comparative claim: \"Manzano is not weaker than OpenAI GPT\u20114o and Google\u2019s Gemini\u202f2.5 Flash Image Generation\"\nOriginal language: zh\nPublish date: September 27, 2025 01:29 PM\nSource:[k.sina.com.cn](https://k.sina.com.cn/article_5953190046_162d6789e0670243hk.html?from=tech)\n\n**A Brief History Of Generative AI**\nThe blog traces the evolution of generative AI from 2017 to 2025, highlighting key papers, models, and scaling insights. It begins with the 2017 paper 'Attention Is All You Need' by Vaswani et\u202fal., which introduced the transformer architecture that replaced RNNs with self\u2011attention, enabling efficient long\u2011range context capture. The narrative then follows OpenAI\u2019s GPT series: GPT\u20111 (2018) introduced a two\u2011stage pre\u2011training/fine\u2011tuning pipeline; GPT\u20112 (2019) scaled to 1.5\u202fB parameters and demonstrated zero\u2011shot learning; GPT\u20113 (2020) reached 175\u202fB parameters and added instruction tuning via InstructGPT and RLHF. The 2020 paper on Scaling Laws formalized the relationship between model size, data, and compute, showing that larger models are more sample\u2011efficient.\n\nThe article discusses DeepMind\u2019s Chinchilla (2022), which proved that a 70\u202fB\u2011parameter model trained on more data outperforms larger models such as Gopher (280\u202fB) and Megatron (530\u202fB), recommending a 20:1 tokens\u2011to\u2011parameters ratio. Google\u2019s PaLM (540\u202fB parameters) leveraged the Pathways system for distributed training, while OpenAI\u2019s ChatGPT (built on GPT\u20113.5) became the fastest\u2011growing consumer app, marking a shift from open research to commercial deployment.\n\nMeta\u2019s LLaMA series (7\u202fB\u201365\u202fB) introduced data\u2011efficient training and architectural tweaks like rotary embeddings and Grouped Query Attention (GQA). Anthropic\u2019s Claude 2 expanded the context window to 100\u202fK tokens, enhancing long\u2011form reasoning. Mixtral, a French MoE model, activated only 8\u202fB parameters during inference, reducing memory usage. Google\u2019s Gemini added multimodal reasoning, and GPT\u20114 (2023) brought multimodality, improved reasoning, and stronger RLHF alignment, powering ChatGPT Plus and enterprise solutions.\n\nOpenAI\u2019s Sora (2024) pushed beyond text with a diffusion\u2011based text\u2011to\u2011video model capable of generating 60\u2011second videos. Meta\u2019s LLaMA\u202f3 (2024) matched GPT\u20114 performance, trained on ~4\u202ftrillion tokens. The 2025 DeepSeek\u2011r1 family emphasized step\u2011by\u2011step reasoning traces and RL\u2011based chain\u2011of\u2011thought alignment, achieving self\u2011correcting behavior. The author notes other notable models (Qwen, Kimi, Grok) and that Kimi has trained a 1\u202fT\u2011parameter model. The blog concludes with a casual sign\u2011off, reflecting its informal tone.\n\nKey metrics cited: 1.5\u202fB parameters (GPT\u20112), 175\u202fB (GPT\u20113), 70\u202fB (Chinchilla), 540\u202fB (PaLM), 100\u202fK\u2011token context (Claude\u202f2), 4\u202ftrillion tokens (LLaMA\u202f3). The narrative remains factual and explanatory, with no overt bias.\nOriginal language: en\nPublish date: September 25, 2025 01:48 PM\nSource:[Medium.com](https://medium.com/@axry/a-brief-history-of-generative-ai-e4fe02a37502)\n\n**Google unveils VaultGemma, a privacy-focused AI model to stop training data leaks**\nGoogle announced a new privacy\u2011focused language model called VaultGemma, a small language model (SLM) with one billion parameters that is described as the largest model ever trained with differential privacy (DP). According to the company\u2019s blog post on September\u202f12, VaultGemma was trained using a new set of scaling laws developed with DeepMind, allowing the model to be released with DP\u2011preserving noise added during pre\u2011training. The weights are available for free on Hugging Face and Kaggle. Google said, 'VaultGemma represents a significant step forward in the journey toward building AI that is both powerful and private by design. By developing and applying a new, robust understanding of the scaling laws for DP, we have successfully trained and released the largest open, DP\u2011trained language model to date.' The company noted that DP training introduces trade\u2011offs such as reduced training stability, larger batch sizes and higher compute costs, and that a key finding is that one should train a much smaller model with a much larger batch size than would be used without DP. In terms of performance, VaultGemma achieved scores comparable to an older, similarly sized GPT\u20112 model on benchmarks including HellaSwag, BoolQ, PIQA, SocialIQA, TriviaQA, ARC\u2011C and ARC\u2011E. To test privacy, Google prompted VaultGemma with partial text from a training document; the model did not return the exact text, but the company warned that 'if many training sequences contain information relevant to a particular fact, then in general VaultGemma will be able to provide that information.' The article also highlights that privacy experts have shown that personal data can be extracted from large language models by crafting specific prompts, citing the New\u202fYork\u202fTimes lawsuit against OpenAI where ChatGPT reproduced articles verbatim. Google stresses that more research is needed to close the utility gap between DP\u2011trained and non\u2011DP\u2011trained models.\nOriginal language: en\nPublish date: September 16, 2025 08:38 AM\nSource:[The Indian Express](https://indianexpress.com/article/technology/artificial-intelligence/google-unveils-vaultgemma-privacy-focused-ai-model-training-data-10253028/)\n\n**ADSM: Boundaries of Model Capabilities**\nThe author explains their user\u2011level understanding of large language models (LLMs), outlining the core components: a weight matrix (trained parameters), a tokenizer, a decoder, and transformer layers that process input tokens and predict new ones step by step. They tested GPT, Gemini, Deepseek, and Grok and found that, for the same prompt, the models give very similar or even identical answers, which is expected because all modern LLMs share the transformer architecture. The article notes that LLMs are fundamentally text\u2011centric; when the author tried to attach an image to a Deepseek prompt, the model replied with the text \"\u043d\u0435 \u043c\u043e\u0433\u0443 \u0438\u0437\u0432\u043b\u0435\u0447\u044c \u0442\u0435\u043a\u0441\u0442 \u0438\u0437 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\". The author introduces ADSM (Agent\u2011Driven Software Management), which relies on text input and output.\n\nThe computational power of a neural network is determined by the number of neurons and the connections between them. One neuron can connect to thousands of others. Width refers to the dimensionality of the token\u2011embedding vector; depth is the number of transformer layers; connections are the weights linking neurons across layers. The author explains that doubling a layer roughly quadruples the number of weights, causing a rapid increase in computational cost. Each token requires a full pass through all layers, width, and connections, making generation resource\u2011intensive; optimisations such as sparse attention and selective \"experts\" reduce the number of active parameters at the cost of some accuracy.\n\nEvery model has a context window that can range from hundreds of thousands to a million tokens. The model\u2019s output is part of the same window as the input. The author discusses two strategies for result generation: one where the output occupies a larger share of the window, and another where it occupies a smaller share, the latter yielding more reproducible results. ADSM uses the second strategy, narrowing context to minimise the influence of randomness (\u0413\u0421\u0427). The author stresses that if a model\u2019s answer cannot be accepted, the prompt should be revised and a new dialogue started. The article concludes that human\u2011model interaction should follow a one\u2011question\u2011one\u2011answer principle to maintain focus.\n\nKey metrics and quotes: \"\u043d\u0435 \u043c\u043e\u0433\u0443 \u0438\u0437\u0432\u043b\u0435\u0447\u044c \u0442\u0435\u043a\u0441\u0442 \u0438\u0437 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\" (Deepseek response to an image), \"\u0417\u043d\u0430\u043d\u0438\u044f\" (distribution of knowledge in weights), \"\u0413\u0421\u0427\" (randomness generator), context window sizes from \"\u0441\u043e\u0442\u043d\u0438 \u0442\u044b\u0441\u044f\u0447 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0434\u043e \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u0430\", and the relationship that doubling a layer roughly quadruples the number of weights.\n\nOriginal language: ru\nPublish date: September 10, 2025 06:09 PM\nSource:[\u0425\u0430\u0431\u0440](https://habr.com/ru/articles/945816/)\n\n**One Possible Future for AI in Science**\nThe article discusses how the scientific community is increasingly adopting AI, driven by the rapid rise of generative models such as ChatGPT and GPT\u20114. It notes that McKinsey estimates AI could generate $4\u202ftrillion annually, prompting enterprises to invest heavily. Scientists, through the Trillion Parameter Consortium (TPC) led by Argonne National Laboratory, aimed to develop large\u2011scale AI models for science, curate data, optimize exascale libraries, and create evaluation platforms. Early optimism was based on research that scaling language models improves performance on downstream tasks, as reported in the August\u202f2022 paper *Emergent Abilities of Large Language Models* by researchers from Google Brain, DeepMind, Stanford, and UNC. OpenAI\u2019s GPT\u20114, with an estimated 1.8\u202ftrillion parameters\u2014roughly ten times GPT\u20113\u2019s 175\u202fbillion\u2014illustrated the speed of scaling. However, the article highlights the \u201cscaling wall\u201d: limited training data, GPU bottlenecks, prohibitive costs, and hallucinations that prevent straightforward gains from larger models. Consequently, the near\u2011term prospect of an AGI capable of solving major scientific problems is now seen as unlikely. Instead, the focus has shifted to reasoning models that can automate routine scientific tasks. At TPC25, leaders outlined five initiatives to guide AI\u2019s role in science, emphasizing that while AI is not a silver bullet, it can accelerate existing workflows and potentially enable breakthroughs powered by human ingenuity.\n\nKey metrics and evidence:\n- AI\u2019s projected annual value: $4\u202ftrillion (McKinsey)\n- GPT\u20114 parameter count: ~1.8\u202ftrillion (OpenAI estimate)\n- GPT\u20113 parameter count: ~175\u202fbillion (OpenAI estimate)\n- Scaling wall factors: finite training data, GPU supply limits, high cost of 20\u2011trillion\u2011parameter models\n- TPC goals: AI models for science, data curation, exascale optimization, evaluation platforms\n- Quote from research: 'Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks,' (Google Brain et al., 2022)\n- Quote from article: 'The idea that we were within a few years of building a super\u2011genius AI system capable of solving the world's toughest scientific problems appears to be over.'\nOriginal language: en\nPublish date: September 08, 2025 06:45 PM\nSource:[HPCwire](https://www.hpcwire.com/2025/09/08/one-possible-future-for-ai-in-science/)\n\n**Open-Source LLMs: Divergent Paths to Similar Peaks**\nOpenAI\u2019s new open\u2011weights model, GPT\u2011OSS, was released as a 120\u202fbillion\u2011parameter and a 20\u202fbillion\u2011parameter autoregressive Mixture\u2011of\u2011Experts transformer.  Each token activates the top four experts, so only a fraction of the total parameters are used at any time, which allows efficient inference while keeping scale.  GPT\u2011OSS incorporates Grouped Query Attention, SwiGLU activations, Rotary Positional Embeddings, and RMSNorm, and it offers a 131,072\u2011token context window thanks to YaRN scaling applied during pre\u2011training.  The model was trained on a large STEM, coding and general\u2011knowledge dataset, with safety filters informed by GPT\u20114o, and was released in a quantized MXFP4 format for consumer\u2011grade hardware.\n\nAlibaba Cloud\u2019s Qwen3 family spans 0.6\u202fbillion to 235\u202fbillion parameters, with both dense and MoE variants.  Its dense models share many architectural features with GPT\u2011OSS, such as GQA, SwiGLU, RoPE and pre\u2011normalization, but introduce QK\u2011Norm, a dynamic rescaling of query and key vectors.  Qwen3\u2019s pre\u2011training uses a 36\u2011trillion\u2011token dataset\u2014twice the size of Qwen2.5\u2014followed by a three\u2011stage regimen that ends with a long\u2011context stage where RoPE\u2019s base frequency is adjusted via ABF and YaRN is applied at inference to reach a 128,000\u2011token context.\n\nDeepSeek V3 is a 671\u202fbillion\u2011parameter MoE model, of which 37\u202fbillion parameters are active per token.  It emphasizes efficiency and capability, featuring a hybrid thinking mode and a Multi\u2011head Latent Attention (MLA) mechanism that compresses keys and values into a latent space for caching, then decompresses them during inference.  The V3.1 update adds a smarter tool\u2011calling feature.\n\nVisiting Partner Ankit Gupta notes that, despite these divergent architectures, the models converge on similar performance.  He says, \u2018This is quite surprising. You\u2019d expect that very different training methods would lead to very different results.\u2019  All three models employ YaRN for context extension\u2014GPT\u2011OSS from pre\u2011training, DeepSeek through fine\u2011tuning stages, and Qwen at inference\u2014illustrating how empirical experimentation drives progress.\n\nGupta also points out that the true competitive advantage lies in the opaque, labor\u2011intensive data engineering and post\u2011training pipelines.  He concludes, \u2018It\u2019s very difficult to replicate what they\u2019re releasing,\u2019 highlighting the barrier to replication despite the open\u2011source nature of the weights.\n\nKey metrics: GPT\u2011OSS 120\u202fB/20\u202fB parameters, 131,072\u2011token context; Qwen3 0.6\u202fB\u2013235\u202fB parameters, 36\u202ftrillion\u2011token pre\u2011training, 128,000\u2011token context; DeepSeek V3 671\u202fB total, 37\u202fB active per token.\nOriginal language: en\nPublish date: August 31, 2025 08:56 AM\nSource:[StartupHub.ai](https://www.startuphub.ai/ai-news/ai-video/2025/open-source-llms-divergent-paths-to-similar-peaks/)\n\n**Zhang Qingrui\u2019s View: Pandora\u2019s Box Opens? When AI Meets Quantum Technology \u2013 Commentary**\nZhang Qingrui\u2019s commentary traces the evolution of artificial intelligence (AI) from Turing\u2019s 1950 question to the present, highlighting key milestones such as IBM Watson\u2019s 1997 chess win, AlphaGo\u2019s 2016 Go victory, and AlphaFold\u2019s protein\u2011folding breakthrough.  In 2023, ChatGPT\u2019s large\u2011language\u2011model (LLM) achieved over one hundred million users, and Elon Musk predicted that by 2026 a GenAI or AGI\u2011like system could produce literature surpassing Harry Potter and invent new technologies.  Mathematician Tsao Zhe\u2011xuan suggested using GenAI outputs as prompts to spark new mathematical ideas, echoing Socratic critical thinking.\n\nKevin Kelly, in *The World 5,000 Days Later*, forecasts that more than 80\u202f% of future influential intelligences will be AI, and that humans will coexist with AI\u2011assisted brain intelligence (AIaBI), ushering a post\u2011Anthropocene era.\n\nThe article explains that training LLMs requires massive data and compute: ChatGPT\u2019s training set is ~45\u202fTB, with >1.75\u202ftrillion parameters, demanding at least 10\u202f000 Nvidia A100 GPUs; GPT\u20114 used ~25\u202f000 A100 GPUs, and GPT\u20115 will need ~50\u202f000 H100 GPUs.  A single H100 costs $25\u202f000\u2013$30\u202f000, and its monthly energy use is ~500\u202fkWh\u2014roughly the average Taiwan household\u2019s off\u2011peak consumption.  Nvidia\u2019s 2020 A100 peaked at 400\u202fW, while an H100 can consume ~500\u202fkWh/month.  The new Blackwell B200 GPU contains >20.8\u202fbillion transistors, doubling the count of its predecessor, and requires liquid cooling.\n\nData centers currently consume 1.5\u20132\u202f% of global electricity; projections estimate this will rise to 4\u202f% by 2030, threatening the UN Sustainable Development Goals and the 2050 net\u2011zero target.\n\nZhang uses vivid imagery\u2014e.g., a poem about a \u201cyoung man who will become a mountain\u2011captive\u201d if he neglects modern AI data centers\u2014to warn that ignoring the energy and environmental costs of AI will lead to future regret.\n\nKey metrics quoted: 45\u202fTB training data, 1.75\u202ftrillion parameters, 25\u202f000 A100 GPUs for GPT\u20114, 50\u202f000 H100 GPUs for GPT\u20115, 400\u202fW A100, 500\u202fkWh/month H100, 1.5\u20132\u202f% global electricity use by data centers, projected 4\u202f% by 2030.\nOriginal language: zh\nPublish date: August 30, 2025 10:38 PM\nSource:[\u98a8\u50b3\u5a92](https://www.storm.mg/article/11055477)\n\n**Scaling Laws in AI: Why Bigger Models Aren't Always Smarter**\nThe article explains that the prevailing belief in AI that larger models always perform better was challenged by the discovery of scaling laws, which describe how model performance improves predictably with increases in parameters, data, or compute. It cites a 2020 OpenAI study by Kaplan and colleagues that identified a rule of thumb: each parameter should be trained on about 6 tokens of data. This guideline was widely adopted, leading to models such as GPT\u20113 with 175\u202fbillion parameters trained on roughly 300\u202fbillion tokens. In 2022, DeepMind\u2019s Chinchilla experiment, which evaluated 400 models, found that the optimal ratio was closer to 20 tokens per parameter, not 6. This shift meant that a 70\u202fbillion\u2011parameter Chinchilla model, trained on a larger data set, outperformed a 280\u202fbillion\u2011parameter Gopher model under the same compute budget. The article uses the analogy of students needing 20 pages of reading material instead of 6 to illustrate the importance of data quantity. It concludes that scaling laws serve as a compass for AI research, emphasizing that efficiency and data quality matter more than sheer size, and that recent models such as Meta\u2019s LLaMA and Mistral follow this balanced approach. Evidence: the article quotes Kaplan\u2019s rule of 6 tokens per parameter, DeepMind\u2019s 20\u2011token finding, the 400\u2011model experiment, and the comparison between Chinchilla (70\u202fbillion parameters) and Gopher (280\u202fbillion parameters).\nOriginal language: en\nPublish date: August 28, 2025 08:47 AM\nSource:[Medium.com](https://medium.com/@jain.sm/scaling-laws-in-ai-why-bigger-models-arent-always-smarter-bcfeac4ba8de)\n\n",
    "date": "2025-11-29T04:16:56.650725",
    "summary": "All expert forecasters\u2014spanning AI infrastructure, ML operations, semiconductor hardware, deep learning theory, algorithmic efficiency, high-performance computing, distributed systems, and economics\u2014independently assess the probability of a dense machine learning model with at least 100 trillion parameters being trained before 2026 as extremely low, almost universally between 1% and 2%.\n\nThey converge on key reasons:\n- The largest publicly reported dense models remain in the 1\u20132 trillion parameter range; all models claiming trillion-plus parameters are sparse (Mixture-of-Experts or similar), not dense as required by the question.\n- Since the 2022 Chinchilla scaling law research, leading labs prioritize scaling through more data, algorithmic advances, and sparse architectures\u2014deeming extreme dense scaling economically, energetically, and technically irrational.\n- Training a 100T dense model would demand unprecedented hardware resources, energy (comparable to small cities), and cost (potentially billions of dollars), which are not supported by current or projected infrastructure deployments.\n- Recent and ongoing hyperscale investments (e.g., AWS Rainier, Oracle Stargate, Nvidia Blackwell) significantly boost overall compute, but are being leveraged for efficiency improvements and massive parallel workloads, not for a single 100T dense run.\n- Forecasts strongly emphasize the lack of any credible leaks, rumors, research preprints, or technical news hinting at the existence or imminent release of such a model. The field\u2019s competitive and high-profile nature makes secret completion or benchmarking of such a project unlikely to escape notice.\n- Even hypothetical scenarios\u2014such as a last-minute national prestige project\u2014are deemed implausible given organizational, logistics, and disclosure realities. Resolution criteria also require credible public evidence, not just unsubstantiated reports.\n\nIn summary, the consensus across domains is that there is no technical, economic, or strategic motive\u2014nor an observable push\u2014toward dense 100T parameter model training before 2026. All evidence and expertise indicate the probability is very close to zero but not strictly impossible, reflecting allowance for radical surprises; thus, nearly all experts assign a final probability of 1% (a few slightly higher at 2%).",
    "forecasters": [
        "AI Infrastructure Engineering (Cluster Resource Management)",
        "AI Infrastructure Engineering (Data Center Optimization)",
        "Machine Learning Operations (ML Model Lifecycle Management)",
        "Semiconductor Industry (GPU Market Analysis)",
        "Computer Science (Deep Learning Scaling Laws)",
        "Computer Science (Algorithmic Efficiency Theory)",
        "Engineering (High Performance Computing)",
        "Engineering (Distributed Systems Architecture)",
        "Economics (Technology Adoption Lifecycle)",
        "Economics (Cost Benefit Analysis)"
    ]
}