{
    "deliberation_results": {
        "Machine Learning Engineering (Large Language Models Development)": {
            "time_to_resolution": "Approximately 3 months and 10 days until January 1, 2026",
            "perspective_relevance": "As a machine learning engineer specializing in LLM development and automated reasoning, I have direct insight into current model architectures, training methodologies (e.g., RLHF, self-consistency, system-2 approaches), benchmark limitations, and SOTA progress on new and difficult mathematical reasoning tasks. I can parse signals from hype, benchmark-specific advances, and understand the practical difficulty of breaking through on benchmarks like FrontierMath, which are designed to minimize solution leakage and reward genuine advances in symbolic/numerical reasoning.",
            "status_quo": "As of early/mid-2025, no AI system has been publicly documented to achieve >85% on the FrontierMath benchmark. Model performance on closely related math benchmarks (AIME, IMO, MathBench) has made major leaps, but FrontierMath remains unsolved at that level.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Breakthroughs in Math/Reasoning Benchmarks",
                    "effect": "Strongly increases probability. GPT-5, DeepSeek-R1, and Gemini 2.5 have shattered prior records on AIME (86.7%+ with self-consistency), IMO (gold-medal level, solving 5/6 problems), ICPC, and other math/coding tasks. The rate of progress has exceeded base rates seen in 2022-2024, indicating that SOTA LLMs are within reach of human-expert or superhuman mathematical performance on some previously unsolved problems."
                },
                {
                    "factor": "FrontierMath Benchmark Design and Leakage Guards",
                    "effect": "Slightly decreases probability. As with ARC-AGI and EnigmaEval, FrontierMath is specifically constructed to resist training data leakage and superficial pattern matching by picking novel/held-out or procedurally generated problems. This makes it inherently harder and less likely that models can brute-force their way via scale or retrieval-based methods."
                },
                {
                    "factor": "Documentation and Publication Lag",
                    "effect": "Modestly decreases probability. The resolution criteria require documented and published evidence (e.g., peer-reviewed paper, official blog post), not just internal claims or unpublished leaderboard results\u2014potentially leaving a lag between actual achievement and public documentation. Labs may delay publication for strategic reasons or to synchronize releases with major conferences."
                },
                {
                    "factor": "Generalization from Other Benchmarks to FrontierMath",
                    "effect": "Mildly increases probability. The dramatic advances on AIME, IMO, and ICPC suggest cross-task transfer and significant improvements in reasoning abilities, but differences in benchmark format or problem diversity may cap achievable performance unless specifically targeted."
                },
                {
                    "factor": "Competitive Pressure and Multiple Actors",
                    "effect": "Moderately increases probability. OpenAI, Google, DeepMind, DeepSeek, and possibly Anthropic are in an intensifying race to demonstrate general reasoning progress, publishing rapidly and outcompeting each other. Novel training paradigms such as deep RL for reasoning (DeepSeek-R1, R-Zero, ProRL) ease the creation of robust, competitive models even with lower compute budgets."
                },
                {
                    "factor": "Compute Resource Accessibility",
                    "effect": "Modestly increases probability. Recent publications demonstrate that state-of-the-art reasoning models can be trained for under $300,000 (DeepSeek-R1), suggesting that more actors beyond just the Western AI giants could contribute results, raising chances of at least one model breaking the 85% threshold."
                },
                {
                    "factor": "Benchmark Overfitting and Goodhart's Law",
                    "effect": "Slightly decreases probability. There is growing concern and evidence (e.g., The Conversation, mint) that benchmarks can be gamed or overfit, but this may be less true for genuinely novel/secret benchmarks like FrontierMath. If a lab tries to specifically optimize for FrontierMath rather than general reasoning, it could increase results, but in that case the win may be more shallow."
                }
            ],
            "no_scenario": "No model achieves >85% by Jan 1, 2026, OR a model does but the result is not published in a form that meets the resolution criteria (e.g., blogs/staged releases are delayed, or research teams hold back for ICML/NeurIPS 2026). Alternatively, SOTA models achieve superhuman results on AIME, IMO, and other math benchmarks, but are unable to breach the final gap on FrontierMath due to its higher difficulty or specific design against solution leakage.",
            "yes_scenario": "OpenAI, DeepMind, or DeepSeek (or even another competitor) specifically targets FrontierMath and, leveraging their recent major advances in reasoning (e.g., DeepSeek-RL's 86.7% on AIME, OpenAI's gold-level IMO and perfect ICPC), applies their latest RL/CoT/Self-Consistency/Verifier methods to FrontierMath, releasing a well-documented result (e.g., a detailed benchmark blog, preprint, or conference talk by Dec 31, 2025) demonstrating >85% accuracy, meeting resolution requirements.",
            "final_reasoning": "The pace of progress in mathematical and algorithmic reasoning benchmarks has been unprecedented in 2024\u20132025. SOTA models like GPT-5 and DeepSeek-R1 have reached or slightly exceeded the 85% threshold on AIME\u2014a difficult math contest previously used as a proxy for mathematical understanding well below IMO or FrontierMath difficulty. They have demonstrated gold-level or better scores on IMO with publication and external validation, and similar reasoning strategies have now clearly transferred to domains as hard as the ICPC, dominating human world champions. Historical base rates would suggest caution, as math reasoning plateaus have previously persisted, and new, hard benchmarks (FrontierMath, ARC-AGI-2, EnigmaEval) are purposely designed for durability. However, the clustering of several key factors\u2014major recent jumps (from 60-70% to nearly 87% on AIME, from 0/6 to 5/6 on IMO), the generalization of reinforcement learning and self-supervised curriculum learning techniques (DeepSeek, ProRL, R-Zero), the number and diversity of well-resourced actors, and the rapidly declining marginal compute cost to train competitive models\u2014collectively shift the base rate upward. The main remaining risk is a gap between technical achievement and resolved, published documentation, or that the final points on FrontierMath prove much harder to close than AIME or IMO due to benchmark construction. Nonetheless, given the current multi-benchmark superhuman math performance and competitive incentives, it is very likely at least one lab will want to claim this milestone by the end of 2025, and will do so in a documented/public way. I judge the probability as high but not an absolute certainty due to potential unknowns (last-mile benchmark difficulty, possible publication embargoes).",
            "final_probability": 85
        },
        "Machine Learning Engineering (Benchmark Evaluation Standards)": {
            "time_to_resolution": "Approx. 3 months and 10 days until January 1, 2026",
            "perspective_relevance": "As an expert in Machine Learning Engineering and Benchmark Evaluation Standards, I am equipped to dissect not only the raw performance claims on public leaderboards, but also the nuances of benchmark construction, overfitting risk, benchmark \u2018gaming\u2019, and the technical requirements for credible public documentation and reproducibility. My perspective is especially relevant given the question\u2019s dependence on official, well-documented results on a cutting-edge, adversarial benchmark designed to thwart overfitting and solution leakage.",
            "status_quo": "No currently documented AI system has officially surpassed 85% on the FrontierMath benchmark as of this forecast date. AI progress on adjacent benchmarks (AIME, IMO, SWE-bench) is at or just below human expert/medalist level; FrontierMath is specifically designed to be harder and more resistant to train-test leakage than previous math benchmarks.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent rapid progress on adjacent math and reasoning benchmarks (AIME, IMO, ICPC, SWE-bench)",
                    "effect": "Increases probability \u2014 Both OpenAI and DeepSeek have surpassed human-level performance on elite math contests (up to 94.6% on 2025 AIME, gold medal-level at IMO, 86.7% with self-consistency), and achieved perfect ICPC scores in adversarial programming settings. The generalization and chain-of-thought capabilities required for these feats suggest plausible, if not imminent, breakthroughs on FrontierMath."
                },
                {
                    "factor": "FrontierMath\u2019s design as an \u2018adversarial\u2019 and novel benchmark",
                    "effect": "Decreases probability \u2014 FrontierMath was constructed to address the \u2018benchmark-leakage\u2019 problem, drawing on fresh, hard, unpublished problems intended to defeat direct memorization, data contamination, and simple pattern-matching. This raises the difficulty bar well above most prior published benchmarks, likely requiring novel research advances."
                },
                {
                    "factor": "Benchmark \u2018saturation\u2019 and Goodhart\u2019s Law effects in the industry",
                    "effect": "Decreases probability (mildly) \u2014 Historical evidence (e.g., ImageNet, MMLU) shows benchmarks become less robust once targeted, with eventual scores often inflated by indirect leakage or optimization. But since the question requires documented, publically published scores per protocol, and FrontierMath is extremely new and closely held, these effects are currently blunted."
                },
                {
                    "factor": "Acceleration in reasoning-method innovations (self-consistency, System-2 chains, RL/self-improvement)",
                    "effect": "Increases probability \u2014 Technical reports from DeepSeek (R1), Tencent (R-Zero), Nvidia (ProRL), and OpenAI (GPT-5) detail reinforcement learning, self-reflection, and \u2018Verifier/Challenger\u2019 frameworks that have translated to dramatic jumps (10-20%+) on tough, previously stagnant reasoning tasks. These allow models to surpass previous ceilings."
                },
                {
                    "factor": "Resolution requirements (public documentation, not just internal claims)",
                    "effect": "Decreases probability \u2014 A result is only counted if it is publicly reported in a manner that meets criteria (paper, official leaderboard, verifiable blog). Some major advances might remain proprietary or delayed for commercial, safety, or academic reasons\u2014especially as labs hedge against premature disclosure or regulatory concerns."
                },
                {
                    "factor": "Possible Resource Bubbles and Market Incentives",
                    "effect": "Increases probability (modestly) \u2014 The low cost and accessibility of state-of-the-art compute (DeepSeek R1 for $294k) enables wider participation. Competitive pressure to demonstrate leadership and win prestigious 'firsts' makes leading labs likely to prioritize public victories on new benchmarks."
                },
                {
                    "factor": "Uncertainty from lack of prior public results on the benchmark",
                    "effect": "Decreases probability (mildly) \u2014 We have almost no public baseline or progress trajectory data for current state-of-the-art models versus FrontierMath, making projections more speculative than e.g., tracking MMLU/ARC progress."
                }
            ],
            "no_scenario": "Despite impressive progress on adjacent benchmarks, no lab manages to get >85% on FrontierMath by Dec 31, 2025. Models like GPT-5, Gemini 2.5, and DeepSeek R1 either fall short on the harder unseen problems, or achieve the result internally but withhold publication for strategic, regulatory, or scientific-prudence reasons. Alternatively, a slight shortfall in accuracy (e.g., 75-84%) is documented but not the threshold.",
            "yes_scenario": "OpenAI, DeepMind, or DeepSeek release a technical report, peer-reviewed paper, or official blog post by the end of 2025 documenting a >85% score (possibly with self-consistency or system-level techniques) on FrontierMath, with credible methods and verification. The achievement is likely celebrated as a major milestone analogous to the recent perfect ICPC or IMO gold medal achievements, and may catalyze further AI benchmarks/innovations.",
            "final_reasoning": "The two most salient considerations are (1) the demonstrated, rapid closing of the gap between humans and large models on previously unthinkable math and programming contests (AIME: 94.6%, IMO: gold medal, ICPC: perfect score for GPT-5), and (2) the exceptional adversarial quality and recency of the FrontierMath benchmark. While concrete results on FrontierMath are not yet public, the technical literature (DeepSeek R1, GPT-5, ProRL) and news suggest that the core ingredients for >85% are very likely already possessed by leading labs, and public incentive to claim the 'first' is high. However, because FrontierMath deliberately avoids overlap and gaming, there remains significant risk that solution approaches effective elsewhere fail to transfer at the same rate\u2014or that last-mile publication delays (e.g., scrutiny, embargoes) push official release just past the cutoff. Taking these factors together, and grounding in base rates from the last two years\u2019 benchmark progress (noting that e.g., MMLU was saturated in <1 year once accessible, but ARC-AGI remains unsolved), a probability of 83% strikes a balance between the revolutionary adjacent results and the remaining risk from benchmark novelty, anti-leakage efforts, and publication lag. If a result does appear, it will almost certainly be documented by one of the leading labs before year-end.",
            "final_probability": 83
        },
        "AI Policy and Forecasting (Technology Forecasting Methodologies)": {
            "time_to_resolution": "About 3 months and 10 days (until January 1, 2026)",
            "perspective_relevance": "As an expert in AI Policy and Forecasting, especially in technology forecasting methodologies, I bring a methodological rigor that focuses on base rates, framing effects, and progress trajectories in AI evaluation. My perspective leverages historical benchmarking progress, the reliability of reported achievements, the limitations of current evaluation practices, and potential incentives/obstacles to resolving this question in the public domain. Specifically, I pay attention to benchmark overfitting, the difference between proxy tests and the specific benchmark in question (FrontierMath), and incentives/disincentives for disclosure.",
            "status_quo": "As of mid-2025, no AI system has been publicly documented to achieve >85% on the FrontierMath benchmark. Progress is advancing rapidly on math and reasoning benchmarks, but FrontierMath itself remains unsolved at this level.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Advances in Mathematical Reasoning Benchmarks",
                    "effect": "Substantially increases probability. There is clear evidence that state-of-the-art models (OpenAI GPT-5, DeepSeek-R1, DeepMind Gemini 2.5) are achieving superhuman results on related math and reasoning benchmarks, including perfect scores in contests like the ICPC, gold-medal level on the IMO, and near or above 85% on AIME. This signals relevant and transferable capability."
                },
                {
                    "factor": "Benchmark Difficulty and Generalization Gap",
                    "effect": "Moderately decreases probability. Despite improvement on Olympiad problems and AIME, FrontierMath is designed to be challenging even for state-of-the-art models, crafted to avoid leakage and Goodharting. There is historical precedent for models scoring highly on familiar benchmarks but falling short on more adversarial or carefully curated sets like ARC-AGI or newly released puzzles."
                },
                {
                    "factor": "Public Reporting and Disclosure Incentives",
                    "effect": "Moderately increases probability. Given the high prestige and signaling value for labs (OpenAI, Google DeepMind, DeepSeek), there is a strong incentive to announce and document achievements on public benchmarks\u2014especially as rival labs race to demonstrate progress. The resolution criteria are liberal enough to include blog posts or preprints, accommodating a range of disclosure forms."
                },
                {
                    "factor": "Speed and Trajectory of Progress",
                    "effect": "Substantially increases probability. The last 6-12 months have seen unprecedented progress in reasoning, likely due to scaling, better RL, and specialist architectures. Models that surpass 85% on AIME and gold on IMO are often one or two iterations away from achieving comparable results on domain-adapted benchmarks, particularly as techniques like extended chain-of-thought, self-verification, and system-2 reasoning are refined."
                },
                {
                    "factor": "Timing, Evaluation, and Potential Delays",
                    "effect": "Slightly decreases probability. With roughly 3 months remaining, there is some risk that even if the capability internally exists, publication delays, verification requirements, or strategic withholding of results (due to regulation, competitive secrecy, or safety concerns) could result in the capability not being documented before the deadline."
                },
                {
                    "factor": "Saturation and Goodhart's Law in Benchmarking",
                    "effect": "Slightly decreases probability. With evidence that benchmarks become less informative as they are targeted (e.g., saturation on GSM8K, MMLU), there may be a risk that some labs deprioritize explicit FrontierMath publication if it's perceived as less meaningful, or that the design of the benchmark has robustly resisted recent gains."
                }
            ],
            "no_scenario": "Despite rapid progress, models continue to top out at ~70-80% on FrontierMath due to its adversarial construction. Labs focus on flagship achievements (ICPC, IMO, SWE-bench, AIME) but judge the remaining gap to >85% on FrontierMath as nontrivial. Additionally, internal breakthroughs are not published before January 1, 2026, or are withheld due to competitive or regulatory concerns. The benchmark retains its status as uncracked, and no qualifying documentation appears.",
            "yes_scenario": "OpenAI, DeepMind, or DeepSeek (or another major lab) releases a technical blog post, preprint, or conference proceeding announcing that GPT-5, Gemini 2.5, or DeepSeek-R1+ achieves >85% on FrontierMath, supported by credible evaluation and reproducible results. Continuous improvement in RL-based training and system-2 reasoning unlocks a rapid leap above the 85% threshold. The achievement is formally documented before the resolution date and qualifies under the criteria.",
            "final_reasoning": "The base rate for such thresholds being crossed on major benchmarks in the last year (e.g., advances in AIME, IMO, SWE-bench, ICPC) is high, suggesting transferability of recent AI reasoning breakthroughs. Multiple models have reached or exceeded >85% on AIME and performed at gold/near-perfect level on the most elite mathematical competitions. As these labs are highly incentivized to publish clear wins on frontier benchmarks, and as FrontierMath\u2019s prestige and media attention are high, I expect that at least one team will produce a transparent, qualifying result before 2026. Risks remain (time, disclosure constraints, and adversarial nature of the benchmark), but given the confluence of technical capability, trajectory, and incentives as of late September 2025, the probability is strongly weighted toward a positive resolution. I estimate an 85% chance that the capability exists and is documented per the rules before the deadline.",
            "final_probability": 85
        },
        "Artificial Intelligence (Deep Learning Theory)": {
            "time_to_resolution": "Approximately 3 months and 10 days (until January 1, 2026)",
            "perspective_relevance": "As an expert in Deep Learning Theory, I have a technical understanding of the capabilities, scaling laws, and limitations of large language models and reinforcement learning-driven AI systems in mathematical reasoning. My experience includes evaluating benchmarks, understanding architecture and training regimes, and tracking how recent innovations map to hard mathematical benchmarks like FrontierMath. This allows for calibrated skepticism or optimism based on how prior progress translates\u2014or fails to\u2014to especially adversarial, unseen benchmarks.",
            "status_quo": "As of early 2025, no AI system had surpassed 85% on the FrontierMath benchmark. Major systems like GPT-4o exhibited strong gains on conventional math benchmarks (AIME, MATH, IMO), but performance on truly frontier, adversarial benchmarks lagged. Typical SOTA on FrontierMath was notably lower and remained unpublicized, signaling high difficulty.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent rapid progress in mathematical reasoning benchmarks",
                    "effect": "Increases probability. Recent models (DeepSeek-R1, GPT-5, Gemini 2.5) now consistently outperform top high schoolers and undergraduates on AIME, IMO, and ICPC, reaching 86.7%+ on AIME and 'gold medal-level' on IMO. This acceleration suggests strong base rates for breakthrough, especially as methods shift to self-improving, RL-based, and self-consistency frameworks."
                },
                {
                    "factor": "Distinctiveness and adversarial design of the FrontierMath benchmark",
                    "effect": "Decreases probability. FrontierMath is engineered as a moving target, immune to data leakage and sabotaging Goodhart\u2019s law (performance inflation via overfitting). Even state-of-the-art models with 95%+ on aged math benchmarks may plateau below 85% here, due to out-of-distribution (OOD) challenge and problem novelty."
                },
                {
                    "factor": "Emergence of new training paradigms (RL, curriculum learning, 'reason-native' architectures)",
                    "effect": "Increases probability. RL approaches like DeepSeek-R1, R-Zero (Tencent), and extended-step RL (NVIDIA ProRL) show that reasoning skills previously capped by imitation learning are now advancing. These enable self-improvement beyond annotated data, matching IMO-level proof construction and suggesting capacity to generalize to adversarial domains like FrontierMath."
                },
                {
                    "factor": "Absence of explicit, public claims of >85% on FrontierMath as of September 2025",
                    "effect": "Decreases probability. Despite press attention to AIME, IMO, and ICPC breakthroughs, no system has yet been publicly documented at >85% on FrontierMath. If models had reached such a milestone, it is highly likely it would be celebrated and cited, as with the 'gold medal' IMO results."
                },
                {
                    "factor": "Time left for further model releases, ensemble approaches, and strategic targeting",
                    "effect": "Increases probability, but modestly. Several months remain for OpenAI, Google DeepMind, or DeepSeek to directly target FrontierMath. Past months have shown rapid capability jumps. Since new model releases (e.g., GPT-5) can be rapidly finetuned or subjected to multi-pass, ensemble solution techniques, a targeted assault is possible."
                },
                {
                    "factor": "Historical base rates for transformative AI math benchmark jumps within a 12-month period",
                    "effect": "Slightly increases probability. Since 2023, SOTA math benchmarks have witnessed significant, discontinuous jumps almost semi-annually: from <50% to 86% on AIME, then to gold in IMO and ICPC perfection within a year. However, truly adversarial benchmarks like ARC-AGI-2 see only incremental improvement."
                }
            ],
            "no_scenario": "Despite rapid advances, the adversarial and OOD aspects of FrontierMath prove too great. Top models like GPT-5, Gemini 2.5, and DeepSeek-R1 top out at 65\u201380%; self-verification and reasoning-native architectures help but plateau. Attempts to ensemble, augment with symbolic solvers, or run multi-hour agentic modes yield only incremental improvements. No published blog, paper, or report claims >85% performance before Jan 1, 2026. The result is analogous to ARC-AGI-2 and ZeroBench, which remain unsolved by SOTA systems.",
            "yes_scenario": "Within months, OpenAI, DeepMind, or DeepSeek explicitly target the FrontierMath benchmark in model training, RL-finetuning, or massive ensemble inference mode (e.g., tree-of-thoughts, massive self-consistency). GPT-5, perhaps assisted by an experimental agentic reasoning model or hybrid neuro-symbolic system, achieves >85% on official FrontierMath problems with strong chain-of-thought and formal verification. The result is documented in an official paper, benchmark website post, or major AI lab blog post and is widely reported by January 1, 2026.",
            "final_reasoning": "Recent months have seen AI systems shatter historic ceilings on AIME, IMO, and ICPC, establishing clear superhuman reasoning on math and algorithmic benchmarks once deemed unreachable. RL and reason-native paradigms unlock OOD reasoning, and there is ample time for labs to focus on FrontierMath. However, notably, there is still no public documentation of >85% FrontierMath performance. The adversarial novelty and intentional OOD challenges of FrontierMath make this a different beast than AIME or IMO, with a proven track record of resisting SOTA progress. Effective performance may require not just scaling, but fundamentally new algorithmic strategies, which might not emerge in three months. Yet, given the momentum, the high likelihood that labs will attempt (and potentially succeed with) ensembles or new methods, and the well-established incentives to claim this milestone, the probability merits being notably above 50%, but not extremely so due to the lack of concrete, public signals. Therefore, I estimate a probability of 68%.",
            "final_probability": 68
        },
        "Artificial Intelligence (Scaling Laws in AI)": {
            "time_to_resolution": "Approximately 3.5 months (from 2025-09-21 to 2026-01-01)",
            "perspective_relevance": "As an expert in AI scaling laws and mathematical reasoning benchmarks, my insights provide nuanced understanding of the relationship between model size/compute/training technique and steeps in mathematical problem-solving capability. I can parse technical claims about new training regimes, architectural shifts (like reasoning-native and RL-based models), and contextualize state-of-the-art mathematical benchmarks like AIME, IMO, SWE-bench, and crucially, the challenges specific to FrontierMath. This helps ground predictions in both historical scaling patterns and recent step-function advances.",
            "status_quo": "As of early 2025, no AI had achieved >85% on FrontierMath; the benchmark was considered extremely challenging as of its introduction, and even top-tier models (GPT-4, Gemini 1.5, Claude Sonnet, etc.) had not come close. Historically, new, hard benchmarks remain unsolved for 1\u20132 years after release, with only steady, incremental progress.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent rapid progress on adjacent benchmarks (AIME, IMO, Math Olympiads, ICPC, SWE-bench)",
                    "effect": "Strongly increases probability; GPT-5, DeepSeek R1, and Google Gemini 2.5 have posted unprecedented scores, at or above gold-medal-winning human levels on Olympiad and AIME tasks which until 2024 were considered out of reach."
                },
                {
                    "factor": "FrontierMath\u2019s unique difficulty and risk of training/test leakage",
                    "effect": "Moderately decreases probability; the benchmark is designed to be more resistant to memorization and overfitting, and less likely to be directly included in pretraining sets, making it a tougher measure of true mathematical reasoning."
                },
                {
                    "factor": "Architectural breakthroughs: reinforcement learning from reward, system-2 scaling, self-consistency/chain-of-thought, inference-time scaling",
                    "effect": "Strongly increases probability; DeepSeek R1 and similar models develop genuine new reasoning strategies via RL, and ProRL/NVIDIA show that more compute and longer RL training cycles improve reasoning. \u201cThinking\u201d modes and ensemble approaches are closing the gap on previously unsolved problems."
                },
                {
                    "factor": "Competitive race and incentives for research labs (OpenAI, Google, DeepSeek, Anthropic) to claim milestone results",
                    "effect": "Moderately increases probability; labs see public demonstration of mathematical prowess as a branding win, and are motivated to officially record/announce results on new benchmarks for prestige and investment."
                },
                {
                    "factor": "Sample efficiency and cost reduction trends (e.g., DeepSeek\u2019s R1 for <$0.3M)",
                    "effect": "Marginally increases probability, as more actors can attempt state-of-the-art and new training regimes are financially feasible for mid-sized labs, increasing the chances of breakthrough results."
                },
                {
                    "factor": "Publication/documentation bottleneck for official resolution",
                    "effect": "Slightly decreases probability, since public blog posts, peer-reviewed or well-documented score reports are required by the criteria, so an internal achievement without an official writeup could delay or prevent resolution."
                },
                {
                    "factor": "Potential for strategic non-disclosure (deliberate or delayed announcements)",
                    "effect": "Marginally decreases probability in the case that a lab withholds results for competitive or safety reasons, particularly if the result is only obtained close to the deadline."
                },
                {
                    "factor": "Base rates for hard, new math benchmarks",
                    "effect": "Moderately decreases probability; hard new math benchmarks typically take 6\u201318 months before being convincingly surpassed by >85% scores, particularly if careful about contamination."
                }
            ],
            "no_scenario": "Despite strong advances on Olympiads, ICPC, and adjacent benchmarks, no lab manages to solve, or convincingly document solving, >85% on the FrontierMath benchmark before January 2026. Either models underperform due to FrontierMath\u2019s unique rigor, or results exist but are not published/officially documented in time\u2014possibly due to internal testing, or efforts being focused on adjacent high-profile competitions as opposed to this specific, less publicized, but more robust benchmark.",
            "yes_scenario": "OpenAI, Google, DeepSeek, or another major lab produce a blog post, preprint, or official score report documenting a >85% run on the FrontierMath benchmark in late 2025. The result leverages recent advances in RL, chain-of-thought, massive context scaling, and/or ensemble system-2 reasoning. A model such as GPT-5, DeepSeek R1, or Gemini 2.5 demonstrates >85% via self-consistency or inference-time scaling, and the result is promptly published for prestige or strategic advantage, satisfying resolution criteria.",
            "final_reasoning": "The news articles document a transformative leap in AI mathematical reasoning: GPT-5 and DeepSeek R1 now outperform Olympiad gold medalists and elite coders, with top models achieving near-perfect accuracy on previously unapproachable tasks (ICPC, IMO, AIME). ProRL and R-Zero show that extended RL can unlock reasoning capabilities not seen with standard training. While FrontierMath is specifically designed to resist overfitting and test contamination\u2014delaying its defeat by a few months\u2014current models\u2019 scaling, ensemble methods, and new training approaches (e.g., RL with only sparse rewards, large-scale inference-time reasoning) make it plausible for a top lab to achieve and document >85% performance before the January 1, 2026 deadline. There is some real risk that the benchmark\u2019s difficulty may hold off >85%, or that documentation lags progress, but the sheer magnitude of the recent step-function advances on Olympiad-like and harder tasks outweighs this. The largest wildcards are publication bottlenecks or narrow gaps between actual performance and the 85% threshold. On net, I judge the probability as considerably above 50%, but not near certainty, due to the strict specs of FrontierMath and the risk that final crucial problems prove out of reach. I am moderately bold in the probability while reflecting genuine uncertainty.",
            "final_probability": 78
        },
        "Computer Science (Algorithmic Complexity)": {
            "time_to_resolution": "Approximately 3 months and 10 days (until January 1, 2026)",
            "perspective_relevance": "As an expert in Algorithmic Complexity, I bring a granular understanding of what constitutes 'genuine' mathematical reasoning in computational systems. I am able to critically dissect the difference between pattern recognition, heuristic exploitation, and grounded, generalizable symbolic reasoning because state-of-the-art benchmarks like FrontierMath are explicitly designed to distinguish between mere data fitting and robust, reasoning-based problem solving. My discipline also equips me to assess overfitting/gaming risk, the relevance of transfer learning, and whether algorithmic advances (such as reinforcement learning for reasoning) are likely to generalize from IMO/AIME/ICPC settings to new benchmarks like FrontierMath.",
            "status_quo": "As of late 2024 and early 2025, the highest performance recorded by large language models on FrontierMath and other hardest advanced math benchmarks was well below 85%. No model had conclusively crossed this line; most public scores ranged between 35-70% depending on the evaluation and benchmark stringency.",
            "perspective_derived_factors": [
                {
                    "factor": "Recent Breakthroughs in Mathematical Reasoning by LLMs (AIME, IMO, ICPC)",
                    "effect": "Strongly increases: GPT-5, Gemini 2.5, and DeepSeek R1 have equaled or surpassed top human performance in standardized math and programming competitions with some public results above 85% on AIME and gold-level scores on IMO and ICPC; this is a massive leap relative to 2024 capabilities."
                },
                {
                    "factor": "Benchmark Overfitting and Goodhart's Law",
                    "effect": "Moderately decreases: Evidence shows that some benchmarks have been saturated or gamed once they become popular, making true reasoning advances more difficult to gauge. However, FrontierMath is cited explicitly as a newer, harder-to-game evaluation."
                },
                {
                    "factor": "Transferability from AIME/IMO/ICPC to FrontierMath",
                    "effect": "Moderately increases: The successful solving of Olympiad-level math problems, advanced algorithmic tasks, and surpassing human performance suggests rapidly improving generalization. But the true structure of FrontierMath matters: if the problems are even more nonstandard or adversarial, transfer may be incomplete."
                },
                {
                    "factor": "Speed and Economy of Algorithmic Innovation (RL, Extended Chain-of-Thought)",
                    "effect": "Increases: DeepSeek\u2019s work (and similar by Tencent and others) shows that reinforcement learning for reasoning, self-consistency, and multi-step chains have led to sudden leaps in performance for substantially reduced cost, implying that even if a model hasn't publicly broken 85% yet, it could with a dedicated, inexpensive effort in coming months."
                },
                {
                    "factor": "Evidence of Public, Documented Results",
                    "effect": "Moderately increases: Based on the recent news, AI labs are incentivized to publicly document breakthroughs (Nature paper for DeepSeek, preprints, and competitive press releases for GPT-5 and Gemini) rather than holding them private; this is specifically required for the question to resolve to Yes."
                },
                {
                    "factor": "Degree of Overlap Between Current SOTA and FrontierMath",
                    "effect": "Slightly decreases: While AIME/IMO/ICPC results are extremely impressive, there remains a nonzero probability that FrontierMath contains qualitative differences, especially adversarially generated or novel types of mathematics that may temporarily stymie even SOTA models."
                }
            ],
            "no_scenario": "Despite high performance on IMO, AIME, and ICPC, no lab is able (or willing) to publicly cross >85% on FrontierMath by Jan 1, 2026. This could be because the benchmark's problems exploit current LLM weaknesses (e.g., true abstraction, semantic ambiguity, multi-step proofs requiring backtracking), or because companies prioritize other milestones. Alternatively, results might leak informally but not be published in the documentation formats required for question resolution.",
            "yes_scenario": "Within the next quarter, a major lab (likely OpenAI, Google DeepMind, or DeepSeek) releases a model, or an ensemble/agent system, that achieves >85% on FrontierMath. This is documented in a peer-reviewed paper, official blog post, or leaderboard entry, possibly hyped in conjunction with other recent world-beating achievements (e.g., 100% at ICPC or gold-level IMO). The result is independently verifiable and widely publicized, in keeping with the trend of competitive public documentation.",
            "final_reasoning": "The last three months have featured dramatic, public advances in AI mathematical reasoning: multiple SOTA LLMs have equaled or surpassed the world's best humans in AIME, IMO, and ICPC, including robust gold-level scores and even perfect submissions. The structure of these problems\u2014long, nontrivial, and requiring multi-step symbolic reasoning\u2014considerably overlaps with the intention of the FrontierMath benchmark. While it is possible that FrontierMath's adversarial or novel flavor (or secrecy) temporarily slows headline results, the innovation rate is such that, even if SOTA models don't yet score 85% in September, continued algorithmic and inferential scaling (e.g., agentic reasoning, longer context windows, RL for reasoning strategies) make it highly likely that someone pushes over 85% in the next three months\u2014especially since cheap RL approaches are accelerating iteration. The biggest risks are either a 'moving goalpost' effect (where organizers update FrontierMath to make it harder) or a lack of public, documented disclosure. However, given incentives and recent patterns, these are unlikely to outweigh the base rate of rapid public achievement. Therefore, my probability must be highly confident but not absolute; the status quo has already shifted to yield nothing but rapid breakthrough signals.",
            "final_probability": 90
        },
        "Computer Science (Computational Learning Theory)": {
            "time_to_resolution": "Approximately 3.5 months (until January 1, 2026).",
            "perspective_relevance": "As an expert in computational learning theory, I focus on the limits and scaling laws of learning systems, the relationship between model complexity and generalization, and the design of benchmarks that test true 'algorithmic reasoning' rather than pattern recognition. I also pay particular attention to the phenomenon of overfitting to benchmarks, the difference between training and test distributions, and methodology caveats that can inflate apparent progress. This allows me to evaluate, with rigor, whether recent improvements in AI mathematical reasoning are likely to cross the high bar set by FrontierMath (>85%), without being deceived by benchmark gaming or saturation on easier tests.",
            "status_quo": "As of early 2025, no AI model had come close to 85% on FrontierMath. State-of-the-art LLMs (e.g., GPT-4, Gemini) exhibited strong performance on AIME and even IMO-level problems, but usually fell short of the hardest synthetic and adversarial mathematical reasoning benchmarks, including FrontierMath. Incremental progress was expected, but consistent superhuman performance on this class of non-trivial, composite-reasoning math has historically required new algorithmic breakthroughs or paradigm shifts.",
            "perspective_derived_factors": [
                {
                    "factor": "Breakthroughs in Training Regimes (e.g., DeepSeek-R1, ProRL, Self-Curricula, Self-Verification)",
                    "effect": "Increases probability. The rise of models like DeepSeek-R1, which use RL-based training, shows that AI can develop step-by-step approaches to reasoning and verification autonomously. ProRL also indicates models can reach new modes of reasoning not accessible to base models, suggesting that compound reasoning benchmarks (like FrontierMath) may be closer to being cracked than previously believed, especially as research community knowledge compounds."
                },
                {
                    "factor": "Rapid Scaling and Compute Democratization",
                    "effect": "Increases probability. The dramatic reduction in cost to reach frontier performance (e.g., DeepSeek-R1 for ~$300k) means more actors and iterations, increasing the probability that a qualifying model is produced, especially given the public benchmarks and competitive environment."
                },
                {
                    "factor": "Still-Limited Performance on the Hardest Synthetic Benchmarks",
                    "effect": "Decreases probability. While models now routinely score at or above gold medal levels on IMO and achieve >94% on AIME, there are hints that performance on benchmarks specifically designed to confound pattern recognition remains limited\u2014i.e., ARC-AGI, EnigmaEval, and prior statements (e.g., 'all models score zero on ZeroBench') indicate that designing genuinely adversarial math challenges still stumps even the best models. This suggests a possible gap between current achievements and the 85% threshold on FrontierMath, which is intentionally resistant to benchmark-overfitting and training-leakage."
                },
                {
                    "factor": "Recent Explosive Progress on Adjacent or Harder Math/Coding Benchmarks",
                    "effect": "Increases probability. Since early 2025, models like GPT-5, DeepSeek-R1, and Gemini have reached or exceeded human gold-medal level in Olympiad settings, solved all 12 problems at ICPC under time pressure, and reached near-human or superhuman performance on SWEBench and AIME. Although these are adjacent, not identical, the rate of progress is much faster than historical trends."
                },
                {
                    "factor": "Resolution Criteria and Publication Requirements",
                    "effect": "Slightly decreases probability. The requirement for >85% FrontierMath performance to be published in a public document (e.g., paper, blog) is a mild but real bottleneck: some companies may withhold results for competitive or safety reasons, or not adequately document their accomplishment before the deadline."
                },
                {
                    "factor": "Increased Industry Incentives and Prestige for Achieving the Milestone",
                    "effect": "Increases probability. There is now significant reputational and recruiting value to being the first to achieve this major public benchmark, incentivizing public announcement and official documentation if it is achieved."
                },
                {
                    "factor": "Model/Benchmark Overlap and Generalization Limits",
                    "effect": "Decreases probability. Numerous articles, and the nature of the benchmark itself, warn about Goodhart\u2019s law and the risk that models 'learn the test' (i.e., the model has seen the problems or distribution during training). FrontierMath specifically guards against this\u2014if held to best practices, it may be that genuine algorithmic reasoning is still needed rather than pattern exploitation, making the 85% bar much harder than IMO/AIME/ICPC."
                },
                {
                    "factor": "Time Remaining to Resolution",
                    "effect": "Increases probability. Over 3 months remain, during which new models (e.g., GPT-5 variants, DeepSeek-R1.5/R2, Gemini 3) could be released and specifically target this milestone. History shows that new public benchmarks and feats are often targeted in this window."
                }
            ],
            "no_scenario": "Despite soaring results on adjacent benchmarks (IMO, AIME, ICPC, SWEBench), all models max out around 60-80% on FrontierMath. Attempts to bridge the remaining gap reveal a 'reasoning wall'\u2014models either hallucinate, falter on long proofs, or simply cannot synthesize new solution paths for genuinely novel problems. Even novel RL+verifier approaches fail to generalize perfectly. Result: No AI model documented above 85% on FrontierMath, or any such result remains unpublished due to timing, caution, or lack of verification.",
            "yes_scenario": "A new GPT-5 variant (or DeepSeek-R1.5, Gemini, or an ensemble approach like those used in ICPC) is trained with a combination of RL-based self-curricula, self-verification, reflection, and extended inference times (e.g., System-2 scaling or external tool use). This model is deliberately benchmarked on FrontierMath, possibly with publication timed to coincide with a major conference or official blog post. The milestone is announced in a high-profile document or major blog, with performance >85%, easily exceeding the threshold due to rapid compounding research and resource allocation, possibly with self-consistency or multi-agent techniques. OpenAI, DeepSeek, or Google/DeepMind releases the result before January 1, 2026.",
            "final_reasoning": "Progress on difficult and even adversarial math and reasoning benchmarks has sharply accelerated in the past year. GPT-5, DeepSeek-R1, and Gemini have publicly achieved superhuman results on AIME, IMO, and world coding competitions (ICPC), indicating that their underlying reasoning and tool-use capabilities are approaching and sometimes exceeding those required for synthetic, unseen benchmarks. Innovations in RL-based training, self-curricular learning, and verification suggest that prior obstacles to 'true' math reasoning are falling, especially as more actors can now participate due to reduced compute barriers. However, there are legitimate concerns: genuine generalization to FrontierMath is more difficult than to standardized competitions, the distribution may differ in subtle but significant ways, and Goodhart's law plus potential data leakage means high scores on other metrics are imperfect predictors. Still, history shows that with multiple high-profile actors racing to cross an official, publicized threshold (with substantial prestige and economic rewards), and with ~3.5 months to go, the probability is heavily elevated above the base rate. Given recent model performance (AIME: 94.6%; IMO: gold medal/83%+; adversarial coding: 86-90%), and assuming one major new release and at least one group explicitly targets this milestone, I estimate the chance of a documented Yes result is very high\u2014though not quite 'certain,' accounting for public-release and adversarial robustness risk.",
            "final_probability": 93
        },
        "Mathematics Education (Mathematics Problem Solving Research)": {
            "time_to_resolution": "Approximately 3 months and 10 days (until Jan 1, 2026)",
            "perspective_relevance": "As an expert in mathematics education and mathematical problem solving research, I bring a nuanced understanding of the complexity of mathematical benchmarks, the depth of abstract reasoning required, the historical difficulty models have faced with rigorous math problem solving versus programming/logic tasks, and the nature of rapid but uneven progress across subdomains of math. My perspective allows me to critically compare benchmark achievements (IMO, AIME, ICPC) to the unique hurdles of the FrontierMath benchmark, assess the transferability of recent breakthroughs, contextualize claims of model success, and scrutinize how educational and cognitive theory aligns or conflicts with machine reasoning advances.",
            "status_quo": "No AI system has yet been publicly documented as surpassing 85% on the FrontierMath benchmark. The highest reported achievements are on AIME (86.7%), IMO (gold medal performances), and programming olympiads (ICPC perfect score), but there is no direct evidence yet of >85% on FrontierMath.",
            "perspective_derived_factors": [
                {
                    "factor": "Breakthroughs in AI mathematical reasoning as evidenced by human-level/above-human results on AIME/IMO/ICPC in 2025",
                    "effect": "Strongly increases the probability, as these results signal a dramatic leap in general, abstract, and complex problem-solving capability, previously thought years away."
                },
                {
                    "factor": "Nature and novelty of the FrontierMath benchmark (difficulty, resistance to simple pattern-matching, requirement for deep symbolic reasoning, and low contamination/overfitting risk)",
                    "effect": "Moderately decreases the probability, since many recent benchmarks (AIME, IMO) have known structures/datasets, and FrontierMath may have been specifically designed to thwart shortcut solutions; solving it is a qualitatively different challenge."
                },
                {
                    "factor": "Recent emergence of self-reinforcing, RL-based training and self-improving curricula (eg. DeepSeek-R1, R-Zero, ProRL)",
                    "effect": "Increases the probability, as these architectures appear to enable generalization beyond narrow tasks and may support transfer to ultra-hard, novel problems."
                },
                {
                    "factor": "Documented rapid scaling\u2014both economically (cheap SOTA training) and scientifically (models outperforming humans in competitions and benchmarks in rapid succession)",
                    "effect": "Moderately increases probability; the democratization of SOTA AI indicates resources and talent are no longer bottlenecks and many entities are motivated to claim mathematical milestones."
                },
                {
                    "factor": "Absence, so far, of direct public results >85% on FrontierMath, despite extreme performance claims elsewhere",
                    "effect": "Decreases probability, as leading labs tend to advertise such results as soon as achieved; the lack of public docs suggests the benchmark remains unsolved or is especially recalcitrant."
                },
                {
                    "factor": "Potential for rapid, late 2025 documentation blitz (Labs may delay formal release until a well-timed paper, competition, or benchmark challenge closes)",
                    "effect": "Moderately increases probability, as similar patterns occur when teams have results in hand but wait for publication or competition deadlines."
                },
                {
                    "factor": "Base rate of new, hardest AI benchmarks being smashed within 1 year of their introduction at moments of rapid progress",
                    "effect": "Increases probability, as historically, once AI achieves a leap across several adjacent/math benchmarks, the lag to conquering the next 'frontier' has shortened dramatically."
                },
                {
                    "factor": "Resolution requirement for public documentation (peer-reviewed paper, conference, or developer blog\u2014no hearsay or private leaderboard)",
                    "effect": "Slightly decreases probability, reliance on formal publication pipelines and possible strategic delays could limit eligible 'Yes' resolutions even if private results exist."
                }
            ],
            "no_scenario": "Despite achieving or even surpassing 85% on AIME, IMO, and programming benchmarks, no AI system achieves >85% on the FrontierMath benchmark before January 1, 2026\u2014either due to its problem set being much more resistant to current breakthroughs (requiring fundamentally new architectures, symbolic manipulation, or formal proof capabilities not yet acquired), or because a team achieves the milestone but opts to withhold or delay public documentation (e.g., for commercial, strategic, or competitive reasons), thus failing the resolution criterion.",
            "yes_scenario": "An AI system\u2014likely from OpenAI (GPT-5, an experimental reasoning model), Google DeepMind (Gemini series), Anthropic, DeepSeek, or perhaps a new RL-driven open-source approach\u2014demonstrates >85% accuracy on the FrontierMath benchmark and its results are documented via a peer-reviewed paper, official benchmark page, or high-credibility developer blog prior to January 1, 2026. The achievement is publicized as the next AI milestone following the recent gold/diamond-level performances at AIME, IMO, and ICPC, and includes chain-of-thought, self-reflection, or verifier-backed outputs to satisfy skeptics on genuine reasoning.",
            "final_reasoning": "The evidence from Sept 2025 news marks a tipping point in AI mathematical reasoning: OpenAI, DeepMind, DeepSeek, and others now have models that consistently outperform elite human problem-solvers in algorithmic, olympiad-level, and real-math exams (IMO, ICPC), with self-consistency, multi-step reasoning, and RL-based curriculum learning. These feats were, until 2024, considered unreachable for AI. The distinctiveness of the FrontierMath benchmark (hard, novel, low contamination, explicitly designed to test general reasoning) means that a direct transfer of performance is not guaranteed\u2014historically, new benchmarks resist immediate conquest, especially when designed with anti-shortcut bias. Yet, there are powerful precedents: when AI crosses a threshold (e.g., Go, language/puzzle reasoning), the timeframe to conquer adjacent, even harder 'frontier' benchmarks collapses. Given that models are excelling at areas like AIME (up to 94.6%\u2014notably even without multi-modal tricks), hitting IMO gold/silver levels, and programming tasks that require real mathematical insight, it is plausible that the leading labs have results on FrontierMath that approach (but perhaps not yet exceed) the 85% bar. The significant downside risk is that the particular novelty, difficulty, and design of FrontierMath does hold out until post-2026, or public documentation lags. Still, the empirical evidence heavily points to a 'Yes' within the resolution window, with a non-trivial tail risk of delay or non-documentation. A sharp update from a status-quo ~25-35% (pre-August '25) to a much higher value is justified. My estimate\u2014balancing generalization gaps, incentive for labs to publish, and recency of other feats\u2014is:",
            "final_probability": 82
        }
    },
    "deliberation_probability": [
        85,
        83,
        85,
        68,
        78,
        90,
        93,
        82
    ],
    "deliberation_mean_probability": 83.0,
    "deliberation_sd": 7.634508123364221,
    "deliberation_probability_result": 83,
    "group_results": {
        "machineLearningEngineeringLargeLanguageModelsDevelopment": {
            "forecaster_to_engage": "Artificial Intelligence (Deep Learning Theory)",
            "response_type": "critique",
            "response": "Your lower probability (68%) is rooted in appropriate caution about the adversarial, OOD design of FrontierMath, noting it may require fundamentally new algorithmic strategies to breach 85%. However, as someone deeply familiar with the current ML engineering pipeline and LLM fine-tuning for mathematical benchmarks, I'd argue you underweight the demonstrated cross-benchmark transfer exhibited by recent SOTA models. Not only are open-ended, proof-oriented benchmarks (e.g. IMO, ICPC) now handled at or above human-expert level, but advances in method\u2014like DeepSeek's binary-reward RL that generated new reasoning chains, and scalable verification/consistency\u2014demonstrate the field no longer relies on brute force or shallow pattern-matching. While last-mile difficulty is real (and so publication risk), the empirical evidence is that labs are rapidly generalizing from one benchmark genre to others, and aggressively targeting novel, held-out evaluations. There is also significant competitive pressure and prestige from being first to exceed 85% and document it publicly, especially since compute and RL-based innovation have now democratized creation of ultra-competitive models. In short: your argument for a 'new beast' is valid, but the current velocity and mechanisms of progress mean genuine algorithmic novelties are emerging in real time. The probability is notably higher than your estimate\u2014recent months show that such walls are now scaled in weeks or months, not years."
        },
        "machineLearningEngineeringBenchmarkEvaluationStandards": {
            "forecaster_to_engage": "Artificial Intelligence (Deep Learning Theory)",
            "response_type": "critique",
            "response": "Your estimate (68%) appropriately recognizes the exceptional adversarial design and out-of-distribution nature of FrontierMath, and the lack of current public >85% performance. However, from a benchmark engineering perspective, you may overstate the likelihood that fundamentally new algorithmic paradigms are needed beyond the 'reasoning-native' approaches already in use. The evidence from the last several months\u2014notably DeepSeek R1's self-reinforcement, OpenAI and DeepMind's pre-emptive dominance of AIME, IMO, and ICPC (with near-zero lag from benchmark launch to superhuman performance), and the transfer of RL-based reflection and checking techniques to new domains\u2014suggests that current leaders already possess both the scalable infrastructure and the capacity for rapid 'last-mile' adaptation. Add to this the strategic importance for labs to rapidly and publicly claim novel benchmarks, and the risk-weighted base rate for crossing such new thresholds is now substantially higher than historical precedent would suggest. Your view is prudent, but I believe it is mildly too conservative for September 2025: the recent pattern is that once performance is within 10\u201315% of a threshold on an adjacent benchmark, that final jump occurs within a single model iteration or a cumulative product of ensembling/S2 inference, especially when the reward for public demonstration is so high."
        },
        "aiPolicyAndForecastingTechnologyForecastingMethodologies": {
            "forecaster_to_engage": "Artificial Intelligence (Scaling Laws in AI)",
            "response_type": "defense",
            "response": "Your forecast (78%) is well-calibrated given the available evidence, and highlights an important nuance: the risk that the last increments on a new, adversarial benchmark are harder than simply extrapolating from previous rapid gains. You recognize that scaling, ensembling, and new RL techniques make >85% plausible, yet also factor in uncertainty due to the possibility of publication lag or benchmark resilience\u2014even with superhuman performance demonstrated elsewhere. From an AI Policy and forecasting methodology angle, your emphasis on the combination of breakthrough advances and nontrivial tail risk of near-miss is more justified than the most bullish forecasters. You correctly avoid both the pitfalls of linear extrapolation and excessive skepticism; your approach provides a rigorously defensible middle ground given both the recency and limitations of available public results."
        },
        "artificialIntelligenceDeepLearningTheory": {
            "forecaster_to_engage": "Computer Science (Computational Learning Theory)",
            "response_type": "critique",
            "response": "Your probability (93%) is notably bolder than mine and, while you correctly emphasize the impressive scaling and transfer of recent models across benchmarks as well as the powerful incentive structure for public disclosure, I believe your forecast slightly underestimates the degree to which adversarially-constructed, genuinely novel benchmarks slow SOTA transfers. Even if recent systems demonstrate remarkable flexibility (AIME, IMO, ICPC), the existence of held-out and anti-leakage benchmarks like ARC-AGI-2 and ZeroBench has shown that true generalization can plateau in the final stretch for months. The absence of >85% public results on FrontierMath despite considerable AI hype, and the consistent failure of top models to instantly crack other adversarial platforms, should factor more heavily to counter the momentum heuristic. A high probability is warranted, but tail risk for last-mile difficulty and documentation is meaningfully higher than your estimate implies. Overconfidence on these extreme tail benchmarks risks Goodharting on very recently-solved (and more familiar) tasks, rather than reflecting the evidence gap still present for the question at hand."
        },
        "artificialIntelligenceScalingLawsInAi": {
            "forecaster_to_engage": "Mathematics Education (Mathematics Problem Solving Research)",
            "response_type": "defense",
            "response": "Your reasoning insightfully balances the historic resistance of new mathematical benchmarks to immediate AI progress with the clear inflection point represented by recent model advances. From a scaling law perspective, I concur with your sharp update: when models transition from merely human-level to decisively superhuman in adjacent domains (Olympiads, ICPC, AIME), scaling history strongly predicts a short time lag before generalization conquers adjacent, harder benchmarks like FrontierMath. You also correctly foreground the crucial risk of documentation lags and the deliberately novel, anti-shortcut construction of FrontierMath\u2014but do not let that swamp the large, clear signals of transformative capability, competitive incentive, and cross-domain transfer. Your probability (82%) is well-justified as an aggressive upward shift while still acknowledging a meaningful tail of risk. Overall, your view is well-calibrated and highly defensible from a scaling law and technical forecasting standpoint."
        },
        "computerScienceAlgorithmicComplexity": {
            "forecaster_to_engage": "Machine Learning Engineering (Benchmark Evaluation Standards)",
            "response_type": "defense",
            "response": "Your forecast (83%) is compellingly constructed from the vantage of someone who understands both the technical pipeline for AI benchmark evaluation and the sociology of competitive documentation. You specifically emphasize the premium placed on public, resolving documentation\u2014justified by the trend of labs rapidly publishing and hyping breakthroughs for prestige\u2014and the distinct anti-gaming, OOD nature of FrontierMath, which raises, but does not dominate, the risk profile. From my perspective as an algorithmic complexity specialist, your point that many of the algorithmic building blocks (RL, chain-of-thought, verification at scale, self-refinement) are already mature enough to generalize is particularly sound: this directly addresses concerns that fundamentally new techniques are needed for the final leap. Your use of historical base rates as an anchor, then commensurately updating for 2025's step-function progress and the dynamics of fresh difficult benchmarks, results in a well-calibrated and nuanced probability. You neither overstate the ease of crossing the bar nor anchor too stubbornly to past benchmark inertia, making your approach robust to both technical and publication dynamics related to the question."
        },
        "computerScienceComputationalLearningTheory": {
            "forecaster_to_engage": "Machine Learning Engineering (Benchmark Evaluation Standards)",
            "response_type": "defense",
            "response": "Your probability (83%) stands out as particularly robust when viewed through the lens of computational learning theory. Your incorporation of both the technical side\u2014the rapid and generalizable progress enabled by reinforcement learning, chain-of-thought prompting, and verification techniques\u2014and the sociological/publication side is grounded and realistic. Critically, you identify that the novelty and anti-leakage principles behind FrontierMath limit direct transfer from other benchmarks, yet don't overwhelmingly prevent it, especially given the well-resourced incentive structure and multi-actor race for public proof of capability. This echoes canonical learning-theoretic models: once a family of hypotheses (model/technique) is within a close neighborhood of the true risk on the test distribution, a moderate increment in resources and search often closes the gap. Your nuanced probability properly adjusts for the upward base-rate shift while acknowledging the last-mile, documentation, and benchmark novelty risks that can disproportionately affect resolution. This strikes an effective middle ground and offers a high-quality, defensible forecast."
        },
        "mathematicsEducationMathematicsProblemSolvingResearch": {
            "forecaster_to_engage": "AI Policy and Forecasting (Technology Forecasting Methodologies)",
            "response_type": "defense",
            "response": "Your reasoning is particularly strong in its focus on base rates and the incentive structure around public documentation. From a mathematics education and mathematics problem solving perspective, your emphasis that leading labs are now strongly motivated to publicize frontier benchmark results is well justified. The historic pattern you call out\u2014where technical achievement closely precedes or rapidly triggers a documented milestone after a highly public push\u2014is consistent with how AI reasoning milestones have played out in the past year. You appropriately temper optimism by acknowledging disclosure and novelty risks, but don't let the tail scenario dominate your mainline probability. This is the sort of balanced judgment that provides reliability across rapid regime shifts, blending evidence from economic, technical, and institutional trends. I find your estimate (85%) and underlying logic to be robust, and an excellent illustration of pragmatic forecasting in an unprecedentedly dynamic domain."
        }
    },
    "revision_results": {
        "Machine Learning Engineering (Large Language Models Development)": {
            "my_phase1_final_probability": 85,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, there is a strong consensus among forecasters with relevant expertise that recent advances in mathematical and logical reasoning benchmarks by SOTA LLMs have greatly increased the probability of exceeding 85% on FrontierMath before 2026. Several independent fields (benchmark evaluation, AI scaling, theory, policy, and math education) cite the extraordinary rate of progress, cross-benchmark generalization, and the competitive incentives among leading labs to publicly document such a milestone. The risks\u2014FrontierMath's adversarial design and potential last-mile/publication lags\u2014are real but seen as outweighed by ongoing breakthroughs and incentive structures. My initial reasoning is aligned with this majority view. While some lower estimates (e.g., 68%) appropriately highlight benchmark novelty, the bulk of evidence and cross-disciplinary analysis supports staying at the higher end of the probability range. No new information compels a downward or upward revision, so I maintain my estimate.",
            "revised_probability": 85
        },
        "Machine Learning Engineering (Benchmark Evaluation Standards)": {
            "my_phase1_final_probability": 83,
            "reasoning_for_revised_probability": "After reviewing the group deliberation, particularly the consensus among domain experts in Machine Learning Engineering, Technology Forecasting, and Algorithmic Complexity (with probabilities clustered in the high 80s and 90s), the original caution regarding last-mile difficulty and the unique adversarial design of FrontierMath still holds, but appears slighty overweighted. The critiques and defenses highlight a common perspective: cross-benchmark generalization is now reliably achieved with rapid model iterations, with labs highly incentivized to document such milestones publicly. The most compelling update comes from the observed empirical fact that, once current models are within 10\u201315% of a new threshold on adjacent benchmarks, the final jump is typically made within a single release cycle or ensemble approach. However, as the status quo is still that no result has been made public for FrontierMath itself, I remain wary of a small but real chance of publication/benchmark-specific barriers persisting. Therefore, I modestly increase my forecast to reflect strong consensus and historical trajectory, but not fully to the 90s\u2014balancing the very high likelihood with a thin tail risk.",
            "revised_probability": 87
        },
        "AI Policy and Forecasting (Technology Forecasting Methodologies)": {
            "my_phase1_final_probability": 85,
            "reasoning_for_revised_probability": "After reviewing the detailed arguments of other forecasters, my central estimate remains robust but becomes mildly more cautious. The critiques of the more conservative forecast (68%) focused heavily on uncertainty about the transferability of recent mathematical reasoning advances to specifically adversarial benchmarks like FrontierMath and the risk of last-mile failures or delays in public documentation. While my initial estimate adequately incorporated these risks, the overall consensus of expert peers is slightly less bullish than my previous estimate, with the most rigorous analyses clustering in the 78-83% range and highlighting that the gap between IMO/AIME and FrontierMath might plausibly be greater than current public results suggest. Nonetheless, no strong new downside evidence has emerged; the preconditions for a breakthrough and public documentation still appear highly favorable. To best reflect both my framework and this peer consensus, I am modestly lowering my probability, not because the evidence for a 'Yes' outcome has weakened, but to better calibrate against residual benchmark-specific and disclosure risks.",
            "revised_probability": 82
        },
        "Artificial Intelligence (Deep Learning Theory)": {
            "my_phase1_final_probability": 68,
            "reasoning_for_revised_probability": "The group deliberation, particularly from Machine Learning Engineering and Benchmark Evaluation perspectives, leaned more heavily on the ability of SOTA models to rapidly generalize to new benchmarks, citing recent ultra-fast jumps on IMO and ICPC that were previously considered out of reach. These forecasters emphasize the 'weeks-not-years' dynamic for closing last-mile benchmark gaps when high incentives are present. On the other hand, my engagement with the Computational Learning Theory perspective reinforced legitimate caution about the unique difficulty of purposely adversarial, OOD benchmarks like FrontierMath, as seen with ARC-AGI-2 and ZeroBench. Importantly, no public >85% FrontierMath result exists as of late September, which ought to be weighted in forecasting. My synthesized view is that optimism is warranted, and the probability may be slightly higher than my original, but not as high as the 85%\u201393% range. I modestly raise my estimate in recognition of both the empirical pattern (rapid, multi-benchmark jumps) and the non-negligible risk of last-mile stall, documentation lags, or unique benchmark resilience.",
            "revised_probability": 73
        },
        "Artificial Intelligence (Scaling Laws in AI)": {
            "my_phase1_final_probability": 78,
            "reasoning_for_revised_probability": "The group discussion included extensively reasoned, high-confidence forecasts from adjacent expert domains\u2014ML Engineering, CS Learning Theory, Mathematics Education\u2014all of whom converged on probabilities (82\u201393%) somewhat higher than my initial 78%, emphasizing that 'last-mile' transfer tends to occur rapidly once SOTA models are within striking distance on adjacent benchmarks. Critiques of more conservative, lower estimates highlighted (with strong historical support) the surprising speed at which recent reasoning benchmarks have fallen, while defenses of a moderate-high estimate (such as my own) stress warranted caution for the final increments due to FrontierMath's adversarial design and documentation bottlenecks. The main update is further confidence that strategic incentives and the robustness of current RL/scaling breakthroughs will outweigh remaining tail risks, especially with multiple labs now able to make serious attempts. However, considering that virtually all forecasters allow for some nontrivial chance of near-miss, publication lag, or last-hard-problem effect (and given no public >85% result is confirmed as of September), I still assign a non-trivial probability to 'No'. On net, I update modestly upward to reflect the consensus and strength of evidence, but retain some tempering for the frontier nature of the benchmark.",
            "revised_probability": 84
        },
        "Computer Science (Algorithmic Complexity)": {
            "my_phase1_final_probability": 90,
            "reasoning_for_revised_probability": "The group deliberation reinforced the high base rate for rapid transfer of recent AI advances to new benchmarks, with multiple expert forecasters highlighting the technical maturity, competitive incentive, and documentation trend pushing toward a breakthrough. However, engagement with the 'Artificial Intelligence (Deep Learning Theory)' forecaster, and the broader set of cautious arguments, brought greater attention to the risk that FrontierMath's adversarial/anti-leakage construction could hold off the final jump despite adjacent breakthroughs elsewhere. Several forecasters prudently emphasized empirical evidence from ARC-AGI and other OOD benchmarks, showing slower progress even in the face of superhuman adjacent task performance. Still, the probability remains high due to the unprecedented capability demonstrations and documented acceleration, but an even bolder stance feels overconfident relative to the uncertainty about true transfer and the lack of explicit public >85% result as of late September 2025. The synthesis justifies a small adjustment downward to better reflect tail risk on adversarial benchmarks and the timing of public documentation.",
            "revised_probability": 87
        },
        "Computer Science (Computational Learning Theory)": {
            "my_phase1_final_probability": 93,
            "reasoning_for_revised_probability": "Group deliberation largely affirmed my phase 1 analysis: the consensus among domain and benchmark experts is that explosive adjacent-benchmark progress, methodological innovation (RL, self-consistency, agentic inference), and competitive documentation incentives mean that a Yes is highly probable. However, several critics (particularly from Deep Learning Theory) rightly emphasized the historical persistence of 'last-mile' gaps on genuinely novel, adversarial benchmarks: ARC-AGI-2, EnigmaEval, and ZeroBench remain unconquered for months despite similar hype and progress in adjacent areas. The group also pointed out that the absence of public >85% FrontierMath results by late September is itself evidence that transfer is non-trivial, and overconfidence risks selective focus on solved adjacent tasks, not the question\u2019s uniquely resilient benchmark. On balance, while the weight of evidence leans strongly toward Yes, adjusting downward to better reflect benchmark anti-leakage design, the real possibility of tail risk, and the possibility that documentation does not co-occur with breakthrough, is justified. Therefore, a modest downward revision, staying boldly positive but not as extreme as before, is warranted.",
            "revised_probability": 87
        },
        "Mathematics Education (Mathematics Problem Solving Research)": {
            "my_phase1_final_probability": 82,
            "reasoning_for_revised_probability": "After considering the group deliberation, especially the perspectives from Machine Learning Engineering (Large Language Models Development), Benchmark Evaluation Standards, and AI Policy and Forecasting, I recognize the substantial consensus around 83-85%. The majority foreground not only the technical plausibility (based on superhuman or near-superhuman results on adjacent math and reasoning benchmarks) but also the overwhelming competitive, reputational, and economic incentives for labs to publish a resolving result if it is achievable. Key critiques of more conservative estimates (e.g., emphasizing lingering OOD-difficulty or the uniqueness of FrontierMath) are persuasive in showing that the final barrier\u2014while real\u2014is unlikely to persist in the context of 2025 breakthroughs, rapid inference-time scaling, and the drive for public recognition. Defenses of mid-80s probabilities are robust and align with both historical base rates post-breakthrough and the well-aligned publication incentives. Given that my earlier estimate already captured substantial tail risk (non-trivial chance of lagging documentation or anti-shortcut hardness), I see justification for a modest upward revision to more tightly align with the modal and median expert view\u2014which I now find more robustly supported after deliberation.",
            "revised_probability": 85
        }
    },
    "revision_probability": [
        85,
        87,
        82,
        73,
        84,
        87,
        87,
        85
    ],
    "revision_mean_probability": 83.75,
    "revision_sd": 4.6827952580239325,
    "revision_probability_result": 84,
    "question_details": {
        "id": 38913,
        "title": "Will an AI achieve >85% performance on the FrontierMath benchmark before 2026?",
        "created_at": "2025-08-31T05:09:18.643187Z",
        "open_time": "2025-09-20T12:00:04Z",
        "cp_reveal_time": "2025-09-20T13:30:04Z",
        "spot_scoring_time": "2025-09-20T13:30:04Z",
        "scheduled_resolve_time": "2025-12-31T22:59:00Z",
        "actual_resolve_time": null,
        "resolution_set_time": null,
        "scheduled_close_time": "2025-09-20T13:30:04Z",
        "actual_close_time": "2025-09-20T13:30:04Z",
        "type": "binary",
        "options": null,
        "group_variable": "",
        "status": "open",
        "possibilities": null,
        "resolution": null,
        "include_bots_in_aggregates": true,
        "question_weight": 1.0,
        "default_score_type": "spot_peer",
        "default_aggregation_method": "unweighted",
        "label": "",
        "unit": "",
        "open_upper_bound": false,
        "open_lower_bound": false,
        "inbound_outcome_count": null,
        "scaling": {
            "range_min": null,
            "range_max": null,
            "nominal_min": null,
            "nominal_max": null,
            "zero_point": null,
            "open_upper_bound": false,
            "open_lower_bound": false,
            "inbound_outcome_count": null,
            "continuous_range": null
        },
        "group_rank": null,
        "description": "This question is synced with an identical question on Metaculus. The original question opened on 2025-02-27 12:56:00 and can be found [here](https://www.metaculus.com/questions/35562). This question will resolve to the same value as the synced question. The original question's background info at this time is below. \n\nThe FrontierMath benchmark, detailed on <u>Epoch AI\u2019s [ FrontierMath page</u> ](https://epoch.ai/frontiermath/the-benchmark), is designed to test cutting-edge mathematical reasoning in AI systems. It consists of challenging, non-trivial problems that require both symbolic and numerical reasoning, going far beyond the scope of standard mathematical benchmarks. As AI research is rapidly advancing\u2014with progress on large language models (LLMs) and mathematical \u201creasoning\u201d systems\u2014achieving a score above 85% on FrontierMath would signal a significant advancement in the field.\n\n`{\"format\":\"metac_closes_in_period\",\"info\":{\"post_id\":35562,\"question_id\":35013}}`",
        "resolution_criteria": "This question will be resolved as **Yes** if, by January 1, 2026, at least one AI system is documented to achieve greater than 85% performance on the FrontierMath benchmark.\n\n The achievement must be reported in a document (such as a peer-reviewed paper, conference presentation, official benchmark publication, or a blog post by the AI developer). A simple reference to progress or internal assessments, without published performance results, will not qualify. If these results are not published, the question will be resolved as **No**.",
        "fine_print": "",
        "post_id": 39536,
        "aggregations": {
            "recency_weighted": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            },
            "unweighted": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            },
            "single_aggregation": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            },
            "metaculus_prediction": {
                "history": [],
                "latest": null,
                "score_data": {},
                "movement": null
            }
        },
        "my_forecasts": {
            "history": [],
            "latest": null,
            "score_data": {}
        }
    },
    "news": "Here are the relevant news articles:\n\n**DeepSeek Reveals How It Achieved an AI Revolution for Only US$294,000**\nDeepSeek\u2019s R1 model was trained using a reinforcement\u2011learning approach that only required a binary reward signal, rather than thousands of step\u2011by\u2011step examples.  The researchers explained that, \"En lugar de ense\u00f1ar expl\u00edcitamente al modelo c\u00f3mo resolver un problema, simplemente le proporcionamos los incentivos correctos y desarrolla de forma aut\u00f3noma estrategias avanzadas de resoluci\u00f3n de problemas,\" and noted that the model spontaneously developed sophisticated behaviours such as self\u2011verification and the frequent use of the word \"wait\" during its reasoning.  During training the average length of the model\u2019s responses grew steadily, from short answers to elaborate chains of reasoning that spanned hundreds or thousands of tokens, a growth that emerged naturally to improve accuracy.\n\nThe results were striking.  On the AIME 2024 competition the model\u2019s accuracy rose from 15.6\u202f% to 77.9\u202f% with a single answer, and to 86.7\u202f% when self\u2011consistency techniques were applied\u2014well above the average human performance in this elite mathematics contest.  It also performed strongly on Codeforces programming challenges and on graduate\u2011level problems in biology, physics and chemistry, demonstrating that its reasoning capabilities extend beyond pure mathematics.\n\nEconomically, the training cost was remarkably low.  According to the supplementary material accompanying the Nature paper, the model was trained for 80\u202fhours on a cluster of 512 Nvidia H800 chips, totaling US$294,000.  This figure includes all operational expenses of the super\u2011computing cluster during the training period, a level of transparency rarely seen in the industry.  The cost is less than 0.3\u202f% of the US$100\u202fmillion that Sam Altman, CEO of OpenAI, had suggested would be required for comparable models.\n\nThe study also revealed that DeepSeek used Nvidia A100 GPUs in the preparatory stages of development.  \"En lo que respecta a nuestra investigaci\u00f3n sobre DeepSeek\u2011R1, utilizamos las GPU A100 para preparar los experimentos con un modelo m\u00e1s peque\u00f1o,\" the researchers admitted.  This admission raises questions about the effectiveness of U.S. export controls, which had banned the export of H100 and A100 chips to Chinese firms since October\u202f2022.\n\nThe implications are significant: if frontier\u2011level AI models can be built with modest investment, more actors could enter the market, potentially reducing the concentration of computational power among a handful of tech giants and challenging U.S. geopolitical strategies aimed at limiting China\u2019s access to advanced AI hardware.\nOriginal language: es\nPublish date: September 19, 2025 09:47 PM\nSource:[El Observador](https://www.elobservador.com.uy/argentina/sociedad/deepseek-revela-como-logro-su-revolucion-inteligencia-artificial-solo-us294000-n6017778)\n\n**OpenAI and Google DeepMind Outshine Students at World\u2019s Top Coding Contest**\nOpenAI and Google DeepMind dominated the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Azerbaijan. OpenAI\u2019s GPT\u20115 solved all 12 problems, achieving a perfect 12\u2011for\u201112 score under the same five\u2011hour time limit as human teams. Eleven of the problems were solved on the first attempt; the twelfth was cracked after nine submissions by an experimental reasoning model. DeepMind\u2019s Gemini\u202f2.5 Deep Think solved 10 of 12 problems, including one that no human team solved, earning a gold\u2011medal level performance. The AI track used the identical problem set, test cases, memory limits, and hardware as the 139 university teams from more than 100 countries. OpenAI said the result would have placed it first in the human leaderboard. Bill Poucher, ICPC Global Executive Director, called the milestone \u2018a key moment in defining the AI tools and academic standards needed for the next generation.\u2019 OpenAI scientist Mostafa Rohaninejad wrote on X that \u2018The next frontier is the discovery of new knowledge, which is the true milestone at the end of the day.\u2019 Gemini also solved a complex network\u2011optimization task that no student team managed, using advanced algorithmic techniques. The contest, which has run for decades, tests 12 problems ranging from graph theory to optimization. Prior to the ICPC, both AI systems earned gold\u2011medal scores at the 2025 International Mathematical Olympiad.\nOriginal language: en\nPublish date: September 18, 2025 07:00 PM\nSource:[Tech Republic](https://www.techrepublic.com/article/openai-deepmind-icpc-2025-results/)\n\n**OpenAI Outperforms Google and Humans: AI Achieves Perfect Score in World Programming Competition**\nOpenAI\u2019s newly released AI system achieved a perfect score in the 2025 International Collegiate Programming Contest (ICPC) held in Baku, Azerbaijan, outperforming both human competitors and Google\u2019s DeepMind model. The system solved all twelve tasks, earning a 100% score within the five\u2011hour time limit and the same conditions as the student teams. According to the company, the system was built from general\u2011purpose reasoning models, none of which were specifically trained for the ICPC. The team used GPT\u20115 together with an internal experimental reasoning model; GPT\u20115 produced correct solutions for eleven of the twelve problems, while the experimental model selected the final solution after nine attempts for the hardest problem. The best human team also solved eleven of the twelve problems. OpenAI highlighted that this success demonstrates the capability of advanced reasoning models to excel where others fail, citing prior gold\u2011level results in the International Mathematical Olympiad and the International Olympiad in Informatics. As stated by the company, the system competed under the same conditions as the students, receiving problems in standard PDF format and having a five\u2011hour deadline to submit solutions to an official ICPC judge. The achievement is framed as part of a continuous progress trajectory, with future goals including the development of systems capable of discovering new knowledge.\nOriginal language: es\nPublish date: September 18, 2025 05:06 PM\nSource:[La Raz\u00f3n](https://www.larazon.es/tecnologia-consumo/openai-humilla-google-humanos-consigue-resultado-perfecto-mayor-competicion-programacion-mundo_2025091868cc0c7a394a8138738ce5b9.html)\n\n**OpenAI GPT-5 with Revolutionary Reasoning Capabilities: What This Means for the Future**\nOpenAI announced GPT\u20115, a large language model that claims a 40% improvement in logical and scientific tasks over GPT\u20114o and the o3 series.  The model achieved 94.6% accuracy on the AIME 2025 math benchmark, 74.9% on SWE\u2011bench for real\u2011world coding, 84.2% on MMMU for multimodal understanding, and 46.2% on HealthBench Hard, with responses ~45% less likely to contain factual inaccuracies than GPT\u20114o and up to 80% less when using its \u2018thinking\u2019 mode.  In the 2025 ICPC World Finals, GPT\u20115 solved 11 of 12 complex algorithmic problems on the first try, outperforming human teams.  GPT\u20115\u2019s architecture is unified, combining reasoning, multimodal input, and agentic execution in a single system; it offers four \u2018thinking\u2019 modes (Standard, Extended, Light, Heavy) and a 256,000\u2011token context window.  The release includes variants such as GPT\u20115\u2011Codex for coding, GPT\u20115\u2011mini and nano for lightweight tasks, and GPT\u20115\u2011chat for conversational use.  The article cites the model\u2019s impact on software development\u2014improving refactoring accuracy by 51% and reducing false positives in code reviews by 68%\u2014and on scientific research, finance, and enterprise AI, noting that Microsoft has embedded GPT\u20115 into Microsoft 365 Copilot and Oracle into its cloud applications.  OpenAI\u2019s safety tests show reduced hallucinations (e.g., only 9% confident answers on non\u2011existent images versus 86.7% for o3).  The piece quotes CEO Sam Altman describing GPT\u20115 as a \u2018significant step along the path to AGI\u2019 and notes a \u2018warmer\u2019 personality update released in mid\u2011August.  Overall, the article presents GPT\u20115 as a transformative tool that could accelerate innovation across sectors while raising concerns about content quality, job displacement, and regulatory scrutiny.\nOriginal language: en\nPublish date: September 18, 2025 03:27 PM\nSource:[Medium.com](https://medium.com/@types24digital/openai-gpt-5-with-revolutionary-reasoning-capabilities-what-this-means-for-the-future-aa6dd5e04d31)\n\n**Google and OpenAI Achieve Superhuman Feats at World Coding Finals**\nAt the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Google\u2019s Gemini\u202f2.5\u202fDeep\u202fThink and OpenAI\u2019s GPT\u20115 both outperformed the best human teams. Gemini earned a gold\u2011medal\u2011level score by solving 10 of 12 problems, including the hardest \u201cProblem\u202fC\u201d that stumped every human team; it completed eight problems in 45\u202fminutes and two more within three hours. GPT\u20115 achieved a perfect 12/12, submitting the correct answer on its first attempt for 11 of the 12 problems. The contest featured 139 elite university teams from nearly 3,000 schools, each tasked with solving 12 complex algorithmic problems in five hours. According to the article, Google\u2019s Gemini used a dynamic\u2011programming approach with a \u201cpriority value\u201d for each reservoir and applied the minimax theorem and nested ternary searches to crack Problem\u202fC. OpenAI\u2019s post on Twitter, quoted in the piece, states, 'our @OpenAI reasoning system got a perfect score of 12/12 during the 2025 ICPC World Finals.' The results illustrate a new frontier in AI, demonstrating multi\u2011step, abstract reasoning that surpasses human performance in competitive programming.\nOriginal language: en\nPublish date: September 18, 2025 11:45 AM\nSource:[Winbuzzer](https://winbuzzer.com/2025/09/18/google-and-openai-achieve-superhuman-feats-at-world-coding-finals-xcxwbn/)\n\n**Dr.\u202fCavadas Warns of an Inevitable AI\u2011Human Conflict**\nPedro\u202fCavadas, in a TVE interview, warned that artificial intelligence (AI) will inevitably clash with humanity because technical systems are advancing faster than human capacities. He cited Epoch\u202fAI\u2019s observation that the computing power used to train leading models has increased 4\u20115\u202ffold per year from 2010 to May\u202f2024, a pace no single human innovation cycle can match. Cavadas added that humans have \"left to evolve\" and are becoming \"more foolish,\" a claim he supported with the Flynn\u2011effect decline in cognitive scores in some countries, though he noted the effect is environmental and not a universal biological collapse.\n\nThe article lists concrete evidence of AI\u2019s reach: the IMF estimates that about 40\u202f% of global employment is exposed to AI, rising to 60\u202f% in advanced economies; a randomized trial of 80,000 women showed AI\u2011assisted mammography reduced workload by half while maintaining diagnostic safety; Med\u2011PaLM\u202f2 achieved expert\u2011level performance on USMLE\u2011style questions, and GPT\u20114 diagnosed 57\u202f% of complex clinical cases in NEJM\u2011AI.\n\nCavadas also highlighted regulatory responses: the EU AI Act entered force on 1\u202fAugust\u202f2024, with phased obligations for high\u2011risk systems by 2026\u201127; the Bletchley Declaration convened 28 countries to assess frontier models; the WHO released 2024\u201125 ethics guidelines; and the Stanford AI Index 2025 reports a growing industrial ecosystem with more private investment and reported incidents.\n\nHe concluded that the real risk is not robotic replacement but opaque interdependence in critical services, urging human\u2011in\u2011the\u2011loop oversight, transparency, and active labor policy to keep pace with AI\u2019s speed.\n\nKey quotes (single\u2011quoted):\n- 'La inteligencia artificial entrar\u00e1 en conflicto con el ser humano en alg\u00fan momento. Claramente.'\n- 'El ser humano hace ya mucho tiempo que dej\u00f3 de evolucionar. No somos m\u00e1s listos. Es m\u00e1s, cada vez somos m\u00e1s tontos. Cada generaci\u00f3n es un poco m\u00e1s tonta porque est\u00e1 menos estimulada.'\n- 'El nivel al que avanza, evoluciona y mejora la inteligencia artificial, nunca, jam\u00e1s, vamos a llegar a esa velocidad.'\nOriginal language: es\nPublish date: September 20, 2025 08:18 AM\nSource:[El Espa\u00f1ol](https://www.elespanol.com/ciencia/20250920/dr-cavadas-alto-claro-espana-ia-generacion-tonta-estimulada/1003743933258_0.html)\n\n**DeepSeek Reveals How It Achieved an AI Revolution for Only US$294,000**\nDeepSeek\u2019s R1 model was trained using a reinforcement\u2011learning approach that only required a binary reward signal, rather than thousands of step\u2011by\u2011step examples.  The researchers explained that, \"En lugar de ense\u00f1ar expl\u00edcitamente al modelo c\u00f3mo resolver un problema, simplemente le proporcionamos los incentivos correctos y desarrolla de forma aut\u00f3noma estrategias avanzadas de resoluci\u00f3n de problemas,\" and noted that the model spontaneously developed sophisticated behaviours such as self\u2011verification and the frequent use of the word \"wait\" during its reasoning.  During training the average length of the model\u2019s responses grew steadily, from short answers to elaborate chains of reasoning that spanned hundreds or thousands of tokens, a growth that emerged naturally to improve accuracy.\n\nThe results were striking.  On the AIME 2024 competition the model\u2019s accuracy rose from 15.6\u202f% to 77.9\u202f% with a single answer, and to 86.7\u202f% when self\u2011consistency techniques were applied\u2014well above the average human performance in this elite mathematics contest.  It also performed strongly on Codeforces programming challenges and on graduate\u2011level problems in biology, physics and chemistry, demonstrating that its reasoning capabilities extend beyond pure mathematics.\n\nEconomically, the training cost was remarkably low.  According to the supplementary material accompanying the Nature paper, the model was trained for 80\u202fhours on a cluster of 512 Nvidia H800 chips, totaling US$294,000.  This figure includes all operational expenses of the super\u2011computing cluster during the training period, a level of transparency rarely seen in the industry.  The cost is less than 0.3\u202f% of the US$100\u202fmillion that Sam Altman, CEO of OpenAI, had suggested would be required for comparable models.\n\nThe study also revealed that DeepSeek used Nvidia A100 GPUs in the preparatory stages of development.  \"En lo que respecta a nuestra investigaci\u00f3n sobre DeepSeek\u2011R1, utilizamos las GPU A100 para preparar los experimentos con un modelo m\u00e1s peque\u00f1o,\" the researchers admitted.  This admission raises questions about the effectiveness of U.S. export controls, which had banned the export of H100 and A100 chips to Chinese firms since October\u202f2022.\n\nThe implications are significant: if frontier\u2011level AI models can be built with modest investment, more actors could enter the market, potentially reducing the concentration of computational power among a handful of tech giants and challenging U.S. geopolitical strategies aimed at limiting China\u2019s access to advanced AI hardware.\nOriginal language: es\nPublish date: September 19, 2025 09:47 PM\nSource:[El Observador](https://www.elobservador.com.uy/argentina/sociedad/deepseek-revela-como-logro-su-revolucion-inteligencia-artificial-solo-us294000-n6017778)\n\n**AI Is Now Way Better at Predicting Startup Success Than VCs - Decrypt**\nResearchers at the University of Oxford and Vela Research released a paper titled 'VCBench: Benchmarking LLMs in Venture Capital' that evaluates whether large language models can predict startup success before it happens. The study built a dataset of 9,000 anonymized founder profiles, 810 of which were labeled as \"successful\"\u2014defined as reaching a major growth milestone such as an exit or IPO. The dataset was scrubbed of names and direct identifiers, and adversarial tests reduced re\u2011identification risk by 92\u202f%. When tested, the models outperformed human benchmarks: the market index baseline achieved 1.9\u202f% precision, Y\u202fCombinator 3.2\u202f%, and tier\u20111 VC firms 5.6\u202f%. DeepSeek\u2011V3 achieved more than six times the market precision, GPT\u20114o topped the leaderboard with the highest F0.5 score, and Claude\u202f3.5\u202fSonnet and Gemini\u202f1.5\u202fPro also matched elite VC performance. The authors released VCBench publicly at vcbench.com to invite further testing, suggesting that LLMs could become essential tools in deal\u2011sourcing and make startup investing more meritocratic.\nOriginal language: en\nPublish date: September 19, 2025 04:27 PM\nSource:[Decrypt](https://decrypt.co/340418/ai-now-way-better-predicting-startup-success-vcs)\n\n**ProRL: When More Training Time Actually Matters**\nThe NVIDIA study, accepted to NeurIPS 2025, shows that extending reinforcement\u2011learning (RL) training to over 2,000 steps\u2014well beyond the typical few hundred\u2014lets a 1.5\u2011billion\u2011parameter language model acquire genuinely new reasoning strategies that its base version cannot reach, even with extensive sampling. The model achieved a 14.7% improvement on math benchmarks, a 13.9% improvement on coding tasks, and a 54.8% improvement on logic\u2011puzzle tasks compared to its base counterpart. The training used 136,000 diverse problems across mathematics, coding, STEM, logic puzzles, and instruction following, and required roughly 16,000 GPU hours on NVIDIA H100s. Pass@k evaluations were used to measure both single\u2011attempt accuracy and broader reasoning capability through multiple attempts. \"ProRL demonstrates that extended RL training enables models to develop genuinely novel reasoning strategies,\" the authors note, underscoring that training compute allocation matters for advancing language\u2011model reasoning.\nOriginal language: en\nPublish date: September 19, 2025 11:06 AM\nSource:[Medium.com](https://medium.com/@stawils/prorl-when-more-training-time-actually-matters-fbb037e50d33)\n\n**The AI model that teaches itself to think through problems, no humans required**\nA new paper in *Nature* reports that DeepSeek AI, a Chinese company, trained its large\u2011language model DeepSeek\u2011R1 to reason on its own using reinforcement learning.  Instead of showing the model every step, researchers gave it a reward only when the final answer was correct, encouraging the model to develop its own problem\u2011solving strategies.  During training, DeepSeek\u2011R1 learned to check its own work, explore alternative approaches, and even use the word \u2018wait\u2019 as it reflected on its thinking process.  The model was evaluated on difficult math, coding and science tasks and outperformed earlier models that relied on human\u2011guided instruction.  Notably, it achieved an 86.7\u202f% accuracy rate on the 2024 American Invitational Mathematics Examination (AIME), a benchmark for top high\u2011school mathematicians.  The authors note that DeepSeek\u2011R1 sometimes mixes languages when prompted in non\u2011English and can over\u2011complicate simple problems, but they expect these issues to be resolved as the approach matures.  The study, cited as Daya\u202fGuo\u202fet\u202fal., *DeepSeek\u2011R1 incentivizes reasoning in LLMs through reinforcement learning*, DOI: 10.1038/s41586-025-09422\u2011z, was published on September\u202f18\u202f2025 by Tech\u202fXplore.\nOriginal language: en\nPublish date: September 18, 2025 08:00 PM\nSource:[Tech Xplore](https://techxplore.com/news/2025-09-ai-problems-humans-required.html)\n\n**OpenAI and Google DeepMind Outshine Students at World\u2019s Top Coding Contest**\nOpenAI and Google DeepMind dominated the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Azerbaijan. OpenAI\u2019s GPT\u20115 solved all 12 problems, achieving a perfect 12\u2011for\u201112 score under the same five\u2011hour time limit as human teams. Eleven of the problems were solved on the first attempt; the twelfth was cracked after nine submissions by an experimental reasoning model. DeepMind\u2019s Gemini\u202f2.5 Deep Think solved 10 of 12 problems, including one that no human team solved, earning a gold\u2011medal level performance. The AI track used the identical problem set, test cases, memory limits, and hardware as the 139 university teams from more than 100 countries. OpenAI said the result would have placed it first in the human leaderboard. Bill Poucher, ICPC Global Executive Director, called the milestone \u2018a key moment in defining the AI tools and academic standards needed for the next generation.\u2019 OpenAI scientist Mostafa Rohaninejad wrote on X that \u2018The next frontier is the discovery of new knowledge, which is the true milestone at the end of the day.\u2019 Gemini also solved a complex network\u2011optimization task that no student team managed, using advanced algorithmic techniques. The contest, which has run for decades, tests 12 problems ranging from graph theory to optimization. Prior to the ICPC, both AI systems earned gold\u2011medal scores at the 2025 International Mathematical Olympiad.\nOriginal language: en\nPublish date: September 18, 2025 07:00 PM\nSource:[Tech Republic](https://www.techrepublic.com/article/openai-deepmind-icpc-2025-results/)\n\n**OpenAI Outperforms Google and Humans: AI Achieves Perfect Score in World Programming Competition**\nOpenAI\u2019s newly released AI system achieved a perfect score in the 2025 International Collegiate Programming Contest (ICPC) held in Baku, Azerbaijan, outperforming both human competitors and Google\u2019s DeepMind model. The system solved all twelve tasks, earning a 100% score within the five\u2011hour time limit and the same conditions as the student teams. According to the company, the system was built from general\u2011purpose reasoning models, none of which were specifically trained for the ICPC. The team used GPT\u20115 together with an internal experimental reasoning model; GPT\u20115 produced correct solutions for eleven of the twelve problems, while the experimental model selected the final solution after nine attempts for the hardest problem. The best human team also solved eleven of the twelve problems. OpenAI highlighted that this success demonstrates the capability of advanced reasoning models to excel where others fail, citing prior gold\u2011level results in the International Mathematical Olympiad and the International Olympiad in Informatics. As stated by the company, the system competed under the same conditions as the students, receiving problems in standard PDF format and having a five\u2011hour deadline to submit solutions to an official ICPC judge. The achievement is framed as part of a continuous progress trajectory, with future goals including the development of systems capable of discovering new knowledge.\nOriginal language: es\nPublish date: September 18, 2025 05:06 PM\nSource:[La Raz\u00f3n](https://www.larazon.es/tecnologia-consumo/openai-humilla-google-humanos-consigue-resultado-perfecto-mayor-competicion-programacion-mundo_2025091868cc0c7a394a8138738ce5b9.html)\n\n**OpenAI GPT-5 with Revolutionary Reasoning Capabilities: What This Means for the Future**\nOpenAI announced GPT\u20115, a large language model that claims a 40% improvement in logical and scientific tasks over GPT\u20114o and the o3 series.  The model achieved 94.6% accuracy on the AIME 2025 math benchmark, 74.9% on SWE\u2011bench for real\u2011world coding, 84.2% on MMMU for multimodal understanding, and 46.2% on HealthBench Hard, with responses ~45% less likely to contain factual inaccuracies than GPT\u20114o and up to 80% less when using its \u2018thinking\u2019 mode.  In the 2025 ICPC World Finals, GPT\u20115 solved 11 of 12 complex algorithmic problems on the first try, outperforming human teams.  GPT\u20115\u2019s architecture is unified, combining reasoning, multimodal input, and agentic execution in a single system; it offers four \u2018thinking\u2019 modes (Standard, Extended, Light, Heavy) and a 256,000\u2011token context window.  The release includes variants such as GPT\u20115\u2011Codex for coding, GPT\u20115\u2011mini and nano for lightweight tasks, and GPT\u20115\u2011chat for conversational use.  The article cites the model\u2019s impact on software development\u2014improving refactoring accuracy by 51% and reducing false positives in code reviews by 68%\u2014and on scientific research, finance, and enterprise AI, noting that Microsoft has embedded GPT\u20115 into Microsoft 365 Copilot and Oracle into its cloud applications.  OpenAI\u2019s safety tests show reduced hallucinations (e.g., only 9% confident answers on non\u2011existent images versus 86.7% for o3).  The piece quotes CEO Sam Altman describing GPT\u20115 as a \u2018significant step along the path to AGI\u2019 and notes a \u2018warmer\u2019 personality update released in mid\u2011August.  Overall, the article presents GPT\u20115 as a transformative tool that could accelerate innovation across sectors while raising concerns about content quality, job displacement, and regulatory scrutiny.\nOriginal language: en\nPublish date: September 18, 2025 03:27 PM\nSource:[Medium.com](https://medium.com/@types24digital/openai-gpt-5-with-revolutionary-reasoning-capabilities-what-this-means-for-the-future-aa6dd5e04d31)\n\n**World programming championship: How ChatGPT, Gemini and AI bots performed**\nThe 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Azerbaijan, marked the first time artificial intelligence systems competed alongside human teams. OpenAI\u2019s GPT\u20115, as part of an ensemble, achieved a perfect score, solving all 12 problems; eleven were accepted on the first submission, and the final problem required only one additional attempt after an experimental variant was deployed. No human team in ICPC history has reached such perfection. Google DeepMind\u2019s Gemini 2.5 solved 10 of the 12 problems, earning a gold\u2011medal\u2011equivalent placement. Gemini\u2019s fastest run saw eight problems solved in just 45 minutes, a pace unmatched by the fastest human squads. The most notable achievement was Gemini\u2019s solution to Problem C, a systems\u2011optimization challenge that no human team solved; Gemini introduced a hybrid minimax\u2011dynamic\u2011programming approach that had not been previously applied. These results demonstrate that general\u2011purpose reasoning models can adapt, reason in real time, and generate novel algorithms under time pressure, challenging the long\u2011standing human monopoly on competitive programming. The article raises questions about future contest formats, potential human\u2011AI collaboration, and the broader impact on education and innovation.\nOriginal language: en\nPublish date: September 18, 2025 12:45 PM\nSource:[Digit](https://www.digit.in/features/general/world-programming-championship-how-chatgpt-gemini-and-ai-bots-performed.html)\n\n**Google and OpenAI Achieve Superhuman Feats at World Coding Finals**\nAt the 2025 International Collegiate Programming Contest (ICPC) World Finals in Baku, Google\u2019s Gemini\u202f2.5\u202fDeep\u202fThink and OpenAI\u2019s GPT\u20115 both outperformed the best human teams. Gemini earned a gold\u2011medal\u2011level score by solving 10 of 12 problems, including the hardest \u201cProblem\u202fC\u201d that stumped every human team; it completed eight problems in 45\u202fminutes and two more within three hours. GPT\u20115 achieved a perfect 12/12, submitting the correct answer on its first attempt for 11 of the 12 problems. The contest featured 139 elite university teams from nearly 3,000 schools, each tasked with solving 12 complex algorithmic problems in five hours. According to the article, Google\u2019s Gemini used a dynamic\u2011programming approach with a \u201cpriority value\u201d for each reservoir and applied the minimax theorem and nested ternary searches to crack Problem\u202fC. OpenAI\u2019s post on Twitter, quoted in the piece, states, 'our @OpenAI reasoning system got a perfect score of 12/12 during the 2025 ICPC World Finals.' The results illustrate a new frontier in AI, demonstrating multi\u2011step, abstract reasoning that surpasses human performance in competitive programming.\nOriginal language: en\nPublish date: September 18, 2025 11:45 AM\nSource:[Winbuzzer](https://winbuzzer.com/2025/09/18/google-and-openai-achieve-superhuman-feats-at-world-coding-finals-xcxwbn/)\n\n**Evaluation of AI: Breathing Life into the Algorithmic Test**\nCl\u00e9mentine Fourrier, a researcher in AI at HuggingFace, explains that the impressive capabilities of generative models such as ChatGPT, Claude, DeepSeek, Grok, and Gemini are assessed through standardized benchmarks that compare their performance to human intelligence. The article notes that the newest benchmark, introduced this winter, is called 'le dernier examen de l'humanit\u00e9' and is designed to test advanced analytical reasoning. It cites FrontierMath as a reference tool for evaluating sophisticated mathematical reasoning in AI, and mentions that OpenAI has reported results comparable to a human on a general\u2011intelligence test, as reported by The Conversation. The piece also references a discussion from Polytechnique Insight about whether AI and human intelligence are comparable. All claims are directly supported by the article text, which includes the quoted terms 'intelligence', 'benchmarks', and 'le dernier examen de l'humanit\u00e9'.\nOriginal language: fr\nPublish date: September 11, 2025 02:59 PM\nSource:[Radio France](https://www.radiofrance.fr/franceculture/podcasts/la-science-cqfd/l-ia-souffle-dans-l-algotest-1340696)\n\n**Blitzy Blows Past SWE-bench Verified, Demonstrating Next Frontier in AI Progress**\nBlitzy, an autonomous software engineering orchestration platform, announced that it has topped the industry\u2011leading benchmark SWE\u2011bench Verified with an 86.8% score, a 13.02% improvement (10 percentage\u2011point leap) over the previous best. According to the article, this marks the largest single advance since March\u202f2024, when Devin achieved a 6.9% improvement (11.9\u2011point leap). The result demonstrates that scaling inference time compute can deliver exponential, not incremental, gains. The article notes that the benchmark had plateaued around 70\u201175% for many models, suggesting a practical ceiling that Blitzy has surpassed. It cites OpenAI\u2019s analysis that human evaluators flagged many samples as \u2018hard or impossible to solve\u2019 due to ambiguous or contradictory requirements. Blitzy\u2019s CTO Sid\u202fPardeshi explained, 'The \u2018unsolvables\u2019 weren\u2019t actually unsolvable \u2013 they just required deeper thinking than System\u20111 AI could provide.' Blitzy\u2019s System\u20112 approach allows AI to reason for hours or days, turning previously unsolvable problems into solvable ones. The article also highlights benchmark limitations: 32.67% of SWE\u2011bench patches may involve solution leakage, and 94% of issues predate LLM training data, raising questions about genuine reasoning versus pattern recognition. Blitzy\u2019s enterprise\u2011scale examples include modernizing 4\u202fmillion lines of legacy Java with 72+ hours of distributed reasoning, extracting services from a 500,000\u2011line monolith requiring 24+ hours of architectural analysis, and cross\u2011language migration with extended verification cycles to preserve semantic equivalence. These achievements illustrate the potential of inference\u2011time scaling beyond isolated coding tasks, suggesting a shift toward System\u20112 AI for complex problem solving.\nOriginal language: en\nPublish date: September 09, 2025 12:30 PM\nSource:[WFMZ.com](https://www.wfmz.com/news/pr_newswire/pr_newswire_technology/blitzy-blows-past-swe-bench-verified-demonstrating-next-frontier-in-ai-progress/article_c7c54a8e-9ef7-5678-8b3b-2d5b5d547f8d.html)\n\n**The Future of AI Reasoning: From Cold Maths to Reason-Native Models: \"AI may never think like us...**\nThe article explains how artificial intelligence is moving from a focus on pattern\u2011recognition and word\u2011prediction to explicit reasoning. It notes that large language models (LLMs) were originally designed to predict the next word, which produced fluent language but unreliable reasoning, as shown by poor performance on benchmarks such as GSM8K and GPQA. Recent research now trains models to reason by incorporating techniques like scratch\u2011pads, self\u2011consistency, tree\u2011of\u2011thoughts, ReAct frameworks, and pause\u2011and\u2011reflect training. The article cites concrete evidence: DeepMind\u2019s AlphaGeometry and AlphaProof solved Olympiad\u2011level problems with symbolic proof checkers, and AlphaGeometry2 outperformed an average gold\u2011medalist in 2025. It also mentions hard benchmarks such as ARC\u2011AGI\u20112 and GPQA that demonstrate the progress of reasoning\u2011native approaches. Key players include OpenAI (o1), DeepSeek (R1), DeepMind (AlphaGeometry/AlphaProof), AI21 Labs (MRKL), and MIT\u2011IBM (neuro\u2011symbolic concept learner). The author argues that the future will likely be a system of systems combining neural perception, symbolic reasoning, verifiers, and reinforcement learning, rather than a single monolithic model. The article concludes with a quote: 'AI may never reason like us, but reasoning\u2011native AI will matter \u2013 because reliability, explainability, and verifiable steps are what we need in systems we trust to make decisions that touch our work, our companies, and our lives.'\nOriginal language: en\nPublish date: September 06, 2025 10:27 AM\nSource:[Medium.com](https://medium.com/@raktims2210/the-future-of-ai-reasoning-from-cold-maths-to-reason-native-models-ai-may-never-think-like-us-6a5f2ab6873c)\n\n**Gemini Symposium Singapore 2025: A Review**\nThe Gemini Symposium Singapore 2025 covered a range of topics on large language models (LLMs) and AI culture, with keynotes from Google DeepMind\u2019s Vice\u2011President Quoc\u202fLe, LLM researcher Denny\u202fZhou, and a fireside chat with Benoit\u202fSchillings, CTO of X and VP of Technology at DeepMind.  Le opened by describing how Google\u2019s culture of \u201caccidental discovery\u201d turned what he calls \u2018bad ideas\u2019 into breakthroughs, citing the evolution from Seq\u20112\u2011Seq LSTMs to the attention\u2011based model in *Attention Is All You Need* and the pre\u2011training paradigm that now underpins LLMs.  He noted that while compression (pre\u2011training on internet\u2011sized corpora) and reasoning (post\u2011training) give a \u201csemblance of intelligence,\u201d current methods are still not efficient enough to reach human\u2011level intelligence, adding that \u201csize is not all that matters in eliciting intelligence.\u201d  Le also highlighted Gemini\u2019s Deep\u2011Think model, which uses self\u2011consistency and chain\u2011of\u2011thought to achieve a gold\u2011medal standard on the International Mathematical Olympiad.  \n\nZhou\u2019s keynote focused on LLM reasoning, arguing that transformers can generate *O(T)* intermediate tokens for problems solvable by boolean circuits of size *T*, and that reasoning can be achieved without fine\u2011tuning.  He cautioned that LLMs are fundamentally probabilistic models that do not \u201cthink like humans,\u201d and that combining reasoning with self\u2011consistency improves complex\u2011problem performance.  \n\nDuring the fireside chat, Schillings discussed the challenges of coding with LLMs, noting that current systems can solve benchmarks such as SWEBench but struggle with extremely long contexts (millions of lines of code).  He emphasized the need for context\u2011efficient models that attend to relevant code segments.  Schillings also spoke about the culture that drives innovation, quoting: \u2018Disney taught us that if you kiss a frog it might turn into a princess, but it does not mean that you go out there and kiss any and every frog out there in hopes that it turns into a princess.\u2019  \n\nThe symposium also included a brief mention of a quote from the event: \u2018Despite so much progress being made on frontier model, no big companies have actually made any money out of AI,....well except for NVIDIA.\u2019  Overall, the review highlights the symposium\u2019s emphasis on iterative learning from past ideas, the importance of reasoning and self\u2011consistency in LLMs, and the cultural factors that enable breakthrough AI research.\n\nOriginal language: en\nPublish date: September 05, 2025 12:08 AM\nSource:[Medium.com](https://medium.com/@shearmanchuaweijie_48703/gemini-symposium-singapore-2025-a-review-fa73f0f13920)\n\n**Forget data labeling: Tencent's R-Zero shows how LLMs can train themselves**\nTencent AI Lab and Washington University in St.\u202fLouis introduced R\u2011Zero, a reinforcement\u2011learning framework that lets two large language models (LLMs) co\u2011evolve without any human\u2011labelled data. In the system a base model is split into a \u2018Challenger\u2019 that generates tasks just beyond the Solver\u2019s current ability and a Solver that is rewarded for solving them. The Challenger\u2019s output is filtered for diversity and used to fine\u2011tune the Solver; the Solver\u2019s own majority\u2011vote answers become the training labels. This loop repeats, creating a self\u2011improving curriculum.\n\nExperiments on open\u2011source LLMs such as Qwen3\u20114B\u2011Base and Qwen3\u20118B\u2011Base showed substantial gains. The Qwen3\u20114B\u2011Base model\u2019s math\u2011reasoning score rose by +6.49 on average after several iterations, while the larger Qwen3\u20118B\u2011Base model improved by +5.51 points after three iterations. General\u2011domain benchmarks also benefited: the same Qwen3\u20114B\u2011Base model improved by +7.54 on MMLU\u2011Pro and SuperGPQA after training on math problems.\n\nThe first iteration produced an immediate performance leap, confirming the effectiveness of the RL\u2011trained Challenger. Moreover, models that first improved via R\u2011Zero achieved even higher performance when later fine\u2011tuned on traditional labelled data, suggesting R\u2011Zero can act as a powerful pre\u2011training step.\n\nHowever, the quality of self\u2011generated labels declined over time: the Solver\u2019s accuracy dropped from 79\u202f% in the first iteration to 63\u202f% by the third, compared with a strong oracle such as GPT\u20114. This trade\u2011off highlights a key bottleneck for long\u2011term improvement.\n\nChengsong Huang, co\u2011author of the paper, noted that \"our approach entirely bypasses the fundamental bottleneck of having to find, label, and curate high\u2011quality datasets,\" and that the framework could enable AI that is no longer limited by the scope of human knowledge. He also suggested adding a third \u2018Verifier\u2019 agent to evaluate quality in more subjective domains.\n\nOverall, R\u2011Zero demonstrates that LLMs can self\u2011evolve reasoning capabilities from zero external data, offering a cost\u2011effective path for enterprises that lack large labelled datasets.\nOriginal language: en\nPublish date: August 28, 2025 09:07 PM\nSource:[VentureBeat](https://venturebeat.com/ai/forget-data-labeling-tencents-r-zero-shows-how-llms-can-train-themselves/)\n\n**How to find the smartest AI**\nThe article explains the growing need for new AI benchmarks that truly challenge state\u2011of\u2011the\u2011art models. It highlights ZeroBench, launched by Jonathan Roberts and colleagues at Cambridge, which targets large multimodal models and currently scores zero for all LLMs. It contrasts this with EnigmaEval, a harder set of over a thousand puzzles from Scale AI, where even Anthropic\u2019s frontier model has only answered one question correctly. The piece notes that older benchmarks such as ImageNet suffered from flawed questions that models could cheat on, and that many modern systems now perform well on those tests because the data is part of their training set. It cites specific metrics: o1\u2011mini scored 98.9\u202f% on a 500\u2011problem high\u2011school maths set, and o3\u2011pro is likely to achieve near\u2011perfect scores on the same set. The article quotes OpenAI CEO Sam Altman saying the new GPT\u20114.5 \u201cwon\u2019t crush benchmarks\u201d and that \u201cthere\u2019s a magic to it I haven\u2019t felt before.\u201d It also discusses newer approaches like Chatbot Arena, which ranks models by blind user preference, and the ARC\u2011AGI series, noting that ARC\u2011AGI\u202f2 still eludes top systems while ARC\u2011AGI\u202f3 is already in development. The author stresses that as models learn existing tests, saturation occurs, making it essential to create fresh, well\u2011vetted challenges that measure genuine progress.\nOriginal language: en\nPublish date: August 26, 2025 07:02 AM\nSource:[mint](https://www.livemint.com/global/how-to-find-the-smartest-ai-11756191365966.html)\n\n**AI systems are great at tests. But how do they perform in real life?**\nThe article discusses how current AI evaluation relies heavily on benchmark tests, which may not reflect real\u2011world performance. It cites OpenAI\u2019s claim that GPT\u20115 is \"much smarter across the board\" and notes that high benchmark scores have led to significant funding, such as Cognition AI\u2019s $175\u202fmillion raise after a software\u2011engineering benchmark. The piece highlights that benchmarks can be gamed, citing Meta\u2019s optimisation of Llama\u20114 for a chatbot\u2011ranking site and OpenAI\u2019s use of the FrontierMath dataset, illustrating Goodhart\u2019s law: \"When a measure becomes a target, it ceases to be a good measure.\" Rumman Chowdhury warns that over\u2011emphasis on metrics can lead to \"manipulation, gaming, and a myopic focus on short\u2011term qualities and inadequate consideration of long\u2011term consequences\". The article argues for more holistic evaluation frameworks, such as MedHELM, which uses 35 benchmarks across five clinical categories to better mimic real\u2011world medical tasks. It calls for a new evaluation ecosystem that incorporates red\u2011team testing and field testing to assess AI\u2019s broader economic, cultural, and societal impacts.\nOriginal language: en\nPublish date: August 24, 2025 08:10 PM\nSource:[The Conversation](http://theconversation.com/ai-systems-are-great-at-tests-but-how-do-they-perform-in-real-life-260176)\n\n**OpenAI's AI Model Achieves 'Gold Medal-Level Performance' in International Mathematical Olympiad**\nOpenAI researcher Alexander Wei announced that their latest experiment reasoning model achieved a 'gold medal-level performance' in the 2025 International Mathematical Olympiad (IMO), solving 5 out of 6 problems and scoring 35 out of 42 points. This achievement is considered a major breakthrough in AI's general reasoning ability, but experts warn that the evaluation conditions may differ from those of human participants. The IMO is recognized as the most prestigious math competition globally, with participants having 4.5 hours to solve 3 difficult math problems each day, without using tools or communicating with other participants. OpenAI's model was evaluated under the same rules as human participants, with two 4.5-hour exam sessions, no tools or internet access, and reading official problem statements and writing natural language proofs. The model's submissions were independently scored by IMO medal winners, with a unanimous agreement on the final score. Wei stated, 'We have achieved a model that can produce complex, rigorous arguments at the level of human mathematicians.' He emphasized that this ability was not achieved through narrow task-specific methods, but through breakthroughs in general reinforcement learning and testing. OpenAI CEO Sam Altman called this a 'significant milestone in AI's progress over the past 10 years,' but noted that the model with 'gold medal-level ability' will not be made public for 'several months.' He added, 'When we first founded OpenAI, this was a dream, but it was not realistic.' The progress in AI's math abilities is remarkable. OpenAI researcher Noam Brown pointed out that in 2024, AI labs were still using elementary school math as a model evaluation standard, and quickly broke through to high school math standards, AIME competitions, and now IMO gold medal levels. However, experts have raised concerns about the evaluation methods. AI critic Gary Marcus praised the model's performance as 'truly impressive,' but questioned the model's training methods, 'general intelligence' range, practicality for the general public, and the cost of each problem. He also noted that the IMO organization has not independently verified these results. Mathematician Terence Tao pointed out that changes in testing conditions can greatly affect the results. He used human competitions as an example, stating that if students were allowed to use calculators, textbooks, or internet searches, or were given several days instead of 4.5 hours to complete the problems, the success rate would be significantly higher. Independent evaluation institution MathArena recently tested major language models, including GPT-4, on 2025 IMO problems and found them to be poor, with logical errors, incomplete arguments, and even fabricated theorems. This makes OpenAI's announcement all the more attention-grabbing, but its true value will depend on whether the results can be independently replicated and applied to actual scientific problems.\nOriginal language: zh\nPublish date: July 20, 2025 02:02 PM\nSource:[\u9999\u6e2f unwire.hk \u73a9\u751f\u6d3b\uff0e\u6a02\u79d1\u6280](https://unwire.hk/2025/07/20/openai-imo-gold/ai/)\n\n**A near AI frontier: Artificial Supertintelligence (ASI)**\nThe development of Artificial Intelligence (AI) has led to the rise of three theoretical fields: Artificial General Intelligence (AGI), Superintelligence (ASI), and Large Language Models (LLMs). AGI is a hypothetical stage where an AI system can match or exceed human cognitive abilities across any task. Currently, reasoning models like OpenAI's o1 and DeepSeek's R1 have scored between 1% and 1.3% in the ARC-AGI-2 test, while non-reasoning models have scored around 1%. However, companies like Meta are moving on to explore Superintelligence, which is defined as a system with an intellectual scope beyond human intelligence. Building blocks of ASI include cutting-edge cognitive functions and highly developed thinking skills. While companies like Meta, Anthropic, Google's DeepMind, and OpenAI are actively working towards developing ASI, the question remains how far we are from achieving Superintelligence, and the answer is that we are still pretty far. Fundamentally speaking, we have to crack AGI first, which includes cracking the human brain to surpass human cognitive capacity.\nOriginal language: en\nPublish date: June 26, 2025 08:52 PM\nSource:[Medium.com](https://medium.com/@i.am.lvcky/a-near-ai-frontier-artificial-supertintelligence-asi-eb2574ed0e23)\n\n",
    "date": "2025-09-21T01:15:45.218464",
    "summary": "Across a diverse set of expert perspectives\u2014spanning machine learning engineering, AI benchmarking, forecasting, deep learning theory, scaling laws, computational learning theory, algorithmic complexity, and mathematics education\u2014there is a very strong consensus that AI systems are highly likely, but not certain, to achieve and publicly document >85% performance on the FrontierMath benchmark before January 1, 2026. \n\nThe basis for this optimism is the extraordinary, well-documented progress seen over the past year on adjacent and previously unthinkable mathematical benchmarks: models like GPT-5, DeepSeek-R1, and Gemini 2.5 have consistently achieved or surpassed human gold medalist-level scores on AIME, IMO, and ICPC, often exceeding 85% and even reaching perfect scores in adversarial programming tasks\u2014achievements that were considered out of reach only a year prior. Rapid advances are attributed to architectural breakthroughs (e.g., reinforcement learning, self-reflection, chain-of-thought, self-consistency, and verifier-augmented reasoning), competitive and prestige-driven incentives for labs to publicly release results, and the accessibility of frontier-level computational resources.\n\nDespite these successes, experts consistently note that FrontierMath is specifically engineered to avoid overfitting and training/test leakage, offering a genuinely novel, adversarial challenge not directly paralleled in prior datasets. This introduces substantial, albeit not dominant, uncertainty: transferring breakthroughs from benchmarks like AIME/IMO to the more resistant and less-public FrontierMath may still demand fundamentally new techniques or encounter unexpected performance plateaus. Additional risk comes from the requirement for timely, public documentation\u2014last-minute embargoes or strategic publication delays could prevent a 'Yes' resolution even if technical capability is achieved.\n\nProbabilities offered by experts largely cluster in the 80\u201393% range, with most prominent factors increasing likelihood being the current competitive race among labs, published step-function advances in reasoning, cross-benchmark transfer observed in 2025, and the incentive for rapid public milestone validation. The main countervailing factors are the possibility that the hardest or most novel FrontierMath problems remain immune to existing advances, or that documentation comes too late.\n\nIn summary: The weight of evidence points strongly toward a public, qualifying >85% achievement before the January 2026 deadline, driven by the convergence of technical capability, rapid learning curves, and strong public incentives\u2014though the scenario is not quite a certainty due to the deliberately adversarial nature of FrontierMath and some risk of publication lag. Forecasted probabilities by domain experts suggest a robust consensus around an 80\u201390% chance of a 'Yes' outcome.",
    "forecasters": [
        "Machine Learning Engineering (Large Language Models Development)",
        "Machine Learning Engineering (Benchmark Evaluation Standards)",
        "AI Policy and Forecasting (Technology Forecasting Methodologies)",
        "Artificial Intelligence (Deep Learning Theory)",
        "Artificial Intelligence (Scaling Laws in AI)",
        "Computer Science (Algorithmic Complexity)",
        "Computer Science (Computational Learning Theory)",
        "Mathematics Education (Mathematics Problem Solving Research)"
    ]
}